{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiWulKAdmAHX"
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KsBGZpKkWki"
   },
   "source": [
    "Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
    "1. Multi-head attention 및 self-attention 구현.\n",
    "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qRU5DFY2OM8"
   },
   "source": [
    "### 필요 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lDtMioSQQ1bB"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH0VdC4uJJVG"
   },
   "source": [
    "## Req. 2-1 Multi-head self-attention 구조 익히기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBiZObgRep_Q"
   },
   "source": [
    "### **데이터 전처리**\n",
    "vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n",
    "\n",
    "각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "e9ULZIqTenSc"
   },
   "outputs": [],
   "source": [
    "pad_id = 0\n",
    "vocab_size = 100\n",
    "\n",
    "data = [\n",
    "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
    "  [60, 96, 51, 32, 90],\n",
    "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
    "  [75, 51],\n",
    "  [66, 88, 98, 47],\n",
    "  [21, 39, 10, 64, 21],\n",
    "  [98],\n",
    "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
    "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
    "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6Hx3mcivgMyH"
   },
   "outputs": [],
   "source": [
    "# 길이 맞춰주기 위해 패딩합니다.\n",
    "def padding(data):\n",
    "  max_len = len(max(data, key=len))\n",
    "  print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "  for i, seq in enumerate(tqdm(data)):\n",
    "    if len(seq) < max_len:\n",
    "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "  return data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3e8FiNvgX60",
    "outputId": "4467b1d0-9e2e-4994-d6db-b9f3c10b20e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data, max_len = padding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "hwPSIWYugaN0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
       " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [77,\n",
       "  65,\n",
       "  51,\n",
       "  77,\n",
       "  19,\n",
       "  15,\n",
       "  35,\n",
       "  19,\n",
       "  23,\n",
       "  97,\n",
       "  50,\n",
       "  46,\n",
       "  53,\n",
       "  42,\n",
       "  45,\n",
       "  91,\n",
       "  66,\n",
       "  3,\n",
       "  43,\n",
       "  10],\n",
       " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
       " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwqjACx8iidc"
   },
   "source": [
    "### Hyperparameter 세팅 및 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p-Ngp2nWimS8"
   },
   "outputs": [],
   "source": [
    "d_model = 512  # model의 hidden size\n",
    "num_heads = 8  # head의 개수\n",
    "\n",
    "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GJMi2Xsni5uq"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# B: batch size, L: maximum sequence length\n",
    "batch = torch.LongTensor(data)  # (B, L)\n",
    "batch_emb = embedding(batch)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "3tLCUQwojcUb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.4039e-02, -2.8913e-01, -8.2269e-01,  ...,  1.3452e-01,\n",
      "           8.6719e-01, -5.0307e-01],\n",
      "         [-9.3842e-01,  2.7392e-01, -5.7227e-02,  ..., -8.8614e-01,\n",
      "           7.2584e-01,  1.4010e-01],\n",
      "         [-1.2716e+00,  7.6939e-01,  5.6826e-01,  ..., -9.3927e-01,\n",
      "          -9.6351e-02,  1.0717e+00],\n",
      "         ...,\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01]],\n",
      "\n",
      "        [[-1.4709e+00,  1.1863e+00,  3.4881e-01,  ..., -5.9714e-02,\n",
      "           8.5336e-01,  3.3637e-01],\n",
      "         [ 7.5859e-01,  7.3552e-02,  3.8392e-01,  ..., -9.6302e-02,\n",
      "          -6.5363e-01,  3.4412e-01],\n",
      "         [-1.0532e+00, -2.1633e+00, -1.2308e+00,  ...,  1.6122e-01,\n",
      "          -6.4195e-01,  1.4310e+00],\n",
      "         ...,\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01]],\n",
      "\n",
      "        [[-2.9003e-01,  8.6165e-01,  7.5550e-01,  ..., -9.3886e-01,\n",
      "           1.3761e+00, -9.9445e-01],\n",
      "         [-7.2960e-01, -5.7798e-01, -6.5148e-01,  ...,  1.7222e+00,\n",
      "           5.1855e-01, -4.5880e-01],\n",
      "         [ 8.9394e-01,  2.1163e+00,  5.1336e-01,  ..., -2.2760e+00,\n",
      "           2.0200e+00,  1.3075e+00],\n",
      "         ...,\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.4929e-01, -1.0974e+00, -1.3464e+00,  ...,  5.6672e-01,\n",
      "          -1.7645e+00, -1.3206e-01],\n",
      "         [ 5.2055e-02, -1.2875e-01, -1.3960e+00,  ..., -8.6761e-01,\n",
      "           1.1457e+00,  3.9612e-01],\n",
      "         [-1.0532e+00, -2.1633e+00, -1.2308e+00,  ...,  1.6122e-01,\n",
      "          -6.4195e-01,  1.4310e+00],\n",
      "         ...,\n",
      "         [-5.4451e-01, -6.2741e-01, -2.0159e-03,  ...,  3.5496e-01,\n",
      "           1.6169e-01,  4.8252e-01],\n",
      "         [ 8.8649e-01, -1.2411e-01, -1.7121e+00,  ...,  2.9087e-01,\n",
      "          -1.1281e+00,  4.2332e-01],\n",
      "         [ 2.4116e-01,  1.6238e+00,  9.4783e-02,  ...,  2.0905e+00,\n",
      "           7.3412e-01,  1.3298e+00]],\n",
      "\n",
      "        [[-2.5484e-01,  9.2921e-02, -7.2103e-01,  ...,  3.1550e-01,\n",
      "          -1.4507e-01,  7.9719e-01],\n",
      "         [ 3.2435e-01, -1.5807e+00,  8.8603e-01,  ..., -2.1333e-01,\n",
      "           1.0264e+00, -8.6379e-01],\n",
      "         [ 9.2771e-02, -1.1498e+00, -6.0352e-01,  ...,  1.9308e-01,\n",
      "           4.1806e-01,  6.6109e-01],\n",
      "         ...,\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01]],\n",
      "\n",
      "        [[-6.2520e-01, -9.7049e-01, -2.0593e-01,  ..., -4.0258e-01,\n",
      "           1.2090e+00, -1.3993e+00],\n",
      "         [ 3.2435e-01, -1.5807e+00,  8.8603e-01,  ..., -2.1333e-01,\n",
      "           1.0264e+00, -8.6379e-01],\n",
      "         [-3.2905e-01, -8.5030e-01,  3.8719e-01,  ...,  1.0563e+00,\n",
      "           7.3798e-01,  9.4348e-02],\n",
      "         ...,\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01],\n",
      "         [ 3.2891e-01,  1.3907e+00,  1.2324e-01,  ...,  1.0098e+00,\n",
      "           8.7211e-01, -5.0238e-01]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(batch_emb)\n",
    "print(batch_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0Lhx892gmi3"
   },
   "source": [
    "### Linear projection & 여러 head로 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urXMBRnRgqvw"
   },
   "source": [
    "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9DWKDqgCgfMk"
   },
   "outputs": [],
   "source": [
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tcLuhda7m-Lm"
   },
   "outputs": [],
   "source": [
    "w_0 = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-vSL7PwnV6k",
    "outputId": "1707f866-ed4f-4b87-89b8-651fe4d29ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "q = w_q(batch_emb)  # (B, L, d_model)\n",
    "k = w_k(batch_emb)  # (B, L, d_model)\n",
    "v = w_v(batch_emb)  # (B, L, d_model)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wnvlum-LnF1T"
   },
   "source": [
    "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXcYLZYvJT_1"
   },
   "source": [
    "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
    "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
    "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tiOKAv9nEli",
    "outputId": "f22f50b5-2830-48fe-8308-ce0ac49f65dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "# num_heads * d_k로 쪼갠다\n",
    "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tNb2isfn5Cx",
    "outputId": "12751a52-14d7-405d-bf6f-a894e972dd10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "# num_heads를 밖으로 뺌으로써\n",
    "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
    "\n",
    "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWrDA5_Sofad"
   },
   "source": [
    "### Scaled dot-product self-attention 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w52C4k3Wfl8m"
   },
   "source": [
    "각 head에서 실행되는 self-attetion 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "A5waKr0Hfi2K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0512, 0.0481, 0.0495,  ..., 0.0583, 0.0583, 0.0583],\n",
      "          [0.0934, 0.0373, 0.0514,  ..., 0.0275, 0.0275, 0.0275],\n",
      "          [0.0435, 0.0527, 0.0631,  ..., 0.0376, 0.0376, 0.0376],\n",
      "          ...,\n",
      "          [0.0456, 0.0216, 0.0390,  ..., 0.0616, 0.0616, 0.0616],\n",
      "          [0.0456, 0.0216, 0.0390,  ..., 0.0616, 0.0616, 0.0616],\n",
      "          [0.0456, 0.0216, 0.0390,  ..., 0.0616, 0.0616, 0.0616]],\n",
      "\n",
      "         [[0.0558, 0.0386, 0.0357,  ..., 0.0398, 0.0398, 0.0398],\n",
      "          [0.0366, 0.0220, 0.0631,  ..., 0.0800, 0.0800, 0.0800],\n",
      "          [0.1042, 0.0313, 0.0192,  ..., 0.0598, 0.0598, 0.0598],\n",
      "          ...,\n",
      "          [0.0325, 0.0626, 0.1139,  ..., 0.0492, 0.0492, 0.0492],\n",
      "          [0.0325, 0.0626, 0.1139,  ..., 0.0492, 0.0492, 0.0492],\n",
      "          [0.0325, 0.0626, 0.1139,  ..., 0.0492, 0.0492, 0.0492]],\n",
      "\n",
      "         [[0.0347, 0.0336, 0.0580,  ..., 0.0540, 0.0540, 0.0540],\n",
      "          [0.0779, 0.0454, 0.0496,  ..., 0.0536, 0.0536, 0.0536],\n",
      "          [0.0451, 0.0897, 0.1621,  ..., 0.0359, 0.0359, 0.0359],\n",
      "          ...,\n",
      "          [0.0703, 0.0457, 0.0458,  ..., 0.0362, 0.0362, 0.0362],\n",
      "          [0.0703, 0.0457, 0.0458,  ..., 0.0362, 0.0362, 0.0362],\n",
      "          [0.0703, 0.0457, 0.0458,  ..., 0.0362, 0.0362, 0.0362]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0428, 0.0643, 0.0411,  ..., 0.0341, 0.0341, 0.0341],\n",
      "          [0.0454, 0.0505, 0.0595,  ..., 0.0475, 0.0475, 0.0475],\n",
      "          [0.0396, 0.0257, 0.0463,  ..., 0.0878, 0.0878, 0.0878],\n",
      "          ...,\n",
      "          [0.0748, 0.0508, 0.0605,  ..., 0.0675, 0.0675, 0.0675],\n",
      "          [0.0748, 0.0508, 0.0605,  ..., 0.0675, 0.0675, 0.0675],\n",
      "          [0.0748, 0.0508, 0.0605,  ..., 0.0675, 0.0675, 0.0675]],\n",
      "\n",
      "         [[0.0443, 0.0522, 0.0344,  ..., 0.0656, 0.0656, 0.0656],\n",
      "          [0.0457, 0.0359, 0.0414,  ..., 0.0367, 0.0367, 0.0367],\n",
      "          [0.0604, 0.0357, 0.0330,  ..., 0.0650, 0.0650, 0.0650],\n",
      "          ...,\n",
      "          [0.0425, 0.0364, 0.0366,  ..., 0.0534, 0.0534, 0.0534],\n",
      "          [0.0425, 0.0364, 0.0366,  ..., 0.0534, 0.0534, 0.0534],\n",
      "          [0.0425, 0.0364, 0.0366,  ..., 0.0534, 0.0534, 0.0534]],\n",
      "\n",
      "         [[0.0749, 0.0421, 0.0616,  ..., 0.0555, 0.0555, 0.0555],\n",
      "          [0.0527, 0.0443, 0.1003,  ..., 0.0354, 0.0354, 0.0354],\n",
      "          [0.0250, 0.0511, 0.0647,  ..., 0.0611, 0.0611, 0.0611],\n",
      "          ...,\n",
      "          [0.0585, 0.0499, 0.0368,  ..., 0.0464, 0.0464, 0.0464],\n",
      "          [0.0585, 0.0499, 0.0368,  ..., 0.0464, 0.0464, 0.0464],\n",
      "          [0.0585, 0.0499, 0.0368,  ..., 0.0464, 0.0464, 0.0464]]],\n",
      "\n",
      "\n",
      "        [[[0.0356, 0.0369, 0.0427,  ..., 0.0544, 0.0544, 0.0544],\n",
      "          [0.0312, 0.0490, 0.0590,  ..., 0.0534, 0.0534, 0.0534],\n",
      "          [0.0740, 0.0313, 0.0493,  ..., 0.0511, 0.0511, 0.0511],\n",
      "          ...,\n",
      "          [0.0369, 0.0354, 0.0328,  ..., 0.0550, 0.0550, 0.0550],\n",
      "          [0.0369, 0.0354, 0.0328,  ..., 0.0550, 0.0550, 0.0550],\n",
      "          [0.0369, 0.0354, 0.0328,  ..., 0.0550, 0.0550, 0.0550]],\n",
      "\n",
      "         [[0.1096, 0.1267, 0.0902,  ..., 0.0375, 0.0375, 0.0375],\n",
      "          [0.0634, 0.0725, 0.0492,  ..., 0.0445, 0.0445, 0.0445],\n",
      "          [0.0441, 0.0460, 0.0267,  ..., 0.0514, 0.0514, 0.0514],\n",
      "          ...,\n",
      "          [0.0520, 0.0499, 0.0570,  ..., 0.0499, 0.0499, 0.0499],\n",
      "          [0.0520, 0.0499, 0.0570,  ..., 0.0499, 0.0499, 0.0499],\n",
      "          [0.0520, 0.0499, 0.0570,  ..., 0.0499, 0.0499, 0.0499]],\n",
      "\n",
      "         [[0.0236, 0.0196, 0.0533,  ..., 0.0564, 0.0564, 0.0564],\n",
      "          [0.0541, 0.0247, 0.0371,  ..., 0.0515, 0.0515, 0.0515],\n",
      "          [0.0194, 0.0389, 0.0169,  ..., 0.0572, 0.0572, 0.0572],\n",
      "          ...,\n",
      "          [0.0279, 0.0510, 0.1255,  ..., 0.0438, 0.0438, 0.0438],\n",
      "          [0.0279, 0.0510, 0.1255,  ..., 0.0438, 0.0438, 0.0438],\n",
      "          [0.0279, 0.0510, 0.1255,  ..., 0.0438, 0.0438, 0.0438]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0736, 0.0633, 0.0753,  ..., 0.0432, 0.0432, 0.0432],\n",
      "          [0.0686, 0.0970, 0.0591,  ..., 0.0425, 0.0425, 0.0425],\n",
      "          [0.0335, 0.0568, 0.0431,  ..., 0.0518, 0.0518, 0.0518],\n",
      "          ...,\n",
      "          [0.0508, 0.0283, 0.0695,  ..., 0.0537, 0.0537, 0.0537],\n",
      "          [0.0508, 0.0283, 0.0695,  ..., 0.0537, 0.0537, 0.0537],\n",
      "          [0.0508, 0.0283, 0.0695,  ..., 0.0537, 0.0537, 0.0537]],\n",
      "\n",
      "         [[0.0448, 0.0387, 0.0463,  ..., 0.0510, 0.0510, 0.0510],\n",
      "          [0.0329, 0.0536, 0.0453,  ..., 0.0511, 0.0511, 0.0511],\n",
      "          [0.0318, 0.0609, 0.0393,  ..., 0.0504, 0.0504, 0.0504],\n",
      "          ...,\n",
      "          [0.0400, 0.0660, 0.0471,  ..., 0.0501, 0.0501, 0.0501],\n",
      "          [0.0400, 0.0660, 0.0471,  ..., 0.0501, 0.0501, 0.0501],\n",
      "          [0.0400, 0.0660, 0.0471,  ..., 0.0501, 0.0501, 0.0501]],\n",
      "\n",
      "         [[0.0205, 0.0444, 0.0191,  ..., 0.0587, 0.0587, 0.0587],\n",
      "          [0.0444, 0.0500, 0.0579,  ..., 0.0496, 0.0496, 0.0496],\n",
      "          [0.0361, 0.0385, 0.0515,  ..., 0.0524, 0.0524, 0.0524],\n",
      "          ...,\n",
      "          [0.0376, 0.0823, 0.0645,  ..., 0.0494, 0.0494, 0.0494],\n",
      "          [0.0376, 0.0823, 0.0645,  ..., 0.0494, 0.0494, 0.0494],\n",
      "          [0.0376, 0.0823, 0.0645,  ..., 0.0494, 0.0494, 0.0494]]],\n",
      "\n",
      "\n",
      "        [[[0.0433, 0.0490, 0.0527,  ..., 0.0579, 0.0579, 0.0579],\n",
      "          [0.0307, 0.0322, 0.0660,  ..., 0.0593, 0.0593, 0.0593],\n",
      "          [0.0480, 0.0563, 0.0671,  ..., 0.0287, 0.0287, 0.0287],\n",
      "          ...,\n",
      "          [0.0375, 0.0298, 0.0379,  ..., 0.0579, 0.0579, 0.0579],\n",
      "          [0.0375, 0.0298, 0.0379,  ..., 0.0579, 0.0579, 0.0579],\n",
      "          [0.0375, 0.0298, 0.0379,  ..., 0.0579, 0.0579, 0.0579]],\n",
      "\n",
      "         [[0.0492, 0.0441, 0.0452,  ..., 0.0555, 0.0555, 0.0555],\n",
      "          [0.0379, 0.0653, 0.0493,  ..., 0.0395, 0.0395, 0.0395],\n",
      "          [0.0394, 0.0703, 0.0578,  ..., 0.0584, 0.0584, 0.0584],\n",
      "          ...,\n",
      "          [0.0564, 0.0645, 0.0455,  ..., 0.0493, 0.0493, 0.0493],\n",
      "          [0.0564, 0.0645, 0.0455,  ..., 0.0493, 0.0493, 0.0493],\n",
      "          [0.0564, 0.0645, 0.0455,  ..., 0.0493, 0.0493, 0.0493]],\n",
      "\n",
      "         [[0.0502, 0.1050, 0.0518,  ..., 0.0308, 0.0308, 0.0308],\n",
      "          [0.0653, 0.0380, 0.0505,  ..., 0.0465, 0.0465, 0.0465],\n",
      "          [0.0480, 0.0265, 0.0493,  ..., 0.0521, 0.0521, 0.0521],\n",
      "          ...,\n",
      "          [0.0457, 0.0391, 0.0719,  ..., 0.0426, 0.0426, 0.0426],\n",
      "          [0.0457, 0.0391, 0.0719,  ..., 0.0426, 0.0426, 0.0426],\n",
      "          [0.0457, 0.0391, 0.0719,  ..., 0.0426, 0.0426, 0.0426]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0395, 0.0514, 0.0494,  ..., 0.0563, 0.0563, 0.0563],\n",
      "          [0.0510, 0.0327, 0.0476,  ..., 0.0424, 0.0424, 0.0424],\n",
      "          [0.0533, 0.0693, 0.0390,  ..., 0.0561, 0.0561, 0.0561],\n",
      "          ...,\n",
      "          [0.0354, 0.0304, 0.0230,  ..., 0.0572, 0.0572, 0.0572],\n",
      "          [0.0354, 0.0304, 0.0230,  ..., 0.0572, 0.0572, 0.0572],\n",
      "          [0.0354, 0.0304, 0.0230,  ..., 0.0572, 0.0572, 0.0572]],\n",
      "\n",
      "         [[0.0799, 0.0422, 0.0404,  ..., 0.0453, 0.0453, 0.0453],\n",
      "          [0.0400, 0.0839, 0.0502,  ..., 0.0441, 0.0441, 0.0441],\n",
      "          [0.0640, 0.0866, 0.0428,  ..., 0.0339, 0.0339, 0.0339],\n",
      "          ...,\n",
      "          [0.0565, 0.0444, 0.0655,  ..., 0.0433, 0.0433, 0.0433],\n",
      "          [0.0565, 0.0444, 0.0655,  ..., 0.0433, 0.0433, 0.0433],\n",
      "          [0.0565, 0.0444, 0.0655,  ..., 0.0433, 0.0433, 0.0433]],\n",
      "\n",
      "         [[0.0438, 0.0511, 0.0471,  ..., 0.0551, 0.0551, 0.0551],\n",
      "          [0.0648, 0.0567, 0.0390,  ..., 0.0470, 0.0470, 0.0470],\n",
      "          [0.0494, 0.0428, 0.0474,  ..., 0.0543, 0.0543, 0.0543],\n",
      "          ...,\n",
      "          [0.0472, 0.0360, 0.0930,  ..., 0.0467, 0.0467, 0.0467],\n",
      "          [0.0472, 0.0360, 0.0930,  ..., 0.0467, 0.0467, 0.0467],\n",
      "          [0.0472, 0.0360, 0.0930,  ..., 0.0467, 0.0467, 0.0467]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0454, 0.0592, 0.0409,  ..., 0.0330, 0.0529, 0.0460],\n",
      "          [0.0433, 0.0733, 0.0530,  ..., 0.0499, 0.0445, 0.0317],\n",
      "          [0.0368, 0.0270, 0.0506,  ..., 0.0869, 0.0355, 0.0689],\n",
      "          ...,\n",
      "          [0.0505, 0.0262, 0.0490,  ..., 0.0416, 0.0551, 0.0762],\n",
      "          [0.0495, 0.0466, 0.0314,  ..., 0.0372, 0.0362, 0.0654],\n",
      "          [0.0724, 0.0475, 0.0425,  ..., 0.0508, 0.0447, 0.0618]],\n",
      "\n",
      "         [[0.0424, 0.0466, 0.0303,  ..., 0.0729, 0.0469, 0.0572],\n",
      "          [0.0922, 0.0529, 0.0225,  ..., 0.0332, 0.0432, 0.0792],\n",
      "          [0.0472, 0.0316, 0.0338,  ..., 0.0314, 0.0379, 0.0523],\n",
      "          ...,\n",
      "          [0.0482, 0.0739, 0.0325,  ..., 0.0369, 0.0536, 0.0494],\n",
      "          [0.0359, 0.0436, 0.0279,  ..., 0.0850, 0.0694, 0.0495],\n",
      "          [0.0339, 0.0753, 0.0508,  ..., 0.0469, 0.0305, 0.0689]],\n",
      "\n",
      "         [[0.0650, 0.0293, 0.0414,  ..., 0.0589, 0.0539, 0.0324],\n",
      "          [0.0683, 0.0537, 0.0412,  ..., 0.0514, 0.0495, 0.0397],\n",
      "          [0.0346, 0.0673, 0.0336,  ..., 0.0536, 0.0805, 0.0534],\n",
      "          ...,\n",
      "          [0.0403, 0.0396, 0.0232,  ..., 0.0343, 0.0345, 0.0416],\n",
      "          [0.0625, 0.0466, 0.0497,  ..., 0.0201, 0.0419, 0.0561],\n",
      "          [0.0585, 0.0260, 0.0715,  ..., 0.0280, 0.0421, 0.0581]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0335, 0.0627, 0.0480,  ..., 0.0381, 0.0781, 0.0409],\n",
      "          [0.0705, 0.0327, 0.0665,  ..., 0.0320, 0.0678, 0.0621],\n",
      "          [0.0650, 0.0702, 0.0424,  ..., 0.0256, 0.0243, 0.0378],\n",
      "          ...,\n",
      "          [0.0473, 0.0301, 0.0475,  ..., 0.0455, 0.0433, 0.0545],\n",
      "          [0.0409, 0.0483, 0.0450,  ..., 0.0620, 0.0927, 0.0314],\n",
      "          [0.0157, 0.0332, 0.1258,  ..., 0.0537, 0.0345, 0.0576]],\n",
      "\n",
      "         [[0.0741, 0.0415, 0.0665,  ..., 0.0402, 0.0348, 0.0523],\n",
      "          [0.0333, 0.0333, 0.0597,  ..., 0.0417, 0.0218, 0.0392],\n",
      "          [0.0436, 0.0391, 0.0443,  ..., 0.0485, 0.0682, 0.0585],\n",
      "          ...,\n",
      "          [0.0512, 0.0364, 0.0567,  ..., 0.0549, 0.0422, 0.0524],\n",
      "          [0.0436, 0.0409, 0.0548,  ..., 0.0514, 0.0496, 0.0578],\n",
      "          [0.0408, 0.0640, 0.0365,  ..., 0.0713, 0.0506, 0.0389]],\n",
      "\n",
      "         [[0.0387, 0.0321, 0.0364,  ..., 0.0570, 0.0501, 0.0479],\n",
      "          [0.0578, 0.0999, 0.0317,  ..., 0.0453, 0.0483, 0.0607],\n",
      "          [0.0561, 0.0734, 0.0613,  ..., 0.0623, 0.0685, 0.0485],\n",
      "          ...,\n",
      "          [0.0455, 0.0513, 0.0518,  ..., 0.0460, 0.0339, 0.0669],\n",
      "          [0.0769, 0.0621, 0.0439,  ..., 0.0506, 0.0883, 0.0213],\n",
      "          [0.0504, 0.0325, 0.0438,  ..., 0.0657, 0.0450, 0.0310]]],\n",
      "\n",
      "\n",
      "        [[[0.0509, 0.0403, 0.0306,  ..., 0.0452, 0.0452, 0.0452],\n",
      "          [0.0424, 0.0539, 0.0372,  ..., 0.0425, 0.0425, 0.0425],\n",
      "          [0.0260, 0.0336, 0.0672,  ..., 0.0247, 0.0247, 0.0247],\n",
      "          ...,\n",
      "          [0.0435, 0.0426, 0.0461,  ..., 0.0638, 0.0638, 0.0638],\n",
      "          [0.0435, 0.0426, 0.0461,  ..., 0.0638, 0.0638, 0.0638],\n",
      "          [0.0435, 0.0426, 0.0461,  ..., 0.0638, 0.0638, 0.0638]],\n",
      "\n",
      "         [[0.0531, 0.0296, 0.0645,  ..., 0.0589, 0.0589, 0.0589],\n",
      "          [0.0352, 0.0283, 0.0620,  ..., 0.0292, 0.0292, 0.0292],\n",
      "          [0.0409, 0.0271, 0.0630,  ..., 0.0397, 0.0397, 0.0397],\n",
      "          ...,\n",
      "          [0.0486, 0.0498, 0.0664,  ..., 0.0524, 0.0524, 0.0524],\n",
      "          [0.0486, 0.0498, 0.0664,  ..., 0.0524, 0.0524, 0.0524],\n",
      "          [0.0486, 0.0498, 0.0664,  ..., 0.0524, 0.0524, 0.0524]],\n",
      "\n",
      "         [[0.0671, 0.0644, 0.0461,  ..., 0.0382, 0.0382, 0.0382],\n",
      "          [0.0500, 0.0629, 0.0665,  ..., 0.0487, 0.0487, 0.0487],\n",
      "          [0.0761, 0.0527, 0.0643,  ..., 0.0341, 0.0341, 0.0341],\n",
      "          ...,\n",
      "          [0.0612, 0.0607, 0.0527,  ..., 0.0402, 0.0402, 0.0402],\n",
      "          [0.0612, 0.0607, 0.0527,  ..., 0.0402, 0.0402, 0.0402],\n",
      "          [0.0612, 0.0607, 0.0527,  ..., 0.0402, 0.0402, 0.0402]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0266, 0.0271, 0.0346,  ..., 0.0523, 0.0523, 0.0523],\n",
      "          [0.0409, 0.0349, 0.0391,  ..., 0.0532, 0.0532, 0.0532],\n",
      "          [0.0370, 0.0629, 0.0444,  ..., 0.0484, 0.0484, 0.0484],\n",
      "          ...,\n",
      "          [0.0346, 0.0750, 0.0422,  ..., 0.0593, 0.0593, 0.0593],\n",
      "          [0.0346, 0.0750, 0.0422,  ..., 0.0593, 0.0593, 0.0593],\n",
      "          [0.0346, 0.0750, 0.0422,  ..., 0.0593, 0.0593, 0.0593]],\n",
      "\n",
      "         [[0.0657, 0.0631, 0.0392,  ..., 0.0313, 0.0313, 0.0313],\n",
      "          [0.0383, 0.0752, 0.0553,  ..., 0.0454, 0.0454, 0.0454],\n",
      "          [0.0660, 0.0338, 0.0528,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          ...,\n",
      "          [0.0576, 0.0310, 0.0934,  ..., 0.0450, 0.0450, 0.0450],\n",
      "          [0.0576, 0.0310, 0.0934,  ..., 0.0450, 0.0450, 0.0450],\n",
      "          [0.0576, 0.0310, 0.0934,  ..., 0.0450, 0.0450, 0.0450]],\n",
      "\n",
      "         [[0.0456, 0.0498, 0.0560,  ..., 0.0552, 0.0552, 0.0552],\n",
      "          [0.0553, 0.0399, 0.0592,  ..., 0.0541, 0.0541, 0.0541],\n",
      "          [0.0869, 0.0460, 0.0316,  ..., 0.0551, 0.0551, 0.0551],\n",
      "          ...,\n",
      "          [0.0977, 0.0466, 0.0374,  ..., 0.0430, 0.0430, 0.0430],\n",
      "          [0.0977, 0.0466, 0.0374,  ..., 0.0430, 0.0430, 0.0430],\n",
      "          [0.0977, 0.0466, 0.0374,  ..., 0.0430, 0.0430, 0.0430]]],\n",
      "\n",
      "\n",
      "        [[[0.0562, 0.0514, 0.0439,  ..., 0.0495, 0.0495, 0.0495],\n",
      "          [0.0623, 0.0597, 0.0538,  ..., 0.0471, 0.0471, 0.0471],\n",
      "          [0.0335, 0.0534, 0.0549,  ..., 0.0594, 0.0594, 0.0594],\n",
      "          ...,\n",
      "          [0.0551, 0.0423, 0.0364,  ..., 0.0633, 0.0633, 0.0633],\n",
      "          [0.0551, 0.0423, 0.0364,  ..., 0.0633, 0.0633, 0.0633],\n",
      "          [0.0551, 0.0423, 0.0364,  ..., 0.0633, 0.0633, 0.0633]],\n",
      "\n",
      "         [[0.0311, 0.0413, 0.0319,  ..., 0.0557, 0.0557, 0.0557],\n",
      "          [0.0408, 0.0319, 0.0610,  ..., 0.0328, 0.0328, 0.0328],\n",
      "          [0.0257, 0.0589, 0.0417,  ..., 0.0419, 0.0419, 0.0419],\n",
      "          ...,\n",
      "          [0.0579, 0.0481, 0.0727,  ..., 0.0506, 0.0506, 0.0506],\n",
      "          [0.0579, 0.0481, 0.0727,  ..., 0.0506, 0.0506, 0.0506],\n",
      "          [0.0579, 0.0481, 0.0727,  ..., 0.0506, 0.0506, 0.0506]],\n",
      "\n",
      "         [[0.0469, 0.0546, 0.0618,  ..., 0.0493, 0.0493, 0.0493],\n",
      "          [0.0800, 0.0622, 0.0573,  ..., 0.0482, 0.0482, 0.0482],\n",
      "          [0.0526, 0.0263, 0.0717,  ..., 0.0436, 0.0436, 0.0436],\n",
      "          ...,\n",
      "          [0.0718, 0.0627, 0.0384,  ..., 0.0415, 0.0415, 0.0415],\n",
      "          [0.0718, 0.0627, 0.0384,  ..., 0.0415, 0.0415, 0.0415],\n",
      "          [0.0718, 0.0627, 0.0384,  ..., 0.0415, 0.0415, 0.0415]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0487, 0.0397, 0.1104,  ..., 0.0368, 0.0368, 0.0368],\n",
      "          [0.0380, 0.0367, 0.0375,  ..., 0.0561, 0.0561, 0.0561],\n",
      "          [0.0416, 0.0453, 0.0483,  ..., 0.0526, 0.0526, 0.0526],\n",
      "          ...,\n",
      "          [0.0299, 0.0756, 0.0241,  ..., 0.0598, 0.0598, 0.0598],\n",
      "          [0.0299, 0.0756, 0.0241,  ..., 0.0598, 0.0598, 0.0598],\n",
      "          [0.0299, 0.0756, 0.0241,  ..., 0.0598, 0.0598, 0.0598]],\n",
      "\n",
      "         [[0.0535, 0.0514, 0.0658,  ..., 0.0487, 0.0487, 0.0487],\n",
      "          [0.0693, 0.0693, 0.0403,  ..., 0.0419, 0.0419, 0.0419],\n",
      "          [0.0511, 0.0397, 0.0529,  ..., 0.0441, 0.0441, 0.0441],\n",
      "          ...,\n",
      "          [0.0519, 0.0334, 0.0578,  ..., 0.0484, 0.0484, 0.0484],\n",
      "          [0.0519, 0.0334, 0.0578,  ..., 0.0484, 0.0484, 0.0484],\n",
      "          [0.0519, 0.0334, 0.0578,  ..., 0.0484, 0.0484, 0.0484]],\n",
      "\n",
      "         [[0.0456, 0.0429, 0.0445,  ..., 0.0421, 0.0421, 0.0421],\n",
      "          [0.0328, 0.0425, 0.0746,  ..., 0.0576, 0.0576, 0.0576],\n",
      "          [0.0529, 0.0482, 0.1411,  ..., 0.0340, 0.0340, 0.0340],\n",
      "          ...,\n",
      "          [0.0434, 0.0537, 0.0291,  ..., 0.0495, 0.0495, 0.0495],\n",
      "          [0.0434, 0.0537, 0.0291,  ..., 0.0495, 0.0495, 0.0495],\n",
      "          [0.0434, 0.0537, 0.0291,  ..., 0.0495, 0.0495, 0.0495]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([10, 8, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "# shape - (L, L)\n",
    "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "# softmax - row-wise이기 때문에 dim은 -1\n",
    "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "print(attn_dists)\n",
    "print(attn_dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7megouWpgCck",
    "outputId": "f8d3f539-fb79-4cde-abb1-6476b86cf81f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmSTaymdg-P_"
   },
   "source": [
    "### 각 head의 결과물 병합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSdQZCk0hCNd"
   },
   "source": [
    "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eaK0bpMGhQZ2",
    "outputId": "45f62395-8ff4-4f0b-b407-f484f2718361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
    "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "id": "LTng_2SXhdH1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0537,  0.0705,  0.0409,  ..., -0.0230,  0.0024,  0.0823],\n",
      "         [ 0.0784,  0.0946,  0.0026,  ..., -0.0569,  0.0832,  0.0862],\n",
      "         [ 0.1182,  0.0166, -0.0293,  ..., -0.0633,  0.0754,  0.0926],\n",
      "         ...,\n",
      "         [ 0.0986,  0.0578, -0.0086,  ...,  0.0233,  0.0402,  0.1176],\n",
      "         [ 0.0986,  0.0578, -0.0086,  ...,  0.0233,  0.0402,  0.1176],\n",
      "         [ 0.0986,  0.0578, -0.0086,  ...,  0.0233,  0.0402,  0.1176]],\n",
      "\n",
      "        [[ 0.1813,  0.0323, -0.1066,  ..., -0.0165, -0.2181,  0.2451],\n",
      "         [ 0.1427,  0.0957, -0.1201,  ..., -0.0217, -0.1856,  0.2232],\n",
      "         [ 0.1673,  0.0554, -0.0820,  ...,  0.0168, -0.1691,  0.1937],\n",
      "         ...,\n",
      "         [ 0.1713,  0.0637, -0.1153,  ...,  0.0086, -0.1351,  0.1935],\n",
      "         [ 0.1713,  0.0637, -0.1153,  ...,  0.0086, -0.1351,  0.1935],\n",
      "         [ 0.1713,  0.0637, -0.1153,  ...,  0.0086, -0.1351,  0.1935]],\n",
      "\n",
      "        [[ 0.0257, -0.0709, -0.0801,  ...,  0.0526, -0.0953,  0.1171],\n",
      "         [ 0.0005, -0.0569, -0.0840,  ..., -0.0055, -0.1746,  0.1564],\n",
      "         [ 0.0266, -0.0973, -0.1270,  ..., -0.0546,  0.0502,  0.1023],\n",
      "         ...,\n",
      "         [ 0.0009, -0.0679, -0.0915,  ...,  0.0011, -0.1121,  0.1379],\n",
      "         [ 0.0009, -0.0679, -0.0915,  ...,  0.0011, -0.1121,  0.1379],\n",
      "         [ 0.0009, -0.0679, -0.0915,  ...,  0.0011, -0.1121,  0.1379]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0472,  0.0246, -0.0220,  ..., -0.0638, -0.0006, -0.0580],\n",
      "         [-0.0926,  0.0409, -0.0939,  ..., -0.0580,  0.0259, -0.0610],\n",
      "         [-0.0837,  0.0150, -0.0622,  ..., -0.0950, -0.0081, -0.0694],\n",
      "         ...,\n",
      "         [-0.0936,  0.0809, -0.0928,  ..., -0.0815, -0.0080, -0.0350],\n",
      "         [-0.0454,  0.0293, -0.0885,  ..., -0.0491,  0.0007, -0.0141],\n",
      "         [-0.0974,  0.0106, -0.0930,  ..., -0.0316,  0.0043, -0.0380]],\n",
      "\n",
      "        [[ 0.1545,  0.1261,  0.1307,  ..., -0.0876, -0.0155,  0.1149],\n",
      "         [ 0.1419,  0.1732,  0.1811,  ..., -0.0653, -0.0074,  0.0676],\n",
      "         [ 0.1565,  0.1502,  0.1526,  ..., -0.0851,  0.0611,  0.0673],\n",
      "         ...,\n",
      "         [ 0.1656,  0.0951,  0.1094,  ..., -0.0639, -0.0190,  0.0766],\n",
      "         [ 0.1656,  0.0951,  0.1094,  ..., -0.0639, -0.0190,  0.0766],\n",
      "         [ 0.1656,  0.0951,  0.1094,  ..., -0.0639, -0.0190,  0.0766]],\n",
      "\n",
      "        [[ 0.0672,  0.0756,  0.0168,  ..., -0.0215, -0.0602,  0.1314],\n",
      "         [ 0.0489,  0.0521,  0.0849,  ..., -0.0332, -0.0253,  0.1053],\n",
      "         [ 0.0887,  0.0351,  0.0721,  ..., -0.0370, -0.0485,  0.1675],\n",
      "         ...,\n",
      "         [ 0.0899,  0.0574,  0.0395,  ..., -0.0293, -0.0677,  0.0818],\n",
      "         [ 0.0899,  0.0574,  0.0395,  ..., -0.0293, -0.0677,  0.0818],\n",
      "         [ 0.0899,  0.0574,  0.0395,  ..., -0.0293, -0.0677,  0.0818]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "# w_0 : (d_model, d_model)\n",
    "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
    "outputs = w_0(attn_values)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goX70VKqhxQH"
   },
   "source": [
    "## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtNyV7mMj7V_"
   },
   "source": [
    "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
    "\n",
    "아래 코드의 TODO 부분을 채워주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "U_kNhOTrkBHm"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MultiheadAttention, self).__init__()\n",
    "\n",
    "    # Q, K, V learnable matrices\n",
    "    self.w_q = nn.Linear(d_model, d_model)\n",
    "    self.w_k = nn.Linear(d_model, d_model)\n",
    "    self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # Linear projection for concatenated outputs\n",
    "    self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "  # scaled-dot product attention\n",
    "  def self_attention(self, q, k, v):\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "    return attn_values\n",
    "\n",
    "  def forward(self, q, k, v):\n",
    "    batch_size = q.shape[0]\n",
    "\n",
    "    # linear projection\n",
    "    ################################################################################\n",
    "    # TODO 1: Implement the forward pass for linear projection.                #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    q=self.w_q(q)\n",
    "    k=self.w_k(k)\n",
    "    v=self.w_v(v)\n",
    "    \n",
    "    # head만큼 쪼개준다\n",
    "    ################################################################################\n",
    "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    q=q.view(batch_size,-1,num_heads,d_k)\n",
    "    k=k.view(batch_size,-1,num_heads,d_k)\n",
    "    v=v.view(batch_size,-1,num_heads,d_k)\n",
    "    \n",
    "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
    "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
    "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
    "\n",
    "    return self.w_0(attn_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jYLuu_9alQxT"
   },
   "outputs": [],
   "source": [
    "multihead_attn = MultiheadAttention()\n",
    "\n",
    "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "id": "KMiXlYjSlTfB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.6090e-01, -1.6496e-03,  9.4370e-02,  ..., -6.2381e-02,\n",
      "           1.1788e-01, -1.7284e-01],\n",
      "         [-1.4018e-01, -6.1340e-02,  1.6322e-01,  ..., -2.3620e-01,\n",
      "           1.3613e-02,  5.0560e-02],\n",
      "         [-1.9657e-01,  2.6002e-02,  7.0686e-02,  ..., -3.6916e-02,\n",
      "           5.6904e-02, -1.0916e-01],\n",
      "         ...,\n",
      "         [-1.7990e-01,  2.4116e-01,  2.4578e-01,  ..., -6.7102e-02,\n",
      "           1.1746e-01, -3.7070e-02],\n",
      "         [-1.7990e-01,  2.4116e-01,  2.4578e-01,  ..., -6.7102e-02,\n",
      "           1.1746e-01, -3.7070e-02],\n",
      "         [-1.7990e-01,  2.4116e-01,  2.4578e-01,  ..., -6.7102e-02,\n",
      "           1.1746e-01, -3.7070e-02]],\n",
      "\n",
      "        [[-1.5282e-01,  4.1749e-01,  1.2376e-01,  ..., -1.4899e-01,\n",
      "          -1.0264e-01,  6.8223e-02],\n",
      "         [-7.5518e-02,  1.4284e-01,  5.3329e-02,  ..., -2.3309e-02,\n",
      "          -5.6342e-02,  9.0262e-02],\n",
      "         [ 8.7592e-03,  2.9211e-02,  2.8965e-02,  ...,  4.5807e-03,\n",
      "           9.0758e-02,  1.0392e-02],\n",
      "         ...,\n",
      "         [-6.5287e-01,  7.6479e-01,  4.9813e-01,  ...,  2.1139e-01,\n",
      "           5.2633e-01, -2.9189e-01],\n",
      "         [-6.5287e-01,  7.6479e-01,  4.9813e-01,  ...,  2.1139e-01,\n",
      "           5.2633e-01, -2.9189e-01],\n",
      "         [-6.5287e-01,  7.6479e-01,  4.9813e-01,  ...,  2.1139e-01,\n",
      "           5.2633e-01, -2.9189e-01]],\n",
      "\n",
      "        [[-3.9668e-02, -2.8867e-02, -3.5017e-02,  ...,  2.6712e-02,\n",
      "          -7.8249e-02,  4.0166e-03],\n",
      "         [-1.4245e-01,  6.3365e-02,  4.9399e-02,  ..., -1.2842e-01,\n",
      "           4.0886e-02, -5.2964e-02],\n",
      "         [ 2.4275e-01, -6.1751e-02, -1.2972e-01,  ..., -5.3176e-02,\n",
      "          -9.5203e-02,  5.6079e-02],\n",
      "         ...,\n",
      "         [-4.5952e-01,  5.1225e-01,  4.5136e-01,  ..., -6.5760e-03,\n",
      "           3.6226e-01, -1.4808e-01],\n",
      "         [-4.5952e-01,  5.1225e-01,  4.5136e-01,  ..., -6.5760e-03,\n",
      "           3.6226e-01, -1.4808e-01],\n",
      "         [-4.5952e-01,  5.1225e-01,  4.5136e-01,  ..., -6.5760e-03,\n",
      "           3.6226e-01, -1.4808e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.2424e-02, -3.1690e-01,  1.1296e-01,  ..., -5.8765e-02,\n",
      "           3.2738e-02, -2.8023e-02],\n",
      "         [-1.9933e-02, -7.8946e-02, -2.4405e-02,  ..., -6.0310e-02,\n",
      "          -9.0699e-02, -1.7741e-01],\n",
      "         [ 3.4028e-02,  1.0152e-01,  4.4599e-02,  ..., -1.7129e-02,\n",
      "           1.5314e-02, -4.5748e-02],\n",
      "         ...,\n",
      "         [ 2.0218e-01,  4.8696e-02, -2.6346e-02,  ..., -6.0607e-02,\n",
      "           2.8454e-02,  6.4008e-02],\n",
      "         [ 4.4939e-02,  9.8676e-02,  8.6028e-02,  ..., -6.0874e-02,\n",
      "           1.5413e-02, -7.9301e-02],\n",
      "         [ 1.1947e-01, -3.6873e-02,  1.6032e-01,  ..., -2.0917e-01,\n",
      "          -1.0654e-01, -1.7354e-01]],\n",
      "\n",
      "        [[-9.8385e-02, -2.0751e-02,  2.6113e-02,  ..., -9.2619e-02,\n",
      "          -5.7266e-02, -3.8685e-02],\n",
      "         [-9.6920e-02, -2.1479e-04,  1.5720e-01,  ...,  1.1538e-01,\n",
      "          -4.6061e-03,  1.0371e-01],\n",
      "         [ 9.5929e-02, -4.3744e-02, -1.4674e-02,  ..., -5.2178e-02,\n",
      "          -6.1113e-02, -2.7811e-03],\n",
      "         ...,\n",
      "         [-2.4090e-01,  2.4069e-01,  2.9166e-01,  ..., -3.6223e-02,\n",
      "           1.1684e-01, -8.2030e-02],\n",
      "         [-2.4090e-01,  2.4069e-01,  2.9166e-01,  ..., -3.6223e-02,\n",
      "           1.1684e-01, -8.2030e-02],\n",
      "         [-2.4090e-01,  2.4069e-01,  2.9166e-01,  ..., -3.6223e-02,\n",
      "           1.1684e-01, -8.2030e-02]],\n",
      "\n",
      "        [[ 1.1283e-01, -1.6866e-02,  1.8660e-02,  ..., -1.1430e-02,\n",
      "           4.6976e-02, -1.8866e-02],\n",
      "         [-9.5419e-02, -1.9521e-03,  1.3765e-01,  ...,  1.1852e-01,\n",
      "           4.4243e-02,  1.1236e-01],\n",
      "         [-8.6708e-02,  1.1543e-01,  2.6880e-02,  ..., -4.8534e-02,\n",
      "          -5.1111e-02,  3.9340e-02],\n",
      "         ...,\n",
      "         [-3.1769e-01,  3.6324e-01,  3.6921e-01,  ..., -3.0611e-02,\n",
      "           2.1133e-01, -9.3086e-02],\n",
      "         [-3.1769e-01,  3.6324e-01,  3.6921e-01,  ..., -3.0611e-02,\n",
      "           2.1133e-01, -9.3086e-02],\n",
      "         [-3.1769e-01,  3.6324e-01,  3.6921e-01,  ..., -3.0611e-02,\n",
      "           2.1133e-01, -9.3086e-02]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)\n",
    "print(outputs.shape)  # (batch_size, length, d_model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1phZ7bP-RrVIHU9AG5fTPTo2TMbZ-EMac",
     "timestamp": 1662306889489
    },
    {
     "file_id": "1XNOcAyN3Q9KN6-9oA1t6Ueh37tJYQFGa",
     "timestamp": 1658069359050
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
