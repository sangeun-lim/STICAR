{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Transformer"],"metadata":{"id":"GiWulKAdmAHX"}},{"cell_type":"markdown","metadata":{"id":"9KsBGZpKkWki"},"source":["Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n","1. Multi-head attention 및 self-attention 구현.\n","2. 각 과정에서 일어나는 연산과 input/output 형태 이해."]},{"cell_type":"markdown","metadata":{"id":"8qRU5DFY2OM8"},"source":["### 필요 패키지 import"]},{"cell_type":"code","metadata":{"id":"lDtMioSQQ1bB","executionInfo":{"status":"ok","timestamp":1662226911708,"user_tz":-540,"elapsed":1356,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["from torch import nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","\n","import torch\n","import math"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Req. 2-1 Multi-head self-attention 구조 익히기"],"metadata":{"id":"HH0VdC4uJJVG"}},{"cell_type":"markdown","metadata":{"id":"QBiZObgRep_Q"},"source":["### **데이터 전처리**\n","vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n","\n","각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."]},{"cell_type":"code","metadata":{"id":"e9ULZIqTenSc","executionInfo":{"status":"ok","timestamp":1662226937540,"user_tz":-540,"elapsed":302,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["pad_id = 0\n","vocab_size = 100\n","\n","data = [\n","  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n","  [60, 96, 51, 32, 90],\n","  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n","  [75, 51],\n","  [66, 88, 98, 47],\n","  [21, 39, 10, 64, 21],\n","  [98],\n","  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n","  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n","  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n","]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Hx3mcivgMyH","executionInfo":{"status":"ok","timestamp":1662226939957,"user_tz":-540,"elapsed":2,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["# 길이 맞춰주기 위해 패딩합니다.\n","def padding(data):\n","  max_len = len(max(data, key=len))\n","  print(f\"Maximum sequence length: {max_len}\")\n","\n","  for i, seq in enumerate(tqdm(data)):\n","    if len(seq) < max_len:\n","      data[i] = seq + [pad_id] * (max_len - len(seq))\n","\n","  return data, max_len"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3e8FiNvgX60","colab":{"base_uri":"https://localhost:8080/"},"outputId":"83668efc-bc34-447c-c24b-fde784a63eed","executionInfo":{"status":"ok","timestamp":1662226942676,"user_tz":-540,"elapsed":252,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["data, max_len = padding(data)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 20\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 79437.58it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"hwPSIWYugaN0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662226945380,"user_tz":-540,"elapsed":286,"user":{"displayName":"MA PI","userId":"00308135229573784967"}},"outputId":"f491412f-ac6c-4fd6-f90d-c95969814589"},"source":["data"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n"," [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," [77,\n","  65,\n","  51,\n","  77,\n","  19,\n","  15,\n","  35,\n","  19,\n","  23,\n","  97,\n","  50,\n","  46,\n","  53,\n","  42,\n","  45,\n","  91,\n","  66,\n","  3,\n","  43,\n","  10],\n"," [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n"," [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"rwqjACx8iidc"},"source":["### Hyperparameter 세팅 및 embedding"]},{"cell_type":"code","metadata":{"id":"p-Ngp2nWimS8","executionInfo":{"status":"ok","timestamp":1662226949509,"user_tz":-540,"elapsed":271,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["d_model = 512  # model의 hidden size\n","num_heads = 8  # head의 개수\n","\n","# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJMi2Xsni5uq","executionInfo":{"status":"ok","timestamp":1662226952976,"user_tz":-540,"elapsed":265,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["embedding = nn.Embedding(vocab_size, d_model)\n","\n","# B: batch size, L: maximum sequence length\n","batch = torch.LongTensor(data)  # (B, L)\n","batch_emb = embedding(batch)  # (B, L, d_model)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tLCUQwojcUb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662226954883,"user_tz":-540,"elapsed":264,"user":{"displayName":"MA PI","userId":"00308135229573784967"}},"outputId":"ebe17525-c4db-4add-dda7-baa17128490d"},"source":["print(batch_emb)\n","print(batch_emb.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.7367,  0.7097,  1.7420,  ..., -0.6935,  0.6663,  0.9574],\n","         [-0.0627, -1.5894, -1.7091,  ...,  0.4887, -0.4114,  0.6533],\n","         [ 1.7523, -1.3457,  0.6836,  ...,  0.0589,  0.1447,  0.8339],\n","         ...,\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755]],\n","\n","        [[-0.7866,  0.6204, -0.2504,  ..., -0.0363,  0.7677,  0.3478],\n","         [-0.3669,  0.0494,  1.9281,  ..., -0.4636,  1.6363, -0.5044],\n","         [-1.6859,  2.1032, -1.2072,  ...,  0.7961,  1.6086,  1.5415],\n","         ...,\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755]],\n","\n","        [[-1.3682,  0.5439, -0.5723,  ...,  1.3878, -1.2619, -0.6299],\n","         [ 0.3999, -0.6068,  0.7600,  ...,  0.0078,  1.1250,  1.4715],\n","         [-0.3773, -0.0371, -0.5946,  ...,  0.1149, -0.2919,  0.3162],\n","         ...,\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755]],\n","\n","        ...,\n","\n","        [[-1.9173,  0.9823, -0.1721,  ...,  0.1588, -1.0074,  1.0792],\n","         [ 1.6689, -0.0518, -0.0510,  ...,  0.6073,  0.1482,  0.2171],\n","         [-1.6859,  2.1032, -1.2072,  ...,  0.7961,  1.6086,  1.5415],\n","         ...,\n","         [-0.7371, -0.8014, -1.6471,  ...,  1.0082,  0.7685, -0.1082],\n","         [ 0.4084, -0.8428, -1.0696,  ..., -0.0157,  1.2383,  1.7490],\n","         [-2.1003,  1.1694,  0.2798,  ...,  0.8598, -1.3379, -1.4926]],\n","\n","        [[ 0.2332,  0.6401,  1.2360,  ...,  0.5939, -0.0229,  0.4536],\n","         [-0.2126,  0.0152, -2.2752,  ..., -1.3455,  1.2505,  1.8110],\n","         [-0.7348,  0.4533, -1.0035,  ...,  1.2346,  0.4028, -0.2028],\n","         ...,\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755]],\n","\n","        [[-0.1339, -0.5087,  0.4632,  ..., -1.3769,  0.1915,  0.9502],\n","         [-0.2126,  0.0152, -2.2752,  ..., -1.3455,  1.2505,  1.8110],\n","         [ 1.3460, -0.5350, -0.6102,  ...,  1.6649,  0.4778,  0.2297],\n","         ...,\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755],\n","         [-0.1690,  1.4675,  0.4294,  ...,  0.4046,  0.8386, -0.8755]]],\n","       grad_fn=<EmbeddingBackward0>)\n","torch.Size([10, 20, 512])\n"]}]},{"cell_type":"markdown","metadata":{"id":"s0Lhx892gmi3"},"source":["### Linear projection & 여러 head로 나누기"]},{"cell_type":"markdown","metadata":{"id":"urXMBRnRgqvw"},"source":["Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."]},{"cell_type":"code","metadata":{"id":"9DWKDqgCgfMk","executionInfo":{"status":"ok","timestamp":1662226969944,"user_tz":-540,"elapsed":269,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["w_q = nn.Linear(d_model, d_model)\n","w_k = nn.Linear(d_model, d_model)\n","w_v = nn.Linear(d_model, d_model)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcLuhda7m-Lm","executionInfo":{"status":"ok","timestamp":1662226972765,"user_tz":-540,"elapsed":380,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["w_0 = nn.Linear(d_model, d_model)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-vSL7PwnV6k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"04cd5075-5e62-46c8-d189-ca3a3a8ac6e3","executionInfo":{"status":"ok","timestamp":1662226974650,"user_tz":-540,"elapsed":263,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["q = w_q(batch_emb)  # (B, L, d_model)\n","k = w_k(batch_emb)  # (B, L, d_model)\n","v = w_v(batch_emb)  # (B, L, d_model)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 20, 512])\n","torch.Size([10, 20, 512])\n","torch.Size([10, 20, 512])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Wnvlum-LnF1T"},"source":["Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."]},{"cell_type":"markdown","metadata":{"id":"sXcYLZYvJT_1"},"source":["- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n","- 구현에서는 Wq, Wk, Wv 한 개씩\n","- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"]},{"cell_type":"code","metadata":{"id":"_tiOKAv9nEli","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7479fede-c11f-4aea-ca06-183f9e1822a5","executionInfo":{"status":"ok","timestamp":1662226978380,"user_tz":-540,"elapsed":271,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["batch_size = q.shape[0]\n","d_k = d_model // num_heads\n","\n","# num_heads * d_k로 쪼갠다\n","q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 20, 8, 64])\n","torch.Size([10, 20, 8, 64])\n","torch.Size([10, 20, 8, 64])\n"]}]},{"cell_type":"code","metadata":{"id":"5tNb2isfn5Cx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2249b9df-4eff-45ab-9099-e7b9e82d0544","executionInfo":{"status":"ok","timestamp":1662226981349,"user_tz":-540,"elapsed":397,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["# num_heads를 밖으로 뺌으로써\n","# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n","\n","q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 8, 20, 64])\n","torch.Size([10, 8, 20, 64])\n","torch.Size([10, 8, 20, 64])\n"]}]},{"cell_type":"markdown","metadata":{"id":"NWrDA5_Sofad"},"source":["### Scaled dot-product self-attention 구현"]},{"cell_type":"markdown","metadata":{"id":"w52C4k3Wfl8m"},"source":["각 head에서 실행되는 self-attetion 과정입니다."]},{"cell_type":"code","metadata":{"id":"A5waKr0Hfi2K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662226984107,"user_tz":-540,"elapsed":259,"user":{"displayName":"MA PI","userId":"00308135229573784967"}},"outputId":"ae93927b-7170-47ab-ca6f-e1b08975c591"},"source":["# shape - (L, L)\n","# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n","attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n","# softmax - row-wise이기 때문에 dim은 -1\n","attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","\n","print(attn_dists)\n","print(attn_dists.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[0.0702, 0.0502, 0.0387,  ..., 0.0497, 0.0497, 0.0497],\n","          [0.0532, 0.0500, 0.0477,  ..., 0.0604, 0.0604, 0.0604],\n","          [0.0559, 0.0422, 0.0559,  ..., 0.0459, 0.0459, 0.0459],\n","          ...,\n","          [0.0675, 0.0890, 0.0800,  ..., 0.0404, 0.0404, 0.0404],\n","          [0.0675, 0.0890, 0.0800,  ..., 0.0404, 0.0404, 0.0404],\n","          [0.0675, 0.0890, 0.0800,  ..., 0.0404, 0.0404, 0.0404]],\n","\n","         [[0.0484, 0.0491, 0.0437,  ..., 0.0626, 0.0626, 0.0626],\n","          [0.0702, 0.0328, 0.0384,  ..., 0.0580, 0.0580, 0.0580],\n","          [0.0303, 0.0229, 0.0431,  ..., 0.0600, 0.0600, 0.0600],\n","          ...,\n","          [0.0353, 0.0449, 0.0512,  ..., 0.0530, 0.0530, 0.0530],\n","          [0.0353, 0.0449, 0.0512,  ..., 0.0530, 0.0530, 0.0530],\n","          [0.0353, 0.0449, 0.0512,  ..., 0.0530, 0.0530, 0.0530]],\n","\n","         [[0.0704, 0.0454, 0.0454,  ..., 0.0426, 0.0426, 0.0426],\n","          [0.0232, 0.0596, 0.0782,  ..., 0.0435, 0.0435, 0.0435],\n","          [0.0473, 0.0489, 0.0798,  ..., 0.0361, 0.0361, 0.0361],\n","          ...,\n","          [0.0422, 0.0331, 0.0489,  ..., 0.0749, 0.0749, 0.0749],\n","          [0.0422, 0.0331, 0.0489,  ..., 0.0749, 0.0749, 0.0749],\n","          [0.0422, 0.0331, 0.0489,  ..., 0.0749, 0.0749, 0.0749]],\n","\n","         ...,\n","\n","         [[0.0333, 0.0591, 0.0348,  ..., 0.0626, 0.0626, 0.0626],\n","          [0.0805, 0.0470, 0.0587,  ..., 0.0511, 0.0511, 0.0511],\n","          [0.0653, 0.0682, 0.0906,  ..., 0.0360, 0.0360, 0.0360],\n","          ...,\n","          [0.0509, 0.0424, 0.0335,  ..., 0.0539, 0.0539, 0.0539],\n","          [0.0509, 0.0424, 0.0335,  ..., 0.0539, 0.0539, 0.0539],\n","          [0.0509, 0.0424, 0.0335,  ..., 0.0539, 0.0539, 0.0539]],\n","\n","         [[0.0329, 0.0541, 0.0206,  ..., 0.0628, 0.0628, 0.0628],\n","          [0.0481, 0.0578, 0.0696,  ..., 0.0334, 0.0334, 0.0334],\n","          [0.0302, 0.0541, 0.0443,  ..., 0.0381, 0.0381, 0.0381],\n","          ...,\n","          [0.0513, 0.0444, 0.0400,  ..., 0.0767, 0.0767, 0.0767],\n","          [0.0513, 0.0444, 0.0400,  ..., 0.0767, 0.0767, 0.0767],\n","          [0.0513, 0.0444, 0.0400,  ..., 0.0767, 0.0767, 0.0767]],\n","\n","         [[0.0516, 0.0374, 0.0595,  ..., 0.0487, 0.0487, 0.0487],\n","          [0.0779, 0.0559, 0.0448,  ..., 0.0543, 0.0543, 0.0543],\n","          [0.0612, 0.0508, 0.0465,  ..., 0.0401, 0.0401, 0.0401],\n","          ...,\n","          [0.0630, 0.0412, 0.0575,  ..., 0.0392, 0.0392, 0.0392],\n","          [0.0630, 0.0412, 0.0575,  ..., 0.0392, 0.0392, 0.0392],\n","          [0.0630, 0.0412, 0.0575,  ..., 0.0392, 0.0392, 0.0392]]],\n","\n","\n","        [[[0.0681, 0.0226, 0.0513,  ..., 0.0490, 0.0490, 0.0490],\n","          [0.0509, 0.0280, 0.0482,  ..., 0.0497, 0.0497, 0.0497],\n","          [0.0425, 0.0329, 0.0732,  ..., 0.0529, 0.0529, 0.0529],\n","          ...,\n","          [0.0585, 0.0525, 0.0315,  ..., 0.0475, 0.0475, 0.0475],\n","          [0.0585, 0.0525, 0.0315,  ..., 0.0475, 0.0475, 0.0475],\n","          [0.0585, 0.0525, 0.0315,  ..., 0.0475, 0.0475, 0.0475]],\n","\n","         [[0.0580, 0.0341, 0.0571,  ..., 0.0479, 0.0479, 0.0479],\n","          [0.0459, 0.0773, 0.0336,  ..., 0.0486, 0.0486, 0.0486],\n","          [0.1025, 0.0524, 0.0674,  ..., 0.0448, 0.0448, 0.0448],\n","          ...,\n","          [0.0391, 0.0802, 0.0621,  ..., 0.0468, 0.0468, 0.0468],\n","          [0.0391, 0.0802, 0.0621,  ..., 0.0468, 0.0468, 0.0468],\n","          [0.0391, 0.0802, 0.0621,  ..., 0.0468, 0.0468, 0.0468]],\n","\n","         [[0.0518, 0.0237, 0.0686,  ..., 0.0509, 0.0509, 0.0509],\n","          [0.0625, 0.0599, 0.0408,  ..., 0.0488, 0.0488, 0.0488],\n","          [0.0376, 0.0508, 0.0742,  ..., 0.0492, 0.0492, 0.0492],\n","          ...,\n","          [0.0519, 0.0292, 0.0203,  ..., 0.0557, 0.0557, 0.0557],\n","          [0.0519, 0.0292, 0.0203,  ..., 0.0557, 0.0557, 0.0557],\n","          [0.0519, 0.0292, 0.0203,  ..., 0.0557, 0.0557, 0.0557]],\n","\n","         ...,\n","\n","         [[0.0652, 0.0462, 0.0737,  ..., 0.0459, 0.0459, 0.0459],\n","          [0.0553, 0.0442, 0.0671,  ..., 0.0464, 0.0464, 0.0464],\n","          [0.0362, 0.0258, 0.0443,  ..., 0.0547, 0.0547, 0.0547],\n","          ...,\n","          [0.0462, 0.0442, 0.0365,  ..., 0.0525, 0.0525, 0.0525],\n","          [0.0462, 0.0442, 0.0365,  ..., 0.0525, 0.0525, 0.0525],\n","          [0.0462, 0.0442, 0.0365,  ..., 0.0525, 0.0525, 0.0525]],\n","\n","         [[0.0504, 0.0510, 0.0349,  ..., 0.0500, 0.0500, 0.0500],\n","          [0.0493, 0.0668, 0.0907,  ..., 0.0470, 0.0470, 0.0470],\n","          [0.0502, 0.0853, 0.1033,  ..., 0.0369, 0.0369, 0.0369],\n","          ...,\n","          [0.0298, 0.0184, 0.0541,  ..., 0.0565, 0.0565, 0.0565],\n","          [0.0298, 0.0184, 0.0541,  ..., 0.0565, 0.0565, 0.0565],\n","          [0.0298, 0.0184, 0.0541,  ..., 0.0565, 0.0565, 0.0565]],\n","\n","         [[0.0407, 0.0843, 0.0353,  ..., 0.0506, 0.0506, 0.0506],\n","          [0.0412, 0.0824, 0.0282,  ..., 0.0495, 0.0495, 0.0495],\n","          [0.0459, 0.0385, 0.0273,  ..., 0.0533, 0.0533, 0.0533],\n","          ...,\n","          [0.0545, 0.0923, 0.1031,  ..., 0.0430, 0.0430, 0.0430],\n","          [0.0545, 0.0923, 0.1031,  ..., 0.0430, 0.0430, 0.0430],\n","          [0.0545, 0.0923, 0.1031,  ..., 0.0430, 0.0430, 0.0430]]],\n","\n","\n","        [[[0.0420, 0.0496, 0.0358,  ..., 0.0477, 0.0477, 0.0477],\n","          [0.0551, 0.0422, 0.0445,  ..., 0.0558, 0.0558, 0.0558],\n","          [0.0430, 0.0916, 0.0832,  ..., 0.0389, 0.0389, 0.0389],\n","          ...,\n","          [0.0405, 0.0778, 0.0399,  ..., 0.0536, 0.0536, 0.0536],\n","          [0.0405, 0.0778, 0.0399,  ..., 0.0536, 0.0536, 0.0536],\n","          [0.0405, 0.0778, 0.0399,  ..., 0.0536, 0.0536, 0.0536]],\n","\n","         [[0.0633, 0.0635, 0.0499,  ..., 0.0533, 0.0533, 0.0533],\n","          [0.0362, 0.0468, 0.0456,  ..., 0.0404, 0.0404, 0.0404],\n","          [0.0669, 0.0694, 0.0473,  ..., 0.0346, 0.0346, 0.0346],\n","          ...,\n","          [0.0472, 0.0423, 0.0675,  ..., 0.0542, 0.0542, 0.0542],\n","          [0.0472, 0.0423, 0.0675,  ..., 0.0542, 0.0542, 0.0542],\n","          [0.0472, 0.0423, 0.0675,  ..., 0.0542, 0.0542, 0.0542]],\n","\n","         [[0.0388, 0.0636, 0.0533,  ..., 0.0473, 0.0473, 0.0473],\n","          [0.0568, 0.0619, 0.0342,  ..., 0.0426, 0.0426, 0.0426],\n","          [0.0391, 0.0566, 0.0597,  ..., 0.0468, 0.0468, 0.0468],\n","          ...,\n","          [0.0389, 0.0352, 0.0381,  ..., 0.0667, 0.0667, 0.0667],\n","          [0.0389, 0.0352, 0.0381,  ..., 0.0667, 0.0667, 0.0667],\n","          [0.0389, 0.0352, 0.0381,  ..., 0.0667, 0.0667, 0.0667]],\n","\n","         ...,\n","\n","         [[0.0278, 0.0325, 0.0371,  ..., 0.0571, 0.0571, 0.0571],\n","          [0.0785, 0.0854, 0.0436,  ..., 0.0337, 0.0337, 0.0337],\n","          [0.0577, 0.0703, 0.0312,  ..., 0.0392, 0.0392, 0.0392],\n","          ...,\n","          [0.0340, 0.0377, 0.0351,  ..., 0.0521, 0.0521, 0.0521],\n","          [0.0340, 0.0377, 0.0351,  ..., 0.0521, 0.0521, 0.0521],\n","          [0.0340, 0.0377, 0.0351,  ..., 0.0521, 0.0521, 0.0521]],\n","\n","         [[0.0353, 0.0321, 0.0496,  ..., 0.0548, 0.0548, 0.0548],\n","          [0.0308, 0.0254, 0.0331,  ..., 0.0712, 0.0712, 0.0712],\n","          [0.0746, 0.0487, 0.0521,  ..., 0.0464, 0.0464, 0.0464],\n","          ...,\n","          [0.0310, 0.0286, 0.0315,  ..., 0.0665, 0.0665, 0.0665],\n","          [0.0310, 0.0286, 0.0315,  ..., 0.0665, 0.0665, 0.0665],\n","          [0.0310, 0.0286, 0.0315,  ..., 0.0665, 0.0665, 0.0665]],\n","\n","         [[0.0278, 0.0502, 0.0408,  ..., 0.0528, 0.0528, 0.0528],\n","          [0.0502, 0.0393, 0.0955,  ..., 0.0401, 0.0401, 0.0401],\n","          [0.0230, 0.0738, 0.0624,  ..., 0.0506, 0.0506, 0.0506],\n","          ...,\n","          [0.0321, 0.1121, 0.0421,  ..., 0.0429, 0.0429, 0.0429],\n","          [0.0321, 0.1121, 0.0421,  ..., 0.0429, 0.0429, 0.0429],\n","          [0.0321, 0.1121, 0.0421,  ..., 0.0429, 0.0429, 0.0429]]],\n","\n","\n","        ...,\n","\n","\n","        [[[0.0479, 0.0567, 0.0757,  ..., 0.0296, 0.0387, 0.0413],\n","          [0.0443, 0.0353, 0.0338,  ..., 0.0689, 0.0436, 0.0661],\n","          [0.0426, 0.0355, 0.0725,  ..., 0.0414, 0.0944, 0.0238],\n","          ...,\n","          [0.0553, 0.0578, 0.0554,  ..., 0.0495, 0.0345, 0.0248],\n","          [0.0490, 0.0641, 0.0471,  ..., 0.0474, 0.0453, 0.0365],\n","          [0.0321, 0.0549, 0.0366,  ..., 0.0396, 0.0723, 0.0526]],\n","\n","         [[0.0705, 0.0927, 0.0484,  ..., 0.0630, 0.0538, 0.0340],\n","          [0.0417, 0.0373, 0.0471,  ..., 0.0637, 0.0450, 0.0510],\n","          [0.0574, 0.0455, 0.0555,  ..., 0.0389, 0.0488, 0.0523],\n","          ...,\n","          [0.0531, 0.0766, 0.0553,  ..., 0.0511, 0.0525, 0.0331],\n","          [0.0382, 0.0381, 0.0502,  ..., 0.0544, 0.0612, 0.0379],\n","          [0.0499, 0.0696, 0.0563,  ..., 0.0509, 0.0291, 0.0500]],\n","\n","         [[0.0401, 0.0580, 0.0403,  ..., 0.0363, 0.0691, 0.0611],\n","          [0.0312, 0.0848, 0.0607,  ..., 0.0350, 0.0314, 0.0446],\n","          [0.0329, 0.0461, 0.0581,  ..., 0.0516, 0.0511, 0.0367],\n","          ...,\n","          [0.0712, 0.0386, 0.0630,  ..., 0.0473, 0.0320, 0.0476],\n","          [0.0457, 0.0681, 0.0473,  ..., 0.0399, 0.0315, 0.0306],\n","          [0.0320, 0.0357, 0.0636,  ..., 0.0635, 0.0649, 0.0448]],\n","\n","         ...,\n","\n","         [[0.0491, 0.0222, 0.0407,  ..., 0.0665, 0.0492, 0.0461],\n","          [0.0609, 0.0811, 0.0571,  ..., 0.0329, 0.0502, 0.0516],\n","          [0.0650, 0.0334, 0.0523,  ..., 0.0429, 0.0590, 0.0579],\n","          ...,\n","          [0.0677, 0.0395, 0.0377,  ..., 0.0454, 0.0526, 0.0709],\n","          [0.0586, 0.0274, 0.0333,  ..., 0.0384, 0.0499, 0.0706],\n","          [0.0516, 0.0579, 0.0510,  ..., 0.0492, 0.0495, 0.0446]],\n","\n","         [[0.0323, 0.0718, 0.0498,  ..., 0.0541, 0.0271, 0.0655],\n","          [0.0681, 0.0485, 0.0527,  ..., 0.0278, 0.0643, 0.0291],\n","          [0.0232, 0.0651, 0.0673,  ..., 0.0597, 0.0462, 0.0868],\n","          ...,\n","          [0.0368, 0.0635, 0.0469,  ..., 0.0393, 0.0426, 0.0965],\n","          [0.0702, 0.0406, 0.0568,  ..., 0.0629, 0.0626, 0.0329],\n","          [0.0555, 0.0548, 0.0660,  ..., 0.0797, 0.0396, 0.0505]],\n","\n","         [[0.0552, 0.0376, 0.0497,  ..., 0.0398, 0.0781, 0.0468],\n","          [0.0628, 0.0520, 0.0237,  ..., 0.0398, 0.0290, 0.0722],\n","          [0.0390, 0.0606, 0.0380,  ..., 0.0324, 0.0335, 0.0384],\n","          ...,\n","          [0.0523, 0.0668, 0.0388,  ..., 0.0641, 0.0613, 0.0409],\n","          [0.0305, 0.0553, 0.0538,  ..., 0.0384, 0.0628, 0.0445],\n","          [0.0581, 0.0447, 0.0502,  ..., 0.0381, 0.0306, 0.0396]]],\n","\n","\n","        [[[0.0476, 0.0414, 0.0387,  ..., 0.0434, 0.0434, 0.0434],\n","          [0.0263, 0.0415, 0.0627,  ..., 0.0473, 0.0473, 0.0473],\n","          [0.0549, 0.0531, 0.0276,  ..., 0.0413, 0.0413, 0.0413],\n","          ...,\n","          [0.0727, 0.0430, 0.0446,  ..., 0.0450, 0.0450, 0.0450],\n","          [0.0727, 0.0430, 0.0446,  ..., 0.0450, 0.0450, 0.0450],\n","          [0.0727, 0.0430, 0.0446,  ..., 0.0450, 0.0450, 0.0450]],\n","\n","         [[0.0685, 0.0383, 0.0374,  ..., 0.0427, 0.0427, 0.0427],\n","          [0.0181, 0.0659, 0.0529,  ..., 0.0427, 0.0427, 0.0427],\n","          [0.0529, 0.0206, 0.0540,  ..., 0.0461, 0.0461, 0.0461],\n","          ...,\n","          [0.0774, 0.0264, 0.0590,  ..., 0.0491, 0.0491, 0.0491],\n","          [0.0774, 0.0264, 0.0590,  ..., 0.0491, 0.0491, 0.0491],\n","          [0.0774, 0.0264, 0.0590,  ..., 0.0491, 0.0491, 0.0491]],\n","\n","         [[0.0424, 0.0437, 0.0444,  ..., 0.0443, 0.0443, 0.0443],\n","          [0.0210, 0.0682, 0.0387,  ..., 0.0423, 0.0423, 0.0423],\n","          [0.0438, 0.0848, 0.0252,  ..., 0.0734, 0.0734, 0.0734],\n","          ...,\n","          [0.0654, 0.0346, 0.0621,  ..., 0.0707, 0.0707, 0.0707],\n","          [0.0654, 0.0346, 0.0621,  ..., 0.0707, 0.0707, 0.0707],\n","          [0.0654, 0.0346, 0.0621,  ..., 0.0707, 0.0707, 0.0707]],\n","\n","         ...,\n","\n","         [[0.0666, 0.0377, 0.0279,  ..., 0.0414, 0.0414, 0.0414],\n","          [0.0483, 0.0439, 0.0498,  ..., 0.0650, 0.0650, 0.0650],\n","          [0.0294, 0.0530, 0.0540,  ..., 0.0590, 0.0590, 0.0590],\n","          ...,\n","          [0.0617, 0.0643, 0.0925,  ..., 0.0510, 0.0510, 0.0510],\n","          [0.0617, 0.0643, 0.0925,  ..., 0.0510, 0.0510, 0.0510],\n","          [0.0617, 0.0643, 0.0925,  ..., 0.0510, 0.0510, 0.0510]],\n","\n","         [[0.0339, 0.0575, 0.0291,  ..., 0.0513, 0.0513, 0.0513],\n","          [0.0560, 0.0437, 0.0479,  ..., 0.0669, 0.0669, 0.0669],\n","          [0.0613, 0.0490, 0.0230,  ..., 0.0571, 0.0571, 0.0571],\n","          ...,\n","          [0.0324, 0.0375, 0.0235,  ..., 0.0825, 0.0825, 0.0825],\n","          [0.0324, 0.0375, 0.0235,  ..., 0.0825, 0.0825, 0.0825],\n","          [0.0324, 0.0375, 0.0235,  ..., 0.0825, 0.0825, 0.0825]],\n","\n","         [[0.0429, 0.0620, 0.0406,  ..., 0.0450, 0.0450, 0.0450],\n","          [0.0535, 0.0407, 0.0654,  ..., 0.0515, 0.0515, 0.0515],\n","          [0.0585, 0.0371, 0.0368,  ..., 0.0658, 0.0658, 0.0658],\n","          ...,\n","          [0.0556, 0.0440, 0.0361,  ..., 0.0358, 0.0358, 0.0358],\n","          [0.0556, 0.0440, 0.0361,  ..., 0.0358, 0.0358, 0.0358],\n","          [0.0556, 0.0440, 0.0361,  ..., 0.0358, 0.0358, 0.0358]]],\n","\n","\n","        [[[0.0330, 0.0583, 0.0407,  ..., 0.0390, 0.0390, 0.0390],\n","          [0.0771, 0.0464, 0.0336,  ..., 0.0529, 0.0529, 0.0529],\n","          [0.0693, 0.0395, 0.0437,  ..., 0.0473, 0.0473, 0.0473],\n","          ...,\n","          [0.0313, 0.0502, 0.0604,  ..., 0.0526, 0.0526, 0.0526],\n","          [0.0313, 0.0502, 0.0604,  ..., 0.0526, 0.0526, 0.0526],\n","          [0.0313, 0.0502, 0.0604,  ..., 0.0526, 0.0526, 0.0526]],\n","\n","         [[0.0503, 0.0490, 0.0590,  ..., 0.0580, 0.0580, 0.0580],\n","          [0.0214, 0.0722, 0.0440,  ..., 0.0467, 0.0467, 0.0467],\n","          [0.0279, 0.0730, 0.0479,  ..., 0.0479, 0.0479, 0.0479],\n","          ...,\n","          [0.0487, 0.0280, 0.0711,  ..., 0.0521, 0.0521, 0.0521],\n","          [0.0487, 0.0280, 0.0711,  ..., 0.0521, 0.0521, 0.0521],\n","          [0.0487, 0.0280, 0.0711,  ..., 0.0521, 0.0521, 0.0521]],\n","\n","         [[0.0523, 0.0837, 0.0511,  ..., 0.0400, 0.0400, 0.0400],\n","          [0.0363, 0.0798, 0.0532,  ..., 0.0495, 0.0495, 0.0495],\n","          [0.0367, 0.0733, 0.0318,  ..., 0.0447, 0.0447, 0.0447],\n","          ...,\n","          [0.0384, 0.0379, 0.0275,  ..., 0.0773, 0.0773, 0.0773],\n","          [0.0384, 0.0379, 0.0275,  ..., 0.0773, 0.0773, 0.0773],\n","          [0.0384, 0.0379, 0.0275,  ..., 0.0773, 0.0773, 0.0773]],\n","\n","         ...,\n","\n","         [[0.0548, 0.0299, 0.0395,  ..., 0.0557, 0.0557, 0.0557],\n","          [0.0492, 0.0397, 0.0398,  ..., 0.0587, 0.0587, 0.0587],\n","          [0.0552, 0.0412, 0.0591,  ..., 0.0573, 0.0573, 0.0573],\n","          ...,\n","          [0.0649, 0.0656, 0.0340,  ..., 0.0520, 0.0520, 0.0520],\n","          [0.0649, 0.0656, 0.0340,  ..., 0.0520, 0.0520, 0.0520],\n","          [0.0649, 0.0656, 0.0340,  ..., 0.0520, 0.0520, 0.0520]],\n","\n","         [[0.0598, 0.0275, 0.0724,  ..., 0.0406, 0.0406, 0.0406],\n","          [0.0263, 0.0421, 0.0581,  ..., 0.0645, 0.0645, 0.0645],\n","          [0.0538, 0.0445, 0.0467,  ..., 0.0508, 0.0508, 0.0508],\n","          ...,\n","          [0.0270, 0.0335, 0.0410,  ..., 0.0737, 0.0737, 0.0737],\n","          [0.0270, 0.0335, 0.0410,  ..., 0.0737, 0.0737, 0.0737],\n","          [0.0270, 0.0335, 0.0410,  ..., 0.0737, 0.0737, 0.0737]],\n","\n","         [[0.0545, 0.0659, 0.0548,  ..., 0.0439, 0.0439, 0.0439],\n","          [0.0373, 0.0407, 0.0232,  ..., 0.0515, 0.0515, 0.0515],\n","          [0.0539, 0.0419, 0.0283,  ..., 0.0419, 0.0419, 0.0419],\n","          ...,\n","          [0.0685, 0.0505, 0.0334,  ..., 0.0411, 0.0411, 0.0411],\n","          [0.0685, 0.0505, 0.0334,  ..., 0.0411, 0.0411, 0.0411],\n","          [0.0685, 0.0505, 0.0334,  ..., 0.0411, 0.0411, 0.0411]]]],\n","       grad_fn=<SoftmaxBackward0>)\n","torch.Size([10, 8, 20, 20])\n"]}]},{"cell_type":"code","metadata":{"id":"7megouWpgCck","colab":{"base_uri":"https://localhost:8080/"},"outputId":"304df750-0da5-4248-8666-22d78cdf748e","executionInfo":{"status":"ok","timestamp":1662226987802,"user_tz":-540,"elapsed":248,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","\n","print(attn_values.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 8, 20, 64])\n"]}]},{"cell_type":"markdown","metadata":{"id":"LmSTaymdg-P_"},"source":["### 각 head의 결과물 병합"]},{"cell_type":"markdown","metadata":{"id":"YSdQZCk0hCNd"},"source":["각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."]},{"cell_type":"code","metadata":{"id":"eaK0bpMGhQZ2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"83e58876-bf83-457f-8302-990da9c769e9","executionInfo":{"status":"ok","timestamp":1662226990516,"user_tz":-540,"elapsed":242,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n","attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n","\n","print(attn_values.shape)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 20, 512])\n"]}]},{"cell_type":"code","metadata":{"id":"LTng_2SXhdH1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662226992088,"user_tz":-540,"elapsed":252,"user":{"displayName":"MA PI","userId":"00308135229573784967"}},"outputId":"3ea2c588-f517-4319-96bb-5586a71cad36"},"source":["# w_0 : (d_model, d_model)\n","# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n","outputs = w_0(attn_values)\n","\n","print(outputs)\n","print(outputs.shape)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.0043, -0.1327,  0.0959,  ...,  0.1094, -0.1679, -0.0445],\n","         [ 0.0996, -0.0957,  0.1594,  ...,  0.1120, -0.1612,  0.0158],\n","         [ 0.0684, -0.0878,  0.1038,  ...,  0.0545, -0.1531,  0.0510],\n","         ...,\n","         [ 0.0721, -0.1334,  0.1440,  ...,  0.1428, -0.1771, -0.0444],\n","         [ 0.0721, -0.1334,  0.1440,  ...,  0.1428, -0.1771, -0.0444],\n","         [ 0.0721, -0.1334,  0.1440,  ...,  0.1428, -0.1771, -0.0444]],\n","\n","        [[-0.0793, -0.3827,  0.0999,  ...,  0.3194, -0.4645,  0.0162],\n","         [-0.0429, -0.3748,  0.0616,  ...,  0.3099, -0.4279,  0.0227],\n","         [-0.0572, -0.3540,  0.0572,  ...,  0.3466, -0.4188, -0.0042],\n","         ...,\n","         [-0.0685, -0.4596,  0.0783,  ...,  0.2936, -0.3906, -0.0609],\n","         [-0.0685, -0.4596,  0.0783,  ...,  0.2936, -0.3906, -0.0609],\n","         [-0.0685, -0.4596,  0.0783,  ...,  0.2936, -0.3906, -0.0609]],\n","\n","        [[ 0.0295, -0.3399,  0.0336,  ...,  0.2284, -0.2756,  0.0141],\n","         [ 0.0222, -0.2637,  0.0246,  ...,  0.1828, -0.2892,  0.1081],\n","         [ 0.0387, -0.2865,  0.0428,  ...,  0.1430, -0.2582,  0.0398],\n","         ...,\n","         [ 0.0162, -0.3768,  0.0548,  ...,  0.2090, -0.2779,  0.0171],\n","         [ 0.0162, -0.3768,  0.0548,  ...,  0.2090, -0.2779,  0.0171],\n","         [ 0.0162, -0.3768,  0.0548,  ...,  0.2090, -0.2779,  0.0171]],\n","\n","        ...,\n","\n","        [[ 0.1386, -0.0180, -0.1264,  ...,  0.1201,  0.1478, -0.0070],\n","         [ 0.0906, -0.0189, -0.0842,  ...,  0.1308,  0.1621, -0.0219],\n","         [ 0.0898,  0.0148, -0.1050,  ...,  0.0838,  0.1300,  0.0413],\n","         ...,\n","         [ 0.1044, -0.0465, -0.1146,  ...,  0.1156,  0.1427,  0.0060],\n","         [ 0.0836, -0.0293, -0.1292,  ...,  0.1456,  0.2045, -0.0524],\n","         [ 0.0642, -0.0011, -0.0931,  ...,  0.0969,  0.1517,  0.0221]],\n","\n","        [[ 0.0094,  0.0165,  0.0783,  ...,  0.2267, -0.0811, -0.0387],\n","         [-0.0661, -0.0097,  0.0860,  ...,  0.2443, -0.1228, -0.1619],\n","         [-0.0515, -0.0519,  0.1196,  ...,  0.2002, -0.0776, -0.1201],\n","         ...,\n","         [-0.0395, -0.0714,  0.0800,  ...,  0.2587, -0.1110, -0.1379],\n","         [-0.0395, -0.0714,  0.0800,  ...,  0.2587, -0.1110, -0.1379],\n","         [-0.0395, -0.0714,  0.0800,  ...,  0.2587, -0.1110, -0.1379]],\n","\n","        [[-0.0604, -0.0815, -0.0317,  ...,  0.1069, -0.1590, -0.0659],\n","         [-0.0279, -0.1105, -0.0796,  ...,  0.1368, -0.1241, -0.0990],\n","         [-0.0901, -0.1074, -0.0389,  ...,  0.1026, -0.1447, -0.0313],\n","         ...,\n","         [-0.0697, -0.1684, -0.0673,  ...,  0.1345, -0.0708, -0.1174],\n","         [-0.0697, -0.1684, -0.0673,  ...,  0.1345, -0.0708, -0.1174],\n","         [-0.0697, -0.1684, -0.0673,  ...,  0.1345, -0.0708, -0.1174]]],\n","       grad_fn=<ViewBackward0>)\n","torch.Size([10, 20, 512])\n"]}]},{"cell_type":"markdown","metadata":{"id":"goX70VKqhxQH"},"source":["## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"]},{"cell_type":"markdown","metadata":{"id":"WtNyV7mMj7V_"},"source":["위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n","\n","아래 코드의 TODO 부분을 채워주세요."]},{"cell_type":"code","metadata":{"id":"U_kNhOTrkBHm","executionInfo":{"status":"ok","timestamp":1662228642628,"user_tz":-540,"elapsed":262,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["class MultiheadAttention(nn.Module):\n","  def __init__(self):\n","    super(MultiheadAttention, self).__init__()\n","\n","    # Q, K, V learnable matrices\n","    self.w_q = nn.Linear(d_model, d_model)\n","    self.w_k = nn.Linear(d_model, d_model)\n","    self.w_v = nn.Linear(d_model, d_model)\n","\n","    # Linear projection for concatenated outputs\n","    self.w_0 = nn.Linear(d_model, d_model)\n","\n","  # scaled-dot product attention\n","  def self_attention(self, q, k, v):\n","    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n","    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","\n","    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","\n","    return attn_values\n","\n","  def forward(self, q, k, v):\n","    batch_size = q.shape[0]\n","\n","    # linear projection\n","    ################################################################################\n","    # TODO 1: Implement the forward pass for linear projection.                #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    q = self.w_q(q)\n","    k = self.w_k(k)\n","    v = self.w_v(v)\n","\n","    # head만큼 쪼개준다\n","    ################################################################################\n","    # TODO 2: Implement the forward pass for \bsplit head.                #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    q = q.view(batch_size, -1, num_heads, d_k)\n","    k = k.view(batch_size, -1, num_heads, d_k)\n","    v = v.view(batch_size, -1, num_heads, d_k)\n","\n","    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n","    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n","    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n","\n","    return self.w_0(attn_values)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYLuu_9alQxT","executionInfo":{"status":"ok","timestamp":1662228645889,"user_tz":-540,"elapsed":241,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":["multihead_attn = MultiheadAttention()\n","\n","outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMiXlYjSlTfB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662228653508,"user_tz":-540,"elapsed":274,"user":{"displayName":"MA PI","userId":"00308135229573784967"}},"outputId":"7af5833c-63a9-417f-9231-8ea7f706e80c"},"source":["print(outputs)\n","print(outputs.shape)  # (batch_size, length, d_model)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.1881,  0.1743,  0.0278,  ...,  0.1217, -0.0694, -0.0161],\n","         [-0.2214,  0.1663, -0.0205,  ...,  0.0765,  0.0010, -0.0358],\n","         [-0.1969,  0.2258,  0.0189,  ...,  0.1599,  0.0009, -0.0354],\n","         ...,\n","         [-0.1963,  0.1910, -0.0243,  ...,  0.1073, -0.0483,  0.0027],\n","         [-0.1963,  0.1910, -0.0243,  ...,  0.1073, -0.0483,  0.0027],\n","         [-0.1963,  0.1910, -0.0243,  ...,  0.1073, -0.0483,  0.0027]],\n","\n","        [[-0.1842, -0.1065,  0.1868,  ...,  0.0323, -0.1754,  0.1611],\n","         [-0.1118, -0.0972,  0.2141,  ...,  0.0993, -0.1083,  0.1668],\n","         [-0.2151, -0.1280,  0.2129,  ..., -0.0043, -0.1942,  0.1965],\n","         ...,\n","         [-0.1780, -0.1197,  0.1646,  ...,  0.0247, -0.1798,  0.1981],\n","         [-0.1780, -0.1197,  0.1646,  ...,  0.0247, -0.1798,  0.1981],\n","         [-0.1780, -0.1197,  0.1646,  ...,  0.0247, -0.1798,  0.1981]],\n","\n","        [[-0.0263, -0.0193,  0.1181,  ...,  0.0981, -0.2188,  0.1416],\n","         [-0.1618, -0.0126,  0.1372,  ...,  0.0510, -0.1803,  0.0503],\n","         [-0.0639,  0.0104,  0.1334,  ...,  0.0848, -0.1897,  0.1131],\n","         ...,\n","         [-0.0969, -0.0048,  0.1152,  ...,  0.0405, -0.1733,  0.0878],\n","         [-0.0969, -0.0048,  0.1152,  ...,  0.0405, -0.1733,  0.0878],\n","         [-0.0969, -0.0048,  0.1152,  ...,  0.0405, -0.1733,  0.0878]],\n","\n","        ...,\n","\n","        [[-0.0637,  0.2179,  0.0355,  ...,  0.0118, -0.0447,  0.0960],\n","         [-0.0669,  0.2078,  0.0264,  ..., -0.0042, -0.0459,  0.1119],\n","         [-0.1229,  0.2094,  0.0591,  ..., -0.0585, -0.0166,  0.0819],\n","         ...,\n","         [-0.0961,  0.1718,  0.0405,  ...,  0.0158, -0.0097,  0.1054],\n","         [-0.0870,  0.1692,  0.0413,  ..., -0.0331, -0.0670,  0.0914],\n","         [-0.0511,  0.1756,  0.0190,  ..., -0.0142, -0.0071,  0.0749]],\n","\n","        [[-0.1693,  0.0909,  0.0773,  ...,  0.0368, -0.2141,  0.0927],\n","         [-0.1021,  0.0362,  0.1069,  ..., -0.0063, -0.2928,  0.1844],\n","         [-0.1221,  0.0965,  0.1082,  ...,  0.0176, -0.2699,  0.1680],\n","         ...,\n","         [-0.1472,  0.0515,  0.1646,  ...,  0.0308, -0.2789,  0.1283],\n","         [-0.1472,  0.0515,  0.1646,  ...,  0.0308, -0.2789,  0.1283],\n","         [-0.1472,  0.0515,  0.1646,  ...,  0.0308, -0.2789,  0.1283]],\n","\n","        [[-0.0946,  0.0826,  0.1063,  ...,  0.0408, -0.1619,  0.0418],\n","         [-0.1074,  0.0330,  0.0522,  ...,  0.0439, -0.2834,  0.0411],\n","         [-0.1069,  0.0521,  0.0965,  ...,  0.0624, -0.1971,  0.0347],\n","         ...,\n","         [-0.1111,  0.0126,  0.0792,  ...,  0.0227, -0.2403,  0.0280],\n","         [-0.1111,  0.0126,  0.0792,  ...,  0.0227, -0.2403,  0.0280],\n","         [-0.1111,  0.0126,  0.0792,  ...,  0.0227, -0.2403,  0.0280]]],\n","       grad_fn=<ViewBackward0>)\n","torch.Size([10, 20, 512])\n"]}]},{"cell_type":"code","metadata":{"id":"OTku1fySVR3L","executionInfo":{"status":"ok","timestamp":1662228675082,"user_tz":-540,"elapsed":251,"user":{"displayName":"MA PI","userId":"00308135229573784967"}}},"source":[],"execution_count":24,"outputs":[]}]}