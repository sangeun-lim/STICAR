{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "GiWulKAdmAHX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
        "1. Multi-head attention 및 self-attention 구현.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### 필요 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Req. 2-1 Multi-head self-attention 구조 익히기"
      ],
      "metadata": {
        "id": "HH0VdC4uJJVG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**\n",
        "vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n",
        "\n",
        "각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\n",
        "vocab_size = 100\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "# 길이 맞춰주기 위해 패딩합니다.\n",
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6311e6-a575-456c-de8d-a8d5ad220175"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 72067.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c86174-e934-4b86-be50-42459f4cf152"
      },
      "source": [
        "data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b8631b3-ec4e-41b9-90d0-3ce709497281"
      },
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 4.5220e-01, -8.0138e-01, -2.6229e+00,  ..., -1.9492e+00,\n",
            "          -9.5384e-01, -3.1468e+00],\n",
            "         [-7.3974e-02, -2.4676e-01,  1.7123e+00,  ...,  3.1493e-01,\n",
            "          -2.8493e+00,  6.1556e-01],\n",
            "         [-9.8629e-01, -1.6576e+00, -9.9501e-02,  ...,  1.1769e+00,\n",
            "           1.3054e+00,  1.6423e+00],\n",
            "         ...,\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00]],\n",
            "\n",
            "        [[ 1.9200e+00,  6.2438e-01,  2.7738e-01,  ..., -1.0504e+00,\n",
            "           4.2086e-01,  8.0782e-01],\n",
            "         [-6.7074e-01, -7.6778e-01,  1.3423e+00,  ...,  8.1818e-01,\n",
            "           7.4122e-01,  1.0891e-01],\n",
            "         [-8.1829e-01, -2.1970e-03, -5.6837e-02,  ...,  3.8242e-01,\n",
            "           4.5298e-01,  2.9236e-02],\n",
            "         ...,\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00]],\n",
            "\n",
            "        [[-1.0748e+00, -1.6847e+00, -1.9929e+00,  ...,  9.0237e-01,\n",
            "          -1.3131e+00,  9.2142e-01],\n",
            "         [-2.0701e-01,  1.2361e+00, -8.7633e-01,  ..., -2.8782e-01,\n",
            "          -9.2337e-01,  8.4622e-01],\n",
            "         [-1.6746e-01, -1.1997e-01,  1.1652e-01,  ...,  7.7522e-01,\n",
            "           5.5518e-02,  3.7935e-01],\n",
            "         ...,\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.0766e+00, -5.0857e-01, -9.8166e-01,  ..., -1.6545e-01,\n",
            "           4.2270e-01, -6.9268e-02],\n",
            "         [ 6.9977e-01, -1.8317e+00, -6.9532e-01,  ...,  1.7790e+00,\n",
            "           5.6504e-01,  3.6732e-01],\n",
            "         [-8.1829e-01, -2.1970e-03, -5.6837e-02,  ...,  3.8242e-01,\n",
            "           4.5298e-01,  2.9236e-02],\n",
            "         ...,\n",
            "         [-6.7479e-01, -6.8940e-01, -2.4634e+00,  ..., -3.0318e-01,\n",
            "          -6.6746e-01,  6.4240e-01],\n",
            "         [-5.0391e-01,  1.2914e+00, -1.4095e+00,  ..., -5.7254e-01,\n",
            "           6.3087e-01,  1.0210e+00],\n",
            "         [ 1.9087e+00,  2.9738e-01,  8.5674e-01,  ...,  1.0989e+00,\n",
            "           9.3817e-01, -6.2763e-01]],\n",
            "\n",
            "        [[-8.1846e-02,  1.9599e-01,  1.4935e+00,  ..., -1.3235e+00,\n",
            "           2.2673e-01,  1.4692e+00],\n",
            "         [-5.3052e-01, -5.2087e-01, -1.8066e+00,  ...,  3.6358e-01,\n",
            "           1.4572e+00, -8.7349e-01],\n",
            "         [ 8.6968e-02,  6.8560e-01,  4.4045e-01,  ..., -5.3633e-01,\n",
            "          -5.4722e-01,  4.5222e-01],\n",
            "         ...,\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00]],\n",
            "\n",
            "        [[-8.7541e-01,  7.4503e-01, -1.1983e+00,  ...,  5.1227e-01,\n",
            "          -1.7338e+00,  5.4671e-01],\n",
            "         [-5.3052e-01, -5.2087e-01, -1.8066e+00,  ...,  3.6358e-01,\n",
            "           1.4572e+00, -8.7349e-01],\n",
            "         [ 1.0387e-01, -1.3999e+00, -7.5752e-01,  ..., -8.0089e-01,\n",
            "           8.6004e-01,  4.9740e-01],\n",
            "         ...,\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00],\n",
            "         [ 7.2711e-01, -8.5266e-01, -2.5614e-01,  ...,  2.6862e-01,\n",
            "          -1.4141e+00,  1.8307e+00]]], grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### Linear projection & 여러 head로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5ec010-dff4-4689-f34a-70cef2be3561"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2fc0236-4ce9-435b-b3ae-a2410559f483"
      },
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# num_heads * d_k로 쪼갠다\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f612bdd-5514-4dd2-b7e3-b5b2317f9849"
      },
      "source": [
        "# num_heads를 밖으로 뺌으로써\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
        "\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### Scaled dot-product self-attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d7d600-f580-4823-e824-ac4eaddf27d9"
      },
      "source": [
        "# shape - (L, L)\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "# softmax - row-wise이기 때문에 dim은 -1\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0586, 0.0460, 0.0470,  ..., 0.0554, 0.0554, 0.0554],\n",
            "          [0.0505, 0.0570, 0.0441,  ..., 0.0416, 0.0416, 0.0416],\n",
            "          [0.0572, 0.0612, 0.0539,  ..., 0.0529, 0.0529, 0.0529],\n",
            "          ...,\n",
            "          [0.0496, 0.0508, 0.0442,  ..., 0.0547, 0.0547, 0.0547],\n",
            "          [0.0496, 0.0508, 0.0442,  ..., 0.0547, 0.0547, 0.0547],\n",
            "          [0.0496, 0.0508, 0.0442,  ..., 0.0547, 0.0547, 0.0547]],\n",
            "\n",
            "         [[0.0611, 0.0447, 0.0514,  ..., 0.0426, 0.0426, 0.0426],\n",
            "          [0.0454, 0.0496, 0.0961,  ..., 0.0338, 0.0338, 0.0338],\n",
            "          [0.0365, 0.0651, 0.0539,  ..., 0.0337, 0.0337, 0.0337],\n",
            "          ...,\n",
            "          [0.0921, 0.0486, 0.0752,  ..., 0.0461, 0.0461, 0.0461],\n",
            "          [0.0921, 0.0486, 0.0752,  ..., 0.0461, 0.0461, 0.0461],\n",
            "          [0.0921, 0.0486, 0.0752,  ..., 0.0461, 0.0461, 0.0461]],\n",
            "\n",
            "         [[0.0557, 0.0457, 0.0426,  ..., 0.0321, 0.0321, 0.0321],\n",
            "          [0.0260, 0.0547, 0.0507,  ..., 0.0363, 0.0363, 0.0363],\n",
            "          [0.0490, 0.0620, 0.0292,  ..., 0.0611, 0.0611, 0.0611],\n",
            "          ...,\n",
            "          [0.0585, 0.0508, 0.0336,  ..., 0.0784, 0.0784, 0.0784],\n",
            "          [0.0585, 0.0508, 0.0336,  ..., 0.0784, 0.0784, 0.0784],\n",
            "          [0.0585, 0.0508, 0.0336,  ..., 0.0784, 0.0784, 0.0784]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0478, 0.0425, 0.0451,  ..., 0.0495, 0.0495, 0.0495],\n",
            "          [0.0413, 0.0951, 0.0712,  ..., 0.0355, 0.0355, 0.0355],\n",
            "          [0.0516, 0.0371, 0.0475,  ..., 0.0713, 0.0713, 0.0713],\n",
            "          ...,\n",
            "          [0.0466, 0.0431, 0.0394,  ..., 0.0561, 0.0561, 0.0561],\n",
            "          [0.0466, 0.0431, 0.0394,  ..., 0.0561, 0.0561, 0.0561],\n",
            "          [0.0466, 0.0431, 0.0394,  ..., 0.0561, 0.0561, 0.0561]],\n",
            "\n",
            "         [[0.0531, 0.0434, 0.0320,  ..., 0.0540, 0.0540, 0.0540],\n",
            "          [0.0516, 0.0313, 0.0384,  ..., 0.0377, 0.0377, 0.0377],\n",
            "          [0.0834, 0.0464, 0.0990,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          ...,\n",
            "          [0.0613, 0.0610, 0.0777,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0613, 0.0610, 0.0777,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0613, 0.0610, 0.0777,  ..., 0.0400, 0.0400, 0.0400]],\n",
            "\n",
            "         [[0.0587, 0.0301, 0.0513,  ..., 0.0718, 0.0718, 0.0718],\n",
            "          [0.0371, 0.0616, 0.0446,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0426, 0.0742, 0.0285,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          ...,\n",
            "          [0.0907, 0.0370, 0.0373,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0907, 0.0370, 0.0373,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0907, 0.0370, 0.0373,  ..., 0.0403, 0.0403, 0.0403]]],\n",
            "\n",
            "\n",
            "        [[[0.0427, 0.0759, 0.0455,  ..., 0.0494, 0.0494, 0.0494],\n",
            "          [0.0260, 0.0201, 0.0196,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0370, 0.0411, 0.0291,  ..., 0.0556, 0.0556, 0.0556],\n",
            "          ...,\n",
            "          [0.0509, 0.0185, 0.0329,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0509, 0.0185, 0.0329,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0509, 0.0185, 0.0329,  ..., 0.0505, 0.0505, 0.0505]],\n",
            "\n",
            "         [[0.0485, 0.0444, 0.0664,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          [0.0828, 0.0455, 0.0769,  ..., 0.0462, 0.0462, 0.0462],\n",
            "          [0.0263, 0.0335, 0.0136,  ..., 0.0579, 0.0579, 0.0579],\n",
            "          ...,\n",
            "          [0.0429, 0.0544, 0.0653,  ..., 0.0474, 0.0474, 0.0474],\n",
            "          [0.0429, 0.0544, 0.0653,  ..., 0.0474, 0.0474, 0.0474],\n",
            "          [0.0429, 0.0544, 0.0653,  ..., 0.0474, 0.0474, 0.0474]],\n",
            "\n",
            "         [[0.0862, 0.0489, 0.0373,  ..., 0.0467, 0.0467, 0.0467],\n",
            "          [0.0362, 0.0316, 0.0342,  ..., 0.0533, 0.0533, 0.0533],\n",
            "          [0.0159, 0.0481, 0.0259,  ..., 0.0550, 0.0550, 0.0550],\n",
            "          ...,\n",
            "          [0.0360, 0.0226, 0.0192,  ..., 0.0583, 0.0583, 0.0583],\n",
            "          [0.0360, 0.0226, 0.0192,  ..., 0.0583, 0.0583, 0.0583],\n",
            "          [0.0360, 0.0226, 0.0192,  ..., 0.0583, 0.0583, 0.0583]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0569, 0.0219, 0.1007,  ..., 0.0473, 0.0473, 0.0473],\n",
            "          [0.0396, 0.0783, 0.0474,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0152, 0.0306, 0.0314,  ..., 0.0565, 0.0565, 0.0565],\n",
            "          ...,\n",
            "          [0.0842, 0.0745, 0.0425,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0842, 0.0745, 0.0425,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0842, 0.0745, 0.0425,  ..., 0.0492, 0.0492, 0.0492]],\n",
            "\n",
            "         [[0.0493, 0.0358, 0.0495,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          [0.1166, 0.0571, 0.0574,  ..., 0.0453, 0.0453, 0.0453],\n",
            "          [0.0749, 0.0607, 0.0939,  ..., 0.0362, 0.0362, 0.0362],\n",
            "          ...,\n",
            "          [0.0518, 0.0553, 0.0613,  ..., 0.0472, 0.0472, 0.0472],\n",
            "          [0.0518, 0.0553, 0.0613,  ..., 0.0472, 0.0472, 0.0472],\n",
            "          [0.0518, 0.0553, 0.0613,  ..., 0.0472, 0.0472, 0.0472]],\n",
            "\n",
            "         [[0.0378, 0.0241, 0.0379,  ..., 0.0519, 0.0519, 0.0519],\n",
            "          [0.0478, 0.0757, 0.0493,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0521, 0.0433, 0.0275,  ..., 0.0530, 0.0530, 0.0530],\n",
            "          ...,\n",
            "          [0.1131, 0.0399, 0.0785,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          [0.1131, 0.0399, 0.0785,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          [0.1131, 0.0399, 0.0785,  ..., 0.0469, 0.0469, 0.0469]]],\n",
            "\n",
            "\n",
            "        [[[0.0414, 0.0358, 0.0602,  ..., 0.0507, 0.0507, 0.0507],\n",
            "          [0.0377, 0.0415, 0.0758,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          [0.0487, 0.0389, 0.0587,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          ...,\n",
            "          [0.0440, 0.0365, 0.0622,  ..., 0.0513, 0.0513, 0.0513],\n",
            "          [0.0440, 0.0365, 0.0622,  ..., 0.0513, 0.0513, 0.0513],\n",
            "          [0.0440, 0.0365, 0.0622,  ..., 0.0513, 0.0513, 0.0513]],\n",
            "\n",
            "         [[0.0420, 0.0709, 0.0442,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0257, 0.0424, 0.0633,  ..., 0.0511, 0.0511, 0.0511],\n",
            "          [0.0506, 0.0579, 0.0595,  ..., 0.0449, 0.0449, 0.0449],\n",
            "          ...,\n",
            "          [0.0415, 0.0852, 0.0463,  ..., 0.0486, 0.0486, 0.0486],\n",
            "          [0.0415, 0.0852, 0.0463,  ..., 0.0486, 0.0486, 0.0486],\n",
            "          [0.0415, 0.0852, 0.0463,  ..., 0.0486, 0.0486, 0.0486]],\n",
            "\n",
            "         [[0.0434, 0.0320, 0.0483,  ..., 0.0581, 0.0581, 0.0581],\n",
            "          [0.0327, 0.0498, 0.0545,  ..., 0.0570, 0.0570, 0.0570],\n",
            "          [0.0476, 0.0777, 0.0372,  ..., 0.0366, 0.0366, 0.0366],\n",
            "          ...,\n",
            "          [0.0308, 0.0321, 0.0230,  ..., 0.0657, 0.0657, 0.0657],\n",
            "          [0.0308, 0.0321, 0.0230,  ..., 0.0657, 0.0657, 0.0657],\n",
            "          [0.0308, 0.0321, 0.0230,  ..., 0.0657, 0.0657, 0.0657]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0483, 0.0550, 0.0815,  ..., 0.0373, 0.0373, 0.0373],\n",
            "          [0.0231, 0.0367, 0.0396,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0468, 0.0586, 0.0531,  ..., 0.0421, 0.0421, 0.0421],\n",
            "          ...,\n",
            "          [0.0384, 0.0378, 0.0617,  ..., 0.0614, 0.0614, 0.0614],\n",
            "          [0.0384, 0.0378, 0.0617,  ..., 0.0614, 0.0614, 0.0614],\n",
            "          [0.0384, 0.0378, 0.0617,  ..., 0.0614, 0.0614, 0.0614]],\n",
            "\n",
            "         [[0.0304, 0.0533, 0.0505,  ..., 0.0398, 0.0398, 0.0398],\n",
            "          [0.0773, 0.0310, 0.0317,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0571, 0.0891, 0.0487,  ..., 0.0503, 0.0503, 0.0503],\n",
            "          ...,\n",
            "          [0.0584, 0.0493, 0.0379,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          [0.0584, 0.0493, 0.0379,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          [0.0584, 0.0493, 0.0379,  ..., 0.0469, 0.0469, 0.0469]],\n",
            "\n",
            "         [[0.0363, 0.0776, 0.0519,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0702, 0.0522, 0.0528,  ..., 0.0369, 0.0369, 0.0369],\n",
            "          [0.0401, 0.0364, 0.0854,  ..., 0.0477, 0.0477, 0.0477],\n",
            "          ...,\n",
            "          [0.0686, 0.0432, 0.0878,  ..., 0.0394, 0.0394, 0.0394],\n",
            "          [0.0686, 0.0432, 0.0878,  ..., 0.0394, 0.0394, 0.0394],\n",
            "          [0.0686, 0.0432, 0.0878,  ..., 0.0394, 0.0394, 0.0394]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0747, 0.0608, 0.0249,  ..., 0.0402, 0.0506, 0.0615],\n",
            "          [0.0186, 0.0644, 0.0586,  ..., 0.0746, 0.0441, 0.0327],\n",
            "          [0.0274, 0.0355, 0.0392,  ..., 0.0499, 0.0433, 0.0418],\n",
            "          ...,\n",
            "          [0.0637, 0.0820, 0.0383,  ..., 0.0619, 0.0826, 0.0325],\n",
            "          [0.0366, 0.0466, 0.0516,  ..., 0.0425, 0.0790, 0.0224],\n",
            "          [0.0431, 0.0352, 0.0345,  ..., 0.1075, 0.0432, 0.0363]],\n",
            "\n",
            "         [[0.0578, 0.0314, 0.0279,  ..., 0.0418, 0.0668, 0.0773],\n",
            "          [0.0441, 0.0596, 0.0384,  ..., 0.0444, 0.0459, 0.0427],\n",
            "          [0.0439, 0.0200, 0.0238,  ..., 0.0550, 0.0528, 0.0463],\n",
            "          ...,\n",
            "          [0.0749, 0.0275, 0.0380,  ..., 0.0344, 0.0650, 0.0402],\n",
            "          [0.0359, 0.0572, 0.0671,  ..., 0.0378, 0.0464, 0.0468],\n",
            "          [0.0792, 0.0961, 0.0451,  ..., 0.0278, 0.0364, 0.0430]],\n",
            "\n",
            "         [[0.0564, 0.0409, 0.0454,  ..., 0.0755, 0.0512, 0.0690],\n",
            "          [0.0497, 0.0579, 0.0460,  ..., 0.0520, 0.0680, 0.0304],\n",
            "          [0.0307, 0.0864, 0.0337,  ..., 0.0298, 0.0406, 0.0313],\n",
            "          ...,\n",
            "          [0.0587, 0.0336, 0.0838,  ..., 0.0411, 0.0492, 0.0311],\n",
            "          [0.0606, 0.0323, 0.0487,  ..., 0.0794, 0.0632, 0.0473],\n",
            "          [0.0423, 0.0347, 0.0477,  ..., 0.0455, 0.0681, 0.0828]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0627, 0.0438, 0.0606,  ..., 0.0568, 0.0553, 0.0712],\n",
            "          [0.0328, 0.0661, 0.0423,  ..., 0.0452, 0.0653, 0.0325],\n",
            "          [0.0530, 0.0512, 0.0515,  ..., 0.0607, 0.0520, 0.0322],\n",
            "          ...,\n",
            "          [0.0330, 0.0505, 0.0376,  ..., 0.0477, 0.0359, 0.0814],\n",
            "          [0.0594, 0.0469, 0.0369,  ..., 0.0419, 0.0636, 0.0382],\n",
            "          [0.0730, 0.0533, 0.0599,  ..., 0.0413, 0.0499, 0.0338]],\n",
            "\n",
            "         [[0.0481, 0.0767, 0.0293,  ..., 0.0336, 0.0795, 0.0448],\n",
            "          [0.0756, 0.0459, 0.0403,  ..., 0.0510, 0.0484, 0.0503],\n",
            "          [0.0315, 0.0606, 0.0612,  ..., 0.0642, 0.0662, 0.0477],\n",
            "          ...,\n",
            "          [0.0341, 0.0421, 0.0530,  ..., 0.0658, 0.0476, 0.0361],\n",
            "          [0.0399, 0.0671, 0.0518,  ..., 0.0272, 0.0566, 0.0345],\n",
            "          [0.0748, 0.0601, 0.0452,  ..., 0.0565, 0.0401, 0.0528]],\n",
            "\n",
            "         [[0.0302, 0.0532, 0.0413,  ..., 0.0999, 0.0352, 0.0679],\n",
            "          [0.0394, 0.0215, 0.0652,  ..., 0.0229, 0.0358, 0.0613],\n",
            "          [0.0368, 0.0640, 0.0330,  ..., 0.0531, 0.0423, 0.0611],\n",
            "          ...,\n",
            "          [0.0428, 0.0414, 0.0661,  ..., 0.0450, 0.0449, 0.0544],\n",
            "          [0.0167, 0.0371, 0.0445,  ..., 0.0414, 0.0461, 0.0409],\n",
            "          [0.0442, 0.0702, 0.0168,  ..., 0.0542, 0.0491, 0.0537]]],\n",
            "\n",
            "\n",
            "        [[[0.1049, 0.0306, 0.0403,  ..., 0.0616, 0.0616, 0.0616],\n",
            "          [0.0396, 0.0390, 0.0332,  ..., 0.0581, 0.0581, 0.0581],\n",
            "          [0.0865, 0.0446, 0.0454,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          ...,\n",
            "          [0.0747, 0.0658, 0.0456,  ..., 0.0503, 0.0503, 0.0503],\n",
            "          [0.0747, 0.0658, 0.0456,  ..., 0.0503, 0.0503, 0.0503],\n",
            "          [0.0747, 0.0658, 0.0456,  ..., 0.0503, 0.0503, 0.0503]],\n",
            "\n",
            "         [[0.0465, 0.0687, 0.0512,  ..., 0.0631, 0.0631, 0.0631],\n",
            "          [0.0372, 0.0646, 0.0481,  ..., 0.0374, 0.0374, 0.0374],\n",
            "          [0.0396, 0.0317, 0.0277,  ..., 0.0659, 0.0659, 0.0659],\n",
            "          ...,\n",
            "          [0.0395, 0.0302, 0.0434,  ..., 0.0506, 0.0506, 0.0506],\n",
            "          [0.0395, 0.0302, 0.0434,  ..., 0.0506, 0.0506, 0.0506],\n",
            "          [0.0395, 0.0302, 0.0434,  ..., 0.0506, 0.0506, 0.0506]],\n",
            "\n",
            "         [[0.0223, 0.0498, 0.0722,  ..., 0.0374, 0.0374, 0.0374],\n",
            "          [0.0304, 0.0933, 0.0293,  ..., 0.0429, 0.0429, 0.0429],\n",
            "          [0.0804, 0.0453, 0.0792,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          ...,\n",
            "          [0.0477, 0.0531, 0.0293,  ..., 0.0755, 0.0755, 0.0755],\n",
            "          [0.0477, 0.0531, 0.0293,  ..., 0.0755, 0.0755, 0.0755],\n",
            "          [0.0477, 0.0531, 0.0293,  ..., 0.0755, 0.0755, 0.0755]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0436, 0.0523, 0.0580,  ..., 0.0395, 0.0395, 0.0395],\n",
            "          [0.0539, 0.0269, 0.0542,  ..., 0.0552, 0.0552, 0.0552],\n",
            "          [0.0297, 0.0413, 0.0265,  ..., 0.0735, 0.0735, 0.0735],\n",
            "          ...,\n",
            "          [0.0454, 0.0347, 0.0542,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0454, 0.0347, 0.0542,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0454, 0.0347, 0.0542,  ..., 0.0600, 0.0600, 0.0600]],\n",
            "\n",
            "         [[0.0412, 0.0317, 0.0415,  ..., 0.0713, 0.0713, 0.0713],\n",
            "          [0.0238, 0.0520, 0.0171,  ..., 0.0607, 0.0607, 0.0607],\n",
            "          [0.0386, 0.0393, 0.0674,  ..., 0.0507, 0.0507, 0.0507],\n",
            "          ...,\n",
            "          [0.0503, 0.0409, 0.0428,  ..., 0.0421, 0.0421, 0.0421],\n",
            "          [0.0503, 0.0409, 0.0428,  ..., 0.0421, 0.0421, 0.0421],\n",
            "          [0.0503, 0.0409, 0.0428,  ..., 0.0421, 0.0421, 0.0421]],\n",
            "\n",
            "         [[0.0432, 0.0175, 0.0347,  ..., 0.0631, 0.0631, 0.0631],\n",
            "          [0.0417, 0.0334, 0.0365,  ..., 0.0473, 0.0473, 0.0473],\n",
            "          [0.0656, 0.0518, 0.0459,  ..., 0.0688, 0.0688, 0.0688],\n",
            "          ...,\n",
            "          [0.0303, 0.0374, 0.0728,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          [0.0303, 0.0374, 0.0728,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          [0.0303, 0.0374, 0.0728,  ..., 0.0379, 0.0379, 0.0379]]],\n",
            "\n",
            "\n",
            "        [[[0.0512, 0.0321, 0.0539,  ..., 0.0605, 0.0605, 0.0605],\n",
            "          [0.0665, 0.0323, 0.0327,  ..., 0.0481, 0.0481, 0.0481],\n",
            "          [0.0520, 0.0659, 0.0569,  ..., 0.0359, 0.0359, 0.0359],\n",
            "          ...,\n",
            "          [0.0545, 0.0631, 0.0652,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          [0.0545, 0.0631, 0.0652,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          [0.0545, 0.0631, 0.0652,  ..., 0.0482, 0.0482, 0.0482]],\n",
            "\n",
            "         [[0.0559, 0.0668, 0.0411,  ..., 0.0416, 0.0416, 0.0416],\n",
            "          [0.0273, 0.0611, 0.0745,  ..., 0.0354, 0.0354, 0.0354],\n",
            "          [0.0226, 0.0797, 0.0558,  ..., 0.0389, 0.0389, 0.0389],\n",
            "          ...,\n",
            "          [0.0620, 0.0270, 0.0985,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0620, 0.0270, 0.0985,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0620, 0.0270, 0.0985,  ..., 0.0452, 0.0452, 0.0452]],\n",
            "\n",
            "         [[0.0536, 0.0220, 0.0492,  ..., 0.0583, 0.0583, 0.0583],\n",
            "          [0.0260, 0.1026, 0.0368,  ..., 0.0472, 0.0472, 0.0472],\n",
            "          [0.0644, 0.0512, 0.0454,  ..., 0.0578, 0.0578, 0.0578],\n",
            "          ...,\n",
            "          [0.0204, 0.0492, 0.0371,  ..., 0.0700, 0.0700, 0.0700],\n",
            "          [0.0204, 0.0492, 0.0371,  ..., 0.0700, 0.0700, 0.0700],\n",
            "          [0.0204, 0.0492, 0.0371,  ..., 0.0700, 0.0700, 0.0700]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0441, 0.0663, 0.0691,  ..., 0.0522, 0.0522, 0.0522],\n",
            "          [0.0420, 0.0260, 0.0423,  ..., 0.0533, 0.0533, 0.0533],\n",
            "          [0.0491, 0.0330, 0.0428,  ..., 0.0743, 0.0743, 0.0743],\n",
            "          ...,\n",
            "          [0.0577, 0.0376, 0.0282,  ..., 0.0651, 0.0651, 0.0651],\n",
            "          [0.0577, 0.0376, 0.0282,  ..., 0.0651, 0.0651, 0.0651],\n",
            "          [0.0577, 0.0376, 0.0282,  ..., 0.0651, 0.0651, 0.0651]],\n",
            "\n",
            "         [[0.0243, 0.0685, 0.0409,  ..., 0.0314, 0.0314, 0.0314],\n",
            "          [0.0307, 0.0494, 0.0704,  ..., 0.0577, 0.0577, 0.0577],\n",
            "          [0.0447, 0.0509, 0.0343,  ..., 0.0555, 0.0555, 0.0555],\n",
            "          ...,\n",
            "          [0.0522, 0.0421, 0.0827,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0522, 0.0421, 0.0827,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0522, 0.0421, 0.0827,  ..., 0.0433, 0.0433, 0.0433]],\n",
            "\n",
            "         [[0.0386, 0.0700, 0.0382,  ..., 0.0611, 0.0611, 0.0611],\n",
            "          [0.0429, 0.0348, 0.0247,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0292, 0.0331, 0.0319,  ..., 0.0742, 0.0742, 0.0742],\n",
            "          ...,\n",
            "          [0.0372, 0.0381, 0.0481,  ..., 0.0386, 0.0386, 0.0386],\n",
            "          [0.0372, 0.0381, 0.0481,  ..., 0.0386, 0.0386, 0.0386],\n",
            "          [0.0372, 0.0381, 0.0481,  ..., 0.0386, 0.0386, 0.0386]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917073a1-1eaa-4464-9ecf-4d7bac8acd66"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### 각 head의 결과물 병합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc2feab-174f-4531-d9a3-a31081ce2a6a"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3f171d-482a-413d-8a67-bb6f700aafa6"
      },
      "source": [
        "# w_0 : (d_model, d_model)\n",
        "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0498, -0.1877,  0.0451,  ..., -0.1121,  0.0017,  0.0561],\n",
            "         [ 0.1213, -0.1669, -0.0111,  ..., -0.1209, -0.0008,  0.0905],\n",
            "         [ 0.0113, -0.1937,  0.0712,  ..., -0.1122, -0.0188,  0.0338],\n",
            "         ...,\n",
            "         [ 0.0945, -0.2164,  0.0144,  ..., -0.1014, -0.0286,  0.0754],\n",
            "         [ 0.0945, -0.2164,  0.0144,  ..., -0.1014, -0.0286,  0.0754],\n",
            "         [ 0.0945, -0.2164,  0.0144,  ..., -0.1014, -0.0286,  0.0754]],\n",
            "\n",
            "        [[-0.0921, -0.0291,  0.0240,  ..., -0.1055,  0.0453,  0.1640],\n",
            "         [-0.0740, -0.0350,  0.0380,  ..., -0.0870,  0.0439,  0.2407],\n",
            "         [-0.0305, -0.0278,  0.0313,  ..., -0.1255, -0.0081,  0.1881],\n",
            "         ...,\n",
            "         [-0.0527, -0.0106,  0.0771,  ..., -0.0947,  0.0251,  0.1803],\n",
            "         [-0.0527, -0.0106,  0.0771,  ..., -0.0947,  0.0251,  0.1803],\n",
            "         [-0.0527, -0.0106,  0.0771,  ..., -0.0947,  0.0251,  0.1803]],\n",
            "\n",
            "        [[ 0.0749, -0.0447, -0.1285,  ..., -0.0682,  0.1201,  0.1210],\n",
            "         [ 0.0324,  0.0119, -0.0381,  ..., -0.0486,  0.1135,  0.0880],\n",
            "         [-0.0480, -0.0187, -0.0157,  ...,  0.0038,  0.1301,  0.0911],\n",
            "         ...,\n",
            "         [ 0.0170, -0.0526, -0.0104,  ..., -0.0501,  0.0642,  0.0950],\n",
            "         [ 0.0170, -0.0526, -0.0104,  ..., -0.0501,  0.0642,  0.0950],\n",
            "         [ 0.0170, -0.0526, -0.0104,  ..., -0.0501,  0.0642,  0.0950]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1129, -0.0489, -0.0355,  ...,  0.0613, -0.0284,  0.1478],\n",
            "         [ 0.1642, -0.0688, -0.0788,  ...,  0.1020, -0.0203,  0.1081],\n",
            "         [ 0.1841, -0.0955, -0.1055,  ...,  0.0736, -0.0368,  0.1325],\n",
            "         ...,\n",
            "         [ 0.1392, -0.0741, -0.0022,  ...,  0.0682, -0.0229,  0.1543],\n",
            "         [ 0.1184, -0.0618, -0.0441,  ...,  0.0804, -0.0435,  0.1358],\n",
            "         [ 0.1246, -0.0582, -0.0609,  ...,  0.0474,  0.0202,  0.1121]],\n",
            "\n",
            "        [[ 0.0795,  0.0713, -0.0930,  ..., -0.0757, -0.1306,  0.1392],\n",
            "         [ 0.1542,  0.0247, -0.1020,  ..., -0.0686, -0.1142,  0.1144],\n",
            "         [ 0.1537,  0.0602, -0.0376,  ..., -0.1271, -0.1648,  0.0373],\n",
            "         ...,\n",
            "         [ 0.1418,  0.0233, -0.1194,  ..., -0.0283, -0.1448,  0.0979],\n",
            "         [ 0.1418,  0.0233, -0.1194,  ..., -0.0283, -0.1448,  0.0979],\n",
            "         [ 0.1418,  0.0233, -0.1194,  ..., -0.0283, -0.1448,  0.0979]],\n",
            "\n",
            "        [[ 0.1015, -0.0329, -0.0307,  ..., -0.0091, -0.0423,  0.1753],\n",
            "         [ 0.0492, -0.0391, -0.0790,  ..., -0.0392,  0.0170,  0.0947],\n",
            "         [ 0.0206, -0.0390,  0.0097,  ..., -0.0674,  0.0124,  0.1138],\n",
            "         ...,\n",
            "         [ 0.0961, -0.1025, -0.0174,  ..., -0.0607, -0.0162,  0.0704],\n",
            "         [ 0.0961, -0.1025, -0.0174,  ..., -0.0607, -0.0162,  0.0704],\n",
            "         [ 0.0961, -0.1025, -0.0174,  ..., -0.0607, -0.0162,  0.0704]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
        "\n",
        "아래 코드의 TODO 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear projection for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled-dot product attention\n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    # linear projection\n",
        "    ################################################################################\n",
        "    # TODO 1: Implement the forward pass for linear projection.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
        "    \n",
        "    # head만큼 쪼개준다\n",
        "    ################################################################################\n",
        "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)\n",
        "\n",
        "\n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e357bf-558a-4bdc-e6c2-f6b8ea106977"
      },
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0419, -0.0112,  0.1490,  ..., -0.1647,  0.0510, -0.0326],\n",
            "         [ 0.0678,  0.0566,  0.1247,  ..., -0.2711,  0.0454, -0.0792],\n",
            "         [ 0.0473, -0.0124,  0.1608,  ..., -0.1406,  0.0541, -0.0815],\n",
            "         ...,\n",
            "         [ 0.0184, -0.0350,  0.1350,  ..., -0.1821,  0.1410, -0.1443],\n",
            "         [ 0.0184, -0.0350,  0.1350,  ..., -0.1821,  0.1410, -0.1443],\n",
            "         [ 0.0184, -0.0350,  0.1350,  ..., -0.1821,  0.1410, -0.1443]],\n",
            "\n",
            "        [[ 0.3123,  0.0116,  0.2712,  ..., -0.2521,  0.3970, -0.0357],\n",
            "         [ 0.2142, -0.0959,  0.2700,  ..., -0.2405,  0.3860, -0.0278],\n",
            "         [ 0.2883, -0.0547,  0.2722,  ..., -0.2167,  0.3707,  0.0013],\n",
            "         ...,\n",
            "         [ 0.2162, -0.0409,  0.2799,  ..., -0.2539,  0.4262, -0.0722],\n",
            "         [ 0.2162, -0.0409,  0.2799,  ..., -0.2539,  0.4262, -0.0722],\n",
            "         [ 0.2162, -0.0409,  0.2799,  ..., -0.2539,  0.4262, -0.0722]],\n",
            "\n",
            "        [[ 0.2389, -0.0675,  0.1496,  ..., -0.1380,  0.1486,  0.0079],\n",
            "         [ 0.2673,  0.0324,  0.1851,  ..., -0.1095,  0.1566, -0.0154],\n",
            "         [ 0.1630, -0.0178,  0.2354,  ..., -0.1131,  0.2005,  0.0155],\n",
            "         ...,\n",
            "         [ 0.1651, -0.0394,  0.1935,  ..., -0.1371,  0.2071, -0.0395],\n",
            "         [ 0.1651, -0.0394,  0.1935,  ..., -0.1371,  0.2071, -0.0395],\n",
            "         [ 0.1651, -0.0394,  0.1935,  ..., -0.1371,  0.2071, -0.0395]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0381, -0.0700, -0.1037,  ...,  0.1030, -0.0101,  0.0087],\n",
            "         [-0.0384, -0.0543, -0.1198,  ...,  0.1498,  0.0326,  0.0185],\n",
            "         [-0.0421, -0.0974, -0.1329,  ...,  0.1518, -0.0202, -0.0015],\n",
            "         ...,\n",
            "         [-0.0050, -0.1194, -0.1565,  ...,  0.1102,  0.0048,  0.0092],\n",
            "         [-0.0068, -0.0996, -0.1443,  ...,  0.0901,  0.0064,  0.0035],\n",
            "         [-0.0542, -0.0720, -0.1146,  ...,  0.1433, -0.0143,  0.0022]],\n",
            "\n",
            "        [[ 0.1140,  0.0135,  0.0171,  ...,  0.0269,  0.1230, -0.0220],\n",
            "         [ 0.0013, -0.0074,  0.0207,  ...,  0.0499,  0.0729,  0.0392],\n",
            "         [ 0.0390,  0.0458,  0.0792,  ...,  0.0350,  0.0730, -0.0140],\n",
            "         ...,\n",
            "         [-0.0020, -0.0180,  0.0485,  ..., -0.0439,  0.1062, -0.0396],\n",
            "         [-0.0020, -0.0180,  0.0485,  ..., -0.0439,  0.1062, -0.0396],\n",
            "         [-0.0020, -0.0180,  0.0485,  ..., -0.0439,  0.1062, -0.0396]],\n",
            "\n",
            "        [[ 0.1790, -0.0578,  0.0195,  ..., -0.0281,  0.0993,  0.0764],\n",
            "         [ 0.2242, -0.0637,  0.0604,  ..., -0.0159,  0.1302,  0.0557],\n",
            "         [ 0.1588, -0.1124,  0.0982,  ..., -0.0551,  0.0867,  0.0726],\n",
            "         ...,\n",
            "         [ 0.2162, -0.1179,  0.0919,  ..., -0.0601,  0.0915,  0.0054],\n",
            "         [ 0.2162, -0.1179,  0.0919,  ..., -0.0601,  0.0915,  0.0054],\n",
            "         [ 0.2162, -0.1179,  0.0919,  ..., -0.0601,  0.0915,  0.0054]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    }
  ]
}