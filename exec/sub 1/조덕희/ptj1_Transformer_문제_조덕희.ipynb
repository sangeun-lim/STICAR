{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "GiWulKAdmAHX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
        "1. Multi-head attention 및 self-attention 구현.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### 필요 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Req. 2-1 Multi-head self-attention 구조 익히기"
      ],
      "metadata": {
        "id": "HH0VdC4uJJVG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**\n",
        "vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n",
        "\n",
        "각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\n",
        "vocab_size = 100\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "# 길이 맞춰주기 위해 패딩합니다.\n",
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aca7313-25f6-4f4c-b026-0826e82df55d"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 61052.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8ecd7a-af34-4dbc-952c-d3adc8c524dd"
      },
      "source": [
        "data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6076d464-cc5c-4c00-ce41-58b89836543e"
      },
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.4131,  0.1323, -1.2643,  ..., -1.0075, -1.2104, -1.1472],\n",
            "         [ 0.6792,  0.6466,  1.2484,  ..., -0.3469,  1.7738,  1.4269],\n",
            "         [-0.8930,  1.7658,  0.0591,  ..., -0.0337, -0.6905,  1.3835],\n",
            "         ...,\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364]],\n",
            "\n",
            "        [[-1.3189, -0.2551,  0.0405,  ...,  0.0503, -0.0490,  0.1149],\n",
            "         [ 0.2593, -0.8840,  0.1121,  ...,  1.3256,  1.0745,  0.1823],\n",
            "         [ 0.3010, -0.0939, -0.2713,  ...,  1.3444, -0.0170,  0.5121],\n",
            "         ...,\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364]],\n",
            "\n",
            "        [[ 1.2501, -1.6768,  0.2525,  ...,  1.2778, -0.3592,  2.7070],\n",
            "         [-0.5703,  0.3093,  0.1860,  ...,  0.9686,  0.1072,  0.2171],\n",
            "         [-0.4486, -0.9348,  1.4384,  ...,  0.1072, -0.4773, -0.9793],\n",
            "         ...,\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0905, -0.4083,  0.5300,  ...,  0.7907,  0.1025, -1.4815],\n",
            "         [ 0.1313,  0.1873, -0.2926,  ...,  1.0951, -0.4027, -0.3490],\n",
            "         [ 0.3010, -0.0939, -0.2713,  ...,  1.3444, -0.0170,  0.5121],\n",
            "         ...,\n",
            "         [ 0.9226, -0.5415,  1.0278,  ..., -0.8193, -0.8274,  0.2409],\n",
            "         [ 0.6742, -0.2164, -0.3549,  ...,  0.0352,  0.1016,  0.0837],\n",
            "         [ 1.0151,  0.6208,  1.3335,  ..., -0.4005,  0.1112, -0.8595]],\n",
            "\n",
            "        [[-1.3557,  0.6921, -0.6732,  ...,  0.9100,  0.2842,  0.7886],\n",
            "         [ 2.0399,  0.2974,  0.9504,  ..., -0.7956,  1.4477, -1.1887],\n",
            "         [ 0.3886,  1.0653,  1.2049,  ...,  1.3768,  0.0864,  1.5242],\n",
            "         ...,\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364]],\n",
            "\n",
            "        [[-0.5642,  1.0365, -0.7841,  ...,  0.6149,  0.2815,  0.4100],\n",
            "         [ 2.0399,  0.2974,  0.9504,  ..., -0.7956,  1.4477, -1.1887],\n",
            "         [ 1.9961, -0.5621, -0.6294,  ...,  0.3642,  0.0751, -0.3365],\n",
            "         ...,\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364],\n",
            "         [ 0.4488, -0.0648, -1.4498,  ...,  0.1912,  0.4578, -0.7364]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### Linear projection & 여러 head로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca33400e-8ca5-4c49-9551-914925bec290"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdeacdd-5628-465a-cdc5-058f3e17984f"
      },
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# num_heads * d_k로 쪼갠다\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7868a2a-6a42-4430-ced4-2824130c7811"
      },
      "source": [
        "# num_heads를 밖으로 뺌으로써\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
        "\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### Scaled dot-product self-attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8306b61-3bb9-4c30-f00c-f5dea037cdb7"
      },
      "source": [
        "# shape - (L, L)\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "# softmax - row-wise이기 때문에 dim은 -1\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0461, 0.0594, 0.0362,  ..., 0.0694, 0.0694, 0.0694],\n",
            "          [0.0413, 0.0439, 0.0543,  ..., 0.0713, 0.0713, 0.0713],\n",
            "          [0.0490, 0.0281, 0.0586,  ..., 0.0638, 0.0638, 0.0638],\n",
            "          ...,\n",
            "          [0.0404, 0.0582, 0.0322,  ..., 0.0601, 0.0601, 0.0601],\n",
            "          [0.0404, 0.0582, 0.0322,  ..., 0.0601, 0.0601, 0.0601],\n",
            "          [0.0404, 0.0582, 0.0322,  ..., 0.0601, 0.0601, 0.0601]],\n",
            "\n",
            "         [[0.0496, 0.0408, 0.0351,  ..., 0.0652, 0.0652, 0.0652],\n",
            "          [0.0746, 0.0298, 0.0781,  ..., 0.0422, 0.0422, 0.0422],\n",
            "          [0.0416, 0.0720, 0.0406,  ..., 0.0364, 0.0364, 0.0364],\n",
            "          ...,\n",
            "          [0.0377, 0.0335, 0.0759,  ..., 0.0426, 0.0426, 0.0426],\n",
            "          [0.0377, 0.0335, 0.0759,  ..., 0.0426, 0.0426, 0.0426],\n",
            "          [0.0377, 0.0335, 0.0759,  ..., 0.0426, 0.0426, 0.0426]],\n",
            "\n",
            "         [[0.0566, 0.0339, 0.0520,  ..., 0.0481, 0.0481, 0.0481],\n",
            "          [0.0581, 0.0613, 0.0553,  ..., 0.0534, 0.0534, 0.0534],\n",
            "          [0.0648, 0.0406, 0.0622,  ..., 0.0288, 0.0288, 0.0288],\n",
            "          ...,\n",
            "          [0.0462, 0.0491, 0.0613,  ..., 0.0514, 0.0514, 0.0514],\n",
            "          [0.0462, 0.0491, 0.0613,  ..., 0.0514, 0.0514, 0.0514],\n",
            "          [0.0462, 0.0491, 0.0613,  ..., 0.0514, 0.0514, 0.0514]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0478, 0.0423, 0.0623,  ..., 0.0360, 0.0360, 0.0360],\n",
            "          [0.0300, 0.0397, 0.0308,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0617, 0.0595, 0.0884,  ..., 0.0343, 0.0343, 0.0343],\n",
            "          ...,\n",
            "          [0.0312, 0.0589, 0.0601,  ..., 0.0692, 0.0692, 0.0692],\n",
            "          [0.0312, 0.0589, 0.0601,  ..., 0.0692, 0.0692, 0.0692],\n",
            "          [0.0312, 0.0589, 0.0601,  ..., 0.0692, 0.0692, 0.0692]],\n",
            "\n",
            "         [[0.0711, 0.0486, 0.0413,  ..., 0.0348, 0.0348, 0.0348],\n",
            "          [0.0723, 0.0441, 0.0456,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0587, 0.0457, 0.0569,  ..., 0.0413, 0.0413, 0.0413],\n",
            "          ...,\n",
            "          [0.0363, 0.0194, 0.0319,  ..., 0.0479, 0.0479, 0.0479],\n",
            "          [0.0363, 0.0194, 0.0319,  ..., 0.0479, 0.0479, 0.0479],\n",
            "          [0.0363, 0.0194, 0.0319,  ..., 0.0479, 0.0479, 0.0479]],\n",
            "\n",
            "         [[0.0351, 0.0319, 0.0409,  ..., 0.0820, 0.0820, 0.0820],\n",
            "          [0.0638, 0.0360, 0.0285,  ..., 0.0530, 0.0530, 0.0530],\n",
            "          [0.0691, 0.0584, 0.0365,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          ...,\n",
            "          [0.0569, 0.0233, 0.1224,  ..., 0.0378, 0.0378, 0.0378],\n",
            "          [0.0569, 0.0233, 0.1224,  ..., 0.0378, 0.0378, 0.0378],\n",
            "          [0.0569, 0.0233, 0.1224,  ..., 0.0378, 0.0378, 0.0378]]],\n",
            "\n",
            "\n",
            "        [[[0.0379, 0.0298, 0.0591,  ..., 0.0525, 0.0525, 0.0525],\n",
            "          [0.0441, 0.0263, 0.0259,  ..., 0.0565, 0.0565, 0.0565],\n",
            "          [0.0515, 0.0609, 0.0506,  ..., 0.0522, 0.0522, 0.0522],\n",
            "          ...,\n",
            "          [0.0424, 0.0481, 0.0310,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0424, 0.0481, 0.0310,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0424, 0.0481, 0.0310,  ..., 0.0542, 0.0542, 0.0542]],\n",
            "\n",
            "         [[0.0242, 0.0274, 0.0185,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0297, 0.0386, 0.1315,  ..., 0.0474, 0.0474, 0.0474],\n",
            "          [0.0526, 0.0885, 0.0489,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          ...,\n",
            "          [0.0523, 0.0558, 0.0529,  ..., 0.0495, 0.0495, 0.0495],\n",
            "          [0.0523, 0.0558, 0.0529,  ..., 0.0495, 0.0495, 0.0495],\n",
            "          [0.0523, 0.0558, 0.0529,  ..., 0.0495, 0.0495, 0.0495]],\n",
            "\n",
            "         [[0.0737, 0.0826, 0.0575,  ..., 0.0438, 0.0438, 0.0438],\n",
            "          [0.0534, 0.0341, 0.0556,  ..., 0.0526, 0.0526, 0.0526],\n",
            "          [0.0358, 0.0447, 0.0955,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          ...,\n",
            "          [0.0461, 0.0279, 0.0451,  ..., 0.0506, 0.0506, 0.0506],\n",
            "          [0.0461, 0.0279, 0.0451,  ..., 0.0506, 0.0506, 0.0506],\n",
            "          [0.0461, 0.0279, 0.0451,  ..., 0.0506, 0.0506, 0.0506]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0389, 0.0714, 0.0526,  ..., 0.0467, 0.0467, 0.0467],\n",
            "          [0.0398, 0.1025, 0.0319,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          [0.0259, 0.0352, 0.0315,  ..., 0.0574, 0.0574, 0.0574],\n",
            "          ...,\n",
            "          [0.0425, 0.0212, 0.0281,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          [0.0425, 0.0212, 0.0281,  ..., 0.0562, 0.0562, 0.0562],\n",
            "          [0.0425, 0.0212, 0.0281,  ..., 0.0562, 0.0562, 0.0562]],\n",
            "\n",
            "         [[0.0848, 0.0504, 0.0560,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0289, 0.0260, 0.0601,  ..., 0.0548, 0.0548, 0.0548],\n",
            "          [0.0758, 0.0627, 0.0391,  ..., 0.0496, 0.0496, 0.0496],\n",
            "          ...,\n",
            "          [0.0532, 0.0167, 0.0503,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          [0.0532, 0.0167, 0.0503,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          [0.0532, 0.0167, 0.0503,  ..., 0.0480, 0.0480, 0.0480]],\n",
            "\n",
            "         [[0.0954, 0.1089, 0.1358,  ..., 0.0306, 0.0306, 0.0306],\n",
            "          [0.1087, 0.0639, 0.0869,  ..., 0.0428, 0.0428, 0.0428],\n",
            "          [0.0536, 0.0892, 0.0656,  ..., 0.0463, 0.0463, 0.0463],\n",
            "          ...,\n",
            "          [0.0601, 0.0414, 0.0530,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0601, 0.0414, 0.0530,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0601, 0.0414, 0.0530,  ..., 0.0452, 0.0452, 0.0452]]],\n",
            "\n",
            "\n",
            "        [[[0.0689, 0.0428, 0.0387,  ..., 0.0534, 0.0534, 0.0534],\n",
            "          [0.0550, 0.0357, 0.0375,  ..., 0.0557, 0.0557, 0.0557],\n",
            "          [0.0380, 0.0331, 0.0430,  ..., 0.0555, 0.0555, 0.0555],\n",
            "          ...,\n",
            "          [0.0548, 0.0317, 0.0382,  ..., 0.0586, 0.0586, 0.0586],\n",
            "          [0.0548, 0.0317, 0.0382,  ..., 0.0586, 0.0586, 0.0586],\n",
            "          [0.0548, 0.0317, 0.0382,  ..., 0.0586, 0.0586, 0.0586]],\n",
            "\n",
            "         [[0.0413, 0.0329, 0.0249,  ..., 0.0474, 0.0474, 0.0474],\n",
            "          [0.0537, 0.0503, 0.0509,  ..., 0.0571, 0.0571, 0.0571],\n",
            "          [0.0572, 0.0459, 0.0424,  ..., 0.0522, 0.0522, 0.0522],\n",
            "          ...,\n",
            "          [0.0715, 0.0550, 0.0498,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          [0.0715, 0.0550, 0.0498,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          [0.0715, 0.0550, 0.0498,  ..., 0.0454, 0.0454, 0.0454]],\n",
            "\n",
            "         [[0.0230, 0.0532, 0.0428,  ..., 0.0619, 0.0619, 0.0619],\n",
            "          [0.0605, 0.0426, 0.0300,  ..., 0.0519, 0.0519, 0.0519],\n",
            "          [0.0532, 0.0613, 0.0438,  ..., 0.0499, 0.0499, 0.0499],\n",
            "          ...,\n",
            "          [0.0302, 0.0448, 0.0383,  ..., 0.0530, 0.0530, 0.0530],\n",
            "          [0.0302, 0.0448, 0.0383,  ..., 0.0530, 0.0530, 0.0530],\n",
            "          [0.0302, 0.0448, 0.0383,  ..., 0.0530, 0.0530, 0.0530]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0488, 0.0898, 0.0658,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0683, 0.0689, 0.0626,  ..., 0.0384, 0.0384, 0.0384],\n",
            "          [0.0335, 0.0418, 0.0481,  ..., 0.0588, 0.0588, 0.0588],\n",
            "          ...,\n",
            "          [0.0363, 0.0345, 0.0446,  ..., 0.0616, 0.0616, 0.0616],\n",
            "          [0.0363, 0.0345, 0.0446,  ..., 0.0616, 0.0616, 0.0616],\n",
            "          [0.0363, 0.0345, 0.0446,  ..., 0.0616, 0.0616, 0.0616]],\n",
            "\n",
            "         [[0.0477, 0.0506, 0.0382,  ..., 0.0464, 0.0464, 0.0464],\n",
            "          [0.0541, 0.0345, 0.0872,  ..., 0.0375, 0.0375, 0.0375],\n",
            "          [0.0519, 0.0789, 0.0601,  ..., 0.0487, 0.0487, 0.0487],\n",
            "          ...,\n",
            "          [0.0465, 0.1174, 0.0618,  ..., 0.0424, 0.0424, 0.0424],\n",
            "          [0.0465, 0.1174, 0.0618,  ..., 0.0424, 0.0424, 0.0424],\n",
            "          [0.0465, 0.1174, 0.0618,  ..., 0.0424, 0.0424, 0.0424]],\n",
            "\n",
            "         [[0.0897, 0.0661, 0.0402,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          [0.0462, 0.0261, 0.0512,  ..., 0.0614, 0.0614, 0.0614],\n",
            "          [0.0629, 0.0291, 0.0642,  ..., 0.0423, 0.0423, 0.0423],\n",
            "          ...,\n",
            "          [0.0330, 0.0395, 0.0568,  ..., 0.0476, 0.0476, 0.0476],\n",
            "          [0.0330, 0.0395, 0.0568,  ..., 0.0476, 0.0476, 0.0476],\n",
            "          [0.0330, 0.0395, 0.0568,  ..., 0.0476, 0.0476, 0.0476]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0866, 0.0450, 0.0328,  ..., 0.0542, 0.0391, 0.0688],\n",
            "          [0.0294, 0.0441, 0.0201,  ..., 0.0305, 0.0417, 0.0776],\n",
            "          [0.0422, 0.0437, 0.0648,  ..., 0.0470, 0.0565, 0.0548],\n",
            "          ...,\n",
            "          [0.0440, 0.0442, 0.0385,  ..., 0.0994, 0.0668, 0.0540],\n",
            "          [0.0450, 0.0448, 0.0349,  ..., 0.0407, 0.0679, 0.0509],\n",
            "          [0.0464, 0.0370, 0.0556,  ..., 0.0721, 0.0575, 0.0505]],\n",
            "\n",
            "         [[0.1062, 0.0561, 0.0651,  ..., 0.0435, 0.0869, 0.0695],\n",
            "          [0.0459, 0.0662, 0.0301,  ..., 0.0401, 0.0479, 0.0724],\n",
            "          [0.0394, 0.1085, 0.0417,  ..., 0.0661, 0.0433, 0.0382],\n",
            "          ...,\n",
            "          [0.0625, 0.0618, 0.0385,  ..., 0.0652, 0.0683, 0.0317],\n",
            "          [0.0328, 0.0513, 0.0372,  ..., 0.0397, 0.0612, 0.0443],\n",
            "          [0.0671, 0.0467, 0.0744,  ..., 0.0763, 0.0328, 0.0493]],\n",
            "\n",
            "         [[0.0651, 0.0218, 0.0496,  ..., 0.0539, 0.0339, 0.0431],\n",
            "          [0.0224, 0.0470, 0.0499,  ..., 0.0469, 0.0620, 0.0752],\n",
            "          [0.0282, 0.0635, 0.0822,  ..., 0.0532, 0.0569, 0.0451],\n",
            "          ...,\n",
            "          [0.0233, 0.0829, 0.0328,  ..., 0.0454, 0.0475, 0.0649],\n",
            "          [0.0920, 0.0309, 0.0538,  ..., 0.0536, 0.0432, 0.0486],\n",
            "          [0.0548, 0.0661, 0.0709,  ..., 0.0485, 0.0443, 0.0337]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0657, 0.0397, 0.0361,  ..., 0.0368, 0.0485, 0.0733],\n",
            "          [0.0294, 0.0424, 0.0796,  ..., 0.0555, 0.0519, 0.0742],\n",
            "          [0.0389, 0.0637, 0.0475,  ..., 0.0378, 0.0355, 0.0397],\n",
            "          ...,\n",
            "          [0.0488, 0.0608, 0.0438,  ..., 0.0410, 0.0522, 0.0522],\n",
            "          [0.0455, 0.0299, 0.0408,  ..., 0.0633, 0.0276, 0.0520],\n",
            "          [0.0490, 0.0656, 0.0463,  ..., 0.0650, 0.0403, 0.0462]],\n",
            "\n",
            "         [[0.0280, 0.0446, 0.0248,  ..., 0.0447, 0.0711, 0.1018],\n",
            "          [0.0459, 0.0674, 0.0423,  ..., 0.0264, 0.0352, 0.0658],\n",
            "          [0.0263, 0.0455, 0.0471,  ..., 0.0682, 0.0407, 0.0664],\n",
            "          ...,\n",
            "          [0.0328, 0.0724, 0.0202,  ..., 0.0445, 0.0661, 0.0455],\n",
            "          [0.0587, 0.0366, 0.0222,  ..., 0.1059, 0.0301, 0.0585],\n",
            "          [0.0325, 0.0604, 0.0520,  ..., 0.0581, 0.0711, 0.0553]],\n",
            "\n",
            "         [[0.0456, 0.0747, 0.0534,  ..., 0.0584, 0.0666, 0.0441],\n",
            "          [0.0351, 0.0354, 0.0846,  ..., 0.0350, 0.0417, 0.0463],\n",
            "          [0.0266, 0.0283, 0.0649,  ..., 0.0480, 0.0569, 0.0505],\n",
            "          ...,\n",
            "          [0.0334, 0.0667, 0.0644,  ..., 0.0545, 0.0567, 0.0379],\n",
            "          [0.0576, 0.0499, 0.0480,  ..., 0.0317, 0.0687, 0.0455],\n",
            "          [0.0409, 0.0341, 0.0605,  ..., 0.0543, 0.0865, 0.0453]]],\n",
            "\n",
            "\n",
            "        [[[0.0537, 0.0337, 0.0381,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0363, 0.0464, 0.0625,  ..., 0.0335, 0.0335, 0.0335],\n",
            "          [0.0493, 0.0316, 0.0578,  ..., 0.0678, 0.0678, 0.0678],\n",
            "          ...,\n",
            "          [0.0703, 0.0613, 0.0255,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0703, 0.0613, 0.0255,  ..., 0.0492, 0.0492, 0.0492],\n",
            "          [0.0703, 0.0613, 0.0255,  ..., 0.0492, 0.0492, 0.0492]],\n",
            "\n",
            "         [[0.0219, 0.0648, 0.0307,  ..., 0.0644, 0.0644, 0.0644],\n",
            "          [0.0650, 0.0530, 0.0571,  ..., 0.0691, 0.0691, 0.0691],\n",
            "          [0.0462, 0.0394, 0.0610,  ..., 0.0327, 0.0327, 0.0327],\n",
            "          ...,\n",
            "          [0.0610, 0.0689, 0.0479,  ..., 0.0413, 0.0413, 0.0413],\n",
            "          [0.0610, 0.0689, 0.0479,  ..., 0.0413, 0.0413, 0.0413],\n",
            "          [0.0610, 0.0689, 0.0479,  ..., 0.0413, 0.0413, 0.0413]],\n",
            "\n",
            "         [[0.0266, 0.0430, 0.0688,  ..., 0.0529, 0.0529, 0.0529],\n",
            "          [0.0605, 0.0294, 0.0674,  ..., 0.0569, 0.0569, 0.0569],\n",
            "          [0.0776, 0.0441, 0.0293,  ..., 0.0420, 0.0420, 0.0420],\n",
            "          ...,\n",
            "          [0.0400, 0.0561, 0.0606,  ..., 0.0551, 0.0551, 0.0551],\n",
            "          [0.0400, 0.0561, 0.0606,  ..., 0.0551, 0.0551, 0.0551],\n",
            "          [0.0400, 0.0561, 0.0606,  ..., 0.0551, 0.0551, 0.0551]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0349, 0.0363, 0.0364,  ..., 0.0643, 0.0643, 0.0643],\n",
            "          [0.0312, 0.0478, 0.0544,  ..., 0.0503, 0.0503, 0.0503],\n",
            "          [0.0551, 0.0521, 0.0396,  ..., 0.0359, 0.0359, 0.0359],\n",
            "          ...,\n",
            "          [0.0538, 0.0314, 0.0324,  ..., 0.0767, 0.0767, 0.0767],\n",
            "          [0.0538, 0.0314, 0.0324,  ..., 0.0767, 0.0767, 0.0767],\n",
            "          [0.0538, 0.0314, 0.0324,  ..., 0.0767, 0.0767, 0.0767]],\n",
            "\n",
            "         [[0.0455, 0.0349, 0.0545,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0320, 0.0332, 0.0456,  ..., 0.0669, 0.0669, 0.0669],\n",
            "          [0.0359, 0.0786, 0.0510,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          ...,\n",
            "          [0.0512, 0.0783, 0.0541,  ..., 0.0458, 0.0458, 0.0458],\n",
            "          [0.0512, 0.0783, 0.0541,  ..., 0.0458, 0.0458, 0.0458],\n",
            "          [0.0512, 0.0783, 0.0541,  ..., 0.0458, 0.0458, 0.0458]],\n",
            "\n",
            "         [[0.0448, 0.0549, 0.0328,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.1062, 0.0415, 0.0353,  ..., 0.0641, 0.0641, 0.0641],\n",
            "          [0.0334, 0.0679, 0.0771,  ..., 0.0382, 0.0382, 0.0382],\n",
            "          ...,\n",
            "          [0.0677, 0.0592, 0.0470,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0677, 0.0592, 0.0470,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0677, 0.0592, 0.0470,  ..., 0.0400, 0.0400, 0.0400]]],\n",
            "\n",
            "\n",
            "        [[[0.0595, 0.0614, 0.0745,  ..., 0.0440, 0.0440, 0.0440],\n",
            "          [0.0667, 0.0482, 0.0508,  ..., 0.0348, 0.0348, 0.0348],\n",
            "          [0.0622, 0.0452, 0.0546,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          ...,\n",
            "          [0.0330, 0.0713, 0.0316,  ..., 0.0572, 0.0572, 0.0572],\n",
            "          [0.0330, 0.0713, 0.0316,  ..., 0.0572, 0.0572, 0.0572],\n",
            "          [0.0330, 0.0713, 0.0316,  ..., 0.0572, 0.0572, 0.0572]],\n",
            "\n",
            "         [[0.0432, 0.0409, 0.0338,  ..., 0.0663, 0.0663, 0.0663],\n",
            "          [0.0387, 0.0520, 0.0519,  ..., 0.0678, 0.0678, 0.0678],\n",
            "          [0.0308, 0.0590, 0.0579,  ..., 0.0550, 0.0550, 0.0550],\n",
            "          ...,\n",
            "          [0.0492, 0.0774, 0.0368,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          [0.0492, 0.0774, 0.0368,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          [0.0492, 0.0774, 0.0368,  ..., 0.0465, 0.0465, 0.0465]],\n",
            "\n",
            "         [[0.0846, 0.0486, 0.0497,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          [0.0765, 0.0261, 0.0502,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0473, 0.0381, 0.0656,  ..., 0.0473, 0.0473, 0.0473],\n",
            "          ...,\n",
            "          [0.0547, 0.0541, 0.0386,  ..., 0.0532, 0.0532, 0.0532],\n",
            "          [0.0547, 0.0541, 0.0386,  ..., 0.0532, 0.0532, 0.0532],\n",
            "          [0.0547, 0.0541, 0.0386,  ..., 0.0532, 0.0532, 0.0532]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0759, 0.0365, 0.0466,  ..., 0.0531, 0.0531, 0.0531],\n",
            "          [0.0578, 0.0508, 0.0457,  ..., 0.0534, 0.0534, 0.0534],\n",
            "          [0.0760, 0.0576, 0.0360,  ..., 0.0343, 0.0343, 0.0343],\n",
            "          ...,\n",
            "          [0.0509, 0.0292, 0.0376,  ..., 0.0712, 0.0712, 0.0712],\n",
            "          [0.0509, 0.0292, 0.0376,  ..., 0.0712, 0.0712, 0.0712],\n",
            "          [0.0509, 0.0292, 0.0376,  ..., 0.0712, 0.0712, 0.0712]],\n",
            "\n",
            "         [[0.0826, 0.0836, 0.0374,  ..., 0.0318, 0.0318, 0.0318],\n",
            "          [0.0314, 0.0306, 0.0528,  ..., 0.0619, 0.0619, 0.0619],\n",
            "          [0.0328, 0.0734, 0.0331,  ..., 0.0521, 0.0521, 0.0521],\n",
            "          ...,\n",
            "          [0.0826, 0.0723, 0.0472,  ..., 0.0423, 0.0423, 0.0423],\n",
            "          [0.0826, 0.0723, 0.0472,  ..., 0.0423, 0.0423, 0.0423],\n",
            "          [0.0826, 0.0723, 0.0472,  ..., 0.0423, 0.0423, 0.0423]],\n",
            "\n",
            "         [[0.0565, 0.0379, 0.0307,  ..., 0.0595, 0.0595, 0.0595],\n",
            "          [0.0441, 0.0400, 0.0540,  ..., 0.0618, 0.0618, 0.0618],\n",
            "          [0.0639, 0.0528, 0.0555,  ..., 0.0381, 0.0381, 0.0381],\n",
            "          ...,\n",
            "          [0.0534, 0.0634, 0.0354,  ..., 0.0428, 0.0428, 0.0428],\n",
            "          [0.0534, 0.0634, 0.0354,  ..., 0.0428, 0.0428, 0.0428],\n",
            "          [0.0534, 0.0634, 0.0354,  ..., 0.0428, 0.0428, 0.0428]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c60da2-483d-4c71-e59f-d6e33d4b4cea"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### 각 head의 결과물 병합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ad7ff7-2975-41c0-a98f-815a81d153a3"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a29523-c918-43e1-cd7b-dc48a46de03a"
      },
      "source": [
        "# w_0 : (d_model, d_model)\n",
        "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0141, -0.1073,  0.0781,  ...,  0.0659,  0.1357,  0.0876],\n",
            "         [-0.0086, -0.1131,  0.0404,  ...,  0.0483,  0.1232,  0.0951],\n",
            "         [ 0.0463, -0.1708, -0.0037,  ...,  0.0350,  0.0904,  0.0354],\n",
            "         ...,\n",
            "         [ 0.0164, -0.1662,  0.0309,  ..., -0.0067,  0.1350,  0.0713],\n",
            "         [ 0.0164, -0.1662,  0.0309,  ..., -0.0067,  0.1350,  0.0713],\n",
            "         [ 0.0164, -0.1662,  0.0309,  ..., -0.0067,  0.1350,  0.0713]],\n",
            "\n",
            "        [[-0.1126, -0.2621,  0.1964,  ...,  0.1528,  0.4709,  0.3580],\n",
            "         [-0.0603, -0.2757,  0.1923,  ...,  0.1401,  0.4854,  0.3843],\n",
            "         [-0.0072, -0.2726,  0.1661,  ...,  0.1287,  0.4862,  0.3150],\n",
            "         ...,\n",
            "         [-0.0451, -0.2731,  0.1589,  ...,  0.1569,  0.5206,  0.3342],\n",
            "         [-0.0451, -0.2731,  0.1589,  ...,  0.1569,  0.5206,  0.3342],\n",
            "         [-0.0451, -0.2731,  0.1589,  ...,  0.1569,  0.5206,  0.3342]],\n",
            "\n",
            "        [[-0.0547, -0.0777,  0.0658,  ...,  0.0826,  0.3953,  0.2767],\n",
            "         [ 0.0202, -0.0791,  0.0453,  ...,  0.0555,  0.4318,  0.2795],\n",
            "         [ 0.0513, -0.0738,  0.0519,  ...,  0.1068,  0.3503,  0.2900],\n",
            "         ...,\n",
            "         [ 0.0092, -0.1109,  0.0691,  ...,  0.0680,  0.4065,  0.2580],\n",
            "         [ 0.0092, -0.1109,  0.0691,  ...,  0.0680,  0.4065,  0.2580],\n",
            "         [ 0.0092, -0.1109,  0.0691,  ...,  0.0680,  0.4065,  0.2580]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0277,  0.0056,  0.0210,  ...,  0.0379,  0.0502, -0.0283],\n",
            "         [-0.0558,  0.0517,  0.0474,  ..., -0.0062,  0.0050, -0.0792],\n",
            "         [-0.0469,  0.0116,  0.0703,  ...,  0.0164,  0.0480, -0.0633],\n",
            "         ...,\n",
            "         [ 0.0128, -0.0467,  0.1195,  ..., -0.0090,  0.0117, -0.0250],\n",
            "         [-0.0705,  0.0489,  0.0809,  ..., -0.0335,  0.0093, -0.0479],\n",
            "         [-0.0668, -0.0705,  0.0781,  ..., -0.0170,  0.0357, -0.0074]],\n",
            "\n",
            "        [[ 0.0432, -0.0667,  0.0054,  ...,  0.0152,  0.0778,  0.0877],\n",
            "         [ 0.0676, -0.0909, -0.0288,  ...,  0.0410,  0.0739,  0.1480],\n",
            "         [ 0.0470, -0.0277, -0.0418,  ...,  0.0665,  0.0841,  0.1082],\n",
            "         ...,\n",
            "         [ 0.0402, -0.0641,  0.0016,  ...,  0.0101,  0.0994,  0.0848],\n",
            "         [ 0.0402, -0.0641,  0.0016,  ...,  0.0101,  0.0994,  0.0848],\n",
            "         [ 0.0402, -0.0641,  0.0016,  ...,  0.0101,  0.0994,  0.0848]],\n",
            "\n",
            "        [[ 0.0716, -0.0899,  0.0241,  ...,  0.0587,  0.2589,  0.1321],\n",
            "         [ 0.0782, -0.1486,  0.0434,  ...,  0.0721,  0.2526,  0.1066],\n",
            "         [ 0.0668, -0.1157,  0.0097,  ...,  0.0422,  0.2705,  0.0921],\n",
            "         ...,\n",
            "         [ 0.0058, -0.1306,  0.0721,  ...,  0.0414,  0.2988,  0.1153],\n",
            "         [ 0.0058, -0.1306,  0.0721,  ...,  0.0414,  0.2988,  0.1153],\n",
            "         [ 0.0058, -0.1306,  0.0721,  ...,  0.0414,  0.2988,  0.1153]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
        "\n",
        "아래 코드의 TODO 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear projection for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled-dot product attention\n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    # linear projection\n",
        "    ################################################################################\n",
        "    # TODO 1: Implement the forward pass for linear projection.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = w_q(batch_emb)\n",
        "    k = w_k(batch_emb)\n",
        "    v = w_v(batch_emb)\n",
        "\n",
        "\n",
        "    # head만큼 쪼개준다\n",
        "    ################################################################################\n",
        "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  \n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  \n",
        "    v = v.view(batch_size, -1, num_heads, d_k) \n",
        "    \n",
        "\n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c12443a-a51e-495d-975f-171cc674ff16"
      },
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.5063e-01,  5.7255e-02,  1.2729e-01,  ..., -7.3855e-02,\n",
            "           7.0249e-02,  1.3474e-01],\n",
            "         [-1.5924e-01,  8.6500e-02,  1.7459e-01,  ..., -8.9794e-02,\n",
            "           8.0234e-02,  1.4159e-01],\n",
            "         [-1.5543e-01,  1.0135e-01,  1.5755e-01,  ..., -1.7906e-02,\n",
            "           8.8580e-02,  1.4999e-01],\n",
            "         ...,\n",
            "         [-2.4042e-01,  6.5054e-02,  1.9314e-01,  ..., -2.5424e-02,\n",
            "           8.0621e-02,  1.7718e-01],\n",
            "         [-2.4042e-01,  6.5054e-02,  1.9314e-01,  ..., -2.5424e-02,\n",
            "           8.0621e-02,  1.7718e-01],\n",
            "         [-2.4042e-01,  6.5054e-02,  1.9314e-01,  ..., -2.5424e-02,\n",
            "           8.0621e-02,  1.7718e-01]],\n",
            "\n",
            "        [[-1.2440e-01, -1.2262e-01,  3.3839e-02,  ..., -1.2694e-01,\n",
            "           3.7475e-01,  2.4266e-02],\n",
            "         [-1.6041e-01, -9.5337e-02,  2.6799e-02,  ..., -1.7088e-01,\n",
            "           4.1076e-01, -2.6719e-02],\n",
            "         [-1.5907e-01, -9.1937e-02,  1.9654e-02,  ..., -1.9577e-01,\n",
            "           3.8025e-01, -1.7951e-02],\n",
            "         ...,\n",
            "         [-1.5510e-01, -1.3844e-01,  2.0978e-02,  ..., -1.9096e-01,\n",
            "           3.7980e-01,  2.2782e-02],\n",
            "         [-1.5510e-01, -1.3844e-01,  2.0978e-02,  ..., -1.9096e-01,\n",
            "           3.7980e-01,  2.2782e-02],\n",
            "         [-1.5510e-01, -1.3844e-01,  2.0978e-02,  ..., -1.9096e-01,\n",
            "           3.7980e-01,  2.2782e-02]],\n",
            "\n",
            "        [[-3.9826e-02, -7.0001e-02, -3.0008e-03,  ..., -1.2303e-01,\n",
            "           1.7504e-01,  9.6645e-02],\n",
            "         [-5.1634e-02, -1.8529e-01, -7.4001e-02,  ..., -1.3987e-01,\n",
            "           1.9953e-01,  9.1271e-02],\n",
            "         [-7.9833e-02, -9.4695e-02, -3.6747e-03,  ..., -1.5857e-01,\n",
            "           2.0073e-01,  8.2736e-02],\n",
            "         ...,\n",
            "         [-4.1425e-02, -1.2507e-01,  1.4946e-02,  ..., -1.5912e-01,\n",
            "           1.6639e-01,  8.2580e-02],\n",
            "         [-4.1425e-02, -1.2507e-01,  1.4946e-02,  ..., -1.5912e-01,\n",
            "           1.6639e-01,  8.2580e-02],\n",
            "         [-4.1425e-02, -1.2507e-01,  1.4946e-02,  ..., -1.5912e-01,\n",
            "           1.6639e-01,  8.2580e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1348e-01, -3.6660e-02, -1.1794e-01,  ..., -9.7372e-02,\n",
            "          -3.1497e-02,  9.7865e-02],\n",
            "         [ 1.5571e-01, -5.1366e-02, -8.0137e-02,  ..., -1.0716e-01,\n",
            "           2.7216e-04,  9.8135e-02],\n",
            "         [ 1.2711e-01, -2.3149e-02, -1.5151e-01,  ..., -7.1831e-02,\n",
            "           1.7128e-03,  7.5831e-02],\n",
            "         ...,\n",
            "         [ 1.9205e-01, -9.0536e-02, -1.0352e-01,  ..., -7.8609e-02,\n",
            "          -6.6131e-02,  1.0608e-01],\n",
            "         [ 1.0634e-01,  3.2572e-02, -9.1740e-02,  ..., -1.0180e-01,\n",
            "          -4.6712e-02,  1.3259e-01],\n",
            "         [ 1.2062e-01, -1.8271e-02, -1.0661e-01,  ..., -7.1152e-02,\n",
            "          -5.6155e-02,  5.3087e-02]],\n",
            "\n",
            "        [[-1.0549e-01,  2.2118e-02,  5.7915e-02,  ..., -1.4184e-01,\n",
            "           6.6393e-02,  1.0349e-01],\n",
            "         [-1.0373e-01,  5.9331e-03,  5.4430e-03,  ..., -1.3019e-01,\n",
            "           1.6210e-01,  1.1987e-01],\n",
            "         [-6.7449e-02,  9.2376e-02,  6.5168e-02,  ..., -1.3665e-01,\n",
            "           9.3117e-02,  9.6519e-02],\n",
            "         ...,\n",
            "         [-7.2890e-02,  1.8140e-02,  5.4773e-02,  ..., -1.5184e-01,\n",
            "           6.1141e-02,  8.1254e-02],\n",
            "         [-7.2890e-02,  1.8140e-02,  5.4773e-02,  ..., -1.5184e-01,\n",
            "           6.1141e-02,  8.1254e-02],\n",
            "         [-7.2890e-02,  1.8140e-02,  5.4773e-02,  ..., -1.5184e-01,\n",
            "           6.1141e-02,  8.1254e-02]],\n",
            "\n",
            "        [[-9.7363e-02, -2.8133e-02, -2.0309e-02,  ..., -1.8254e-01,\n",
            "           2.2084e-01,  1.2298e-01],\n",
            "         [-8.8612e-02, -4.3799e-02, -2.9571e-02,  ..., -1.9853e-01,\n",
            "           2.0505e-01,  1.1895e-01],\n",
            "         [-4.8741e-02, -1.0223e-02, -1.1275e-02,  ..., -1.3429e-01,\n",
            "           1.4381e-01,  8.1389e-02],\n",
            "         ...,\n",
            "         [-7.8844e-02, -2.9853e-02,  2.1597e-02,  ..., -1.6925e-01,\n",
            "           1.4399e-01,  1.1707e-01],\n",
            "         [-7.8844e-02, -2.9853e-02,  2.1597e-02,  ..., -1.6925e-01,\n",
            "           1.4399e-01,  1.1707e-01],\n",
            "         [-7.8844e-02, -2.9853e-02,  2.1597e-02,  ..., -1.6925e-01,\n",
            "           1.4399e-01,  1.1707e-01]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTku1fySVR3L"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}