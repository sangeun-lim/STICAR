{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "GiWulKAdmAHX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n",
        "1. Multi-head attention 및 self-attention 구현.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### 필요 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Req. 2-1 Multi-head self-attention 구조 익히기"
      ],
      "metadata": {
        "id": "HH0VdC4uJJVG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**\n",
        "vocab_size 100인 가상의 시퀀스 데이터를 생성합니다. \n",
        "\n",
        "각 데이터에 할당된 숫자는 tokenizing과 정수화가 이뤄진 형태입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\n",
        "vocab_size = 100\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "# 길이 맞춰주기 위해 패딩합니다.\n",
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1604ff5-7c96-4231-afc7-e900b014629a"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 17593.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07e7e03-78ce-4af3-c569-2163fc1ea496"
      },
      "source": [
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # head의 개수\n",
        "\n",
        "# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: batch size, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5390886e-312f-4141-f1e9-9b53bd43ad04"
      },
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.8299, -1.0274,  0.2678,  ...,  0.9656, -0.8258,  2.0953],\n",
            "         [-0.5363, -1.5044, -0.1813,  ...,  0.9791, -0.8086,  1.3326],\n",
            "         [-0.8490, -1.3976, -0.0542,  ...,  1.3797,  0.6005, -0.8897],\n",
            "         ...,\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315]],\n",
            "\n",
            "        [[-0.3280,  0.6802,  0.5387,  ...,  0.3527, -0.2873,  1.3021],\n",
            "         [ 0.1888, -0.9981, -0.4318,  ...,  0.8275,  0.1479,  0.2592],\n",
            "         [-0.3581, -0.3562,  0.8462,  ...,  0.2870,  0.6184,  0.1386],\n",
            "         ...,\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315]],\n",
            "\n",
            "        [[-0.1300, -0.2602, -0.4899,  ...,  0.0735,  2.4445,  0.6219],\n",
            "         [ 0.8364, -0.8342,  0.3948,  ...,  0.8762,  0.9679, -0.6287],\n",
            "         [-0.8069, -0.0665,  1.4606,  ...,  1.6922, -0.7271, -0.2460],\n",
            "         ...,\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.5011, -0.7790, -0.7729,  ...,  0.3205, -0.4214, -0.2181],\n",
            "         [-0.4439, -0.5431, -1.1669,  ...,  0.9903,  0.0733, -2.9094],\n",
            "         [-0.3581, -0.3562,  0.8462,  ...,  0.2870,  0.6184,  0.1386],\n",
            "         ...,\n",
            "         [ 0.1666,  0.9806, -0.0670,  ..., -0.0055,  1.7451, -0.2899],\n",
            "         [-0.7328, -0.4621, -1.6875,  ..., -0.2302,  1.2630,  2.8218],\n",
            "         [ 1.0184, -0.3900,  0.1047,  ..., -1.0805,  1.9637,  0.1113]],\n",
            "\n",
            "        [[-0.6187, -0.0442, -0.9108,  ..., -1.1240, -1.3902, -0.2810],\n",
            "         [ 0.7800,  0.2932,  0.2030,  ...,  0.0240,  1.0353, -1.0404],\n",
            "         [ 0.5586, -1.6510,  1.1680,  ...,  0.5197,  0.4151,  1.5230],\n",
            "         ...,\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315]],\n",
            "\n",
            "        [[-2.0215, -0.3194,  1.9521,  ...,  1.5460,  1.6532,  0.5114],\n",
            "         [ 0.7800,  0.2932,  0.2030,  ...,  0.0240,  1.0353, -1.0404],\n",
            "         [-0.0872, -0.6987, -1.2951,  ...,  1.9520, -1.6484, -2.5814],\n",
            "         ...,\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315],\n",
            "         [ 0.7753,  0.2142,  0.0456,  ..., -0.2554,  1.1011,  0.4315]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### Linear projection & 여러 head로 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4871cab7-7816-44c1-fbaa-6268d43628fb"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcYLZYvJT_1"
      },
      "source": [
        "- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n",
        "- 구현에서는 Wq, Wk, Wv 한 개씩\n",
        "- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f18ca6f-1cc8-4842-f79e-91aba271fc0e"
      },
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads\n",
        "\n",
        "# num_heads * d_k로 쪼갠다\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d3ae39-4d98-4641-f36f-5ba2401db021"
      },
      "source": [
        "# num_heads를 밖으로 뺌으로써\n",
        "# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n",
        "\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### Scaled dot-product self-attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dcdbc51-7490-47e9-8dc5-168153c6e6be"
      },
      "source": [
        "# shape - (L, L)\n",
        "# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n",
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "# softmax - row-wise이기 때문에 dim은 -1\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0478, 0.0460, 0.0667,  ..., 0.0318, 0.0318, 0.0318],\n",
            "          [0.0934, 0.0463, 0.0583,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          [0.0351, 0.0502, 0.0856,  ..., 0.0629, 0.0629, 0.0629],\n",
            "          ...,\n",
            "          [0.0384, 0.0524, 0.0689,  ..., 0.0369, 0.0369, 0.0369],\n",
            "          [0.0384, 0.0524, 0.0689,  ..., 0.0369, 0.0369, 0.0369],\n",
            "          [0.0384, 0.0524, 0.0689,  ..., 0.0369, 0.0369, 0.0369]],\n",
            "\n",
            "         [[0.0374, 0.0378, 0.0504,  ..., 0.0362, 0.0362, 0.0362],\n",
            "          [0.0312, 0.0799, 0.0456,  ..., 0.0572, 0.0572, 0.0572],\n",
            "          [0.0602, 0.0407, 0.0275,  ..., 0.0571, 0.0571, 0.0571],\n",
            "          ...,\n",
            "          [0.0607, 0.0558, 0.0573,  ..., 0.0438, 0.0438, 0.0438],\n",
            "          [0.0607, 0.0558, 0.0573,  ..., 0.0438, 0.0438, 0.0438],\n",
            "          [0.0607, 0.0558, 0.0573,  ..., 0.0438, 0.0438, 0.0438]],\n",
            "\n",
            "         [[0.0247, 0.0427, 0.0314,  ..., 0.0654, 0.0654, 0.0654],\n",
            "          [0.0310, 0.0879, 0.0359,  ..., 0.0422, 0.0422, 0.0422],\n",
            "          [0.0539, 0.0492, 0.0637,  ..., 0.0476, 0.0476, 0.0476],\n",
            "          ...,\n",
            "          [0.0553, 0.0436, 0.0677,  ..., 0.0318, 0.0318, 0.0318],\n",
            "          [0.0553, 0.0436, 0.0677,  ..., 0.0318, 0.0318, 0.0318],\n",
            "          [0.0553, 0.0436, 0.0677,  ..., 0.0318, 0.0318, 0.0318]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0433, 0.0609, 0.0441,  ..., 0.0291, 0.0291, 0.0291],\n",
            "          [0.0247, 0.0764, 0.0434,  ..., 0.0597, 0.0597, 0.0597],\n",
            "          [0.0353, 0.0317, 0.0755,  ..., 0.0440, 0.0440, 0.0440],\n",
            "          ...,\n",
            "          [0.0312, 0.0641, 0.0491,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          [0.0312, 0.0641, 0.0491,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          [0.0312, 0.0641, 0.0491,  ..., 0.0454, 0.0454, 0.0454]],\n",
            "\n",
            "         [[0.0296, 0.0404, 0.0631,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0426, 0.0392, 0.0426,  ..., 0.0575, 0.0575, 0.0575],\n",
            "          [0.0557, 0.0479, 0.0481,  ..., 0.0372, 0.0372, 0.0372],\n",
            "          ...,\n",
            "          [0.0560, 0.0629, 0.0491,  ..., 0.0497, 0.0497, 0.0497],\n",
            "          [0.0560, 0.0629, 0.0491,  ..., 0.0497, 0.0497, 0.0497],\n",
            "          [0.0560, 0.0629, 0.0491,  ..., 0.0497, 0.0497, 0.0497]],\n",
            "\n",
            "         [[0.0545, 0.0530, 0.0491,  ..., 0.0595, 0.0595, 0.0595],\n",
            "          [0.0741, 0.0388, 0.0684,  ..., 0.0362, 0.0362, 0.0362],\n",
            "          [0.0558, 0.0549, 0.0555,  ..., 0.0441, 0.0441, 0.0441],\n",
            "          ...,\n",
            "          [0.0393, 0.0460, 0.0555,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0393, 0.0460, 0.0555,  ..., 0.0403, 0.0403, 0.0403],\n",
            "          [0.0393, 0.0460, 0.0555,  ..., 0.0403, 0.0403, 0.0403]]],\n",
            "\n",
            "\n",
            "        [[[0.0568, 0.1076, 0.0413,  ..., 0.0476, 0.0476, 0.0476],\n",
            "          [0.0317, 0.0518, 0.0524,  ..., 0.0526, 0.0526, 0.0526],\n",
            "          [0.0725, 0.0521, 0.0457,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          ...,\n",
            "          [0.0524, 0.0422, 0.0468,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          [0.0524, 0.0422, 0.0468,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          [0.0524, 0.0422, 0.0468,  ..., 0.0451, 0.0451, 0.0451]],\n",
            "\n",
            "         [[0.0552, 0.0543, 0.0460,  ..., 0.0477, 0.0477, 0.0477],\n",
            "          [0.0341, 0.0453, 0.1184,  ..., 0.0467, 0.0467, 0.0467],\n",
            "          [0.0324, 0.0458, 0.0267,  ..., 0.0531, 0.0531, 0.0531],\n",
            "          ...,\n",
            "          [0.0475, 0.0532, 0.0448,  ..., 0.0507, 0.0507, 0.0507],\n",
            "          [0.0475, 0.0532, 0.0448,  ..., 0.0507, 0.0507, 0.0507],\n",
            "          [0.0475, 0.0532, 0.0448,  ..., 0.0507, 0.0507, 0.0507]],\n",
            "\n",
            "         [[0.0519, 0.0553, 0.0557,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0883, 0.0858, 0.0735,  ..., 0.0426, 0.0426, 0.0426],\n",
            "          [0.0966, 0.0726, 0.0539,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          ...,\n",
            "          [0.1349, 0.0495, 0.0718,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          [0.1349, 0.0495, 0.0718,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          [0.1349, 0.0495, 0.0718,  ..., 0.0410, 0.0410, 0.0410]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1209, 0.0588, 0.0593,  ..., 0.0424, 0.0424, 0.0424],\n",
            "          [0.0321, 0.0182, 0.0266,  ..., 0.0553, 0.0553, 0.0553],\n",
            "          [0.0636, 0.0637, 0.0912,  ..., 0.0421, 0.0421, 0.0421],\n",
            "          ...,\n",
            "          [0.0673, 0.0332, 0.0342,  ..., 0.0514, 0.0514, 0.0514],\n",
            "          [0.0673, 0.0332, 0.0342,  ..., 0.0514, 0.0514, 0.0514],\n",
            "          [0.0673, 0.0332, 0.0342,  ..., 0.0514, 0.0514, 0.0514]],\n",
            "\n",
            "         [[0.0382, 0.0957, 0.0640,  ..., 0.0479, 0.0479, 0.0479],\n",
            "          [0.1354, 0.0412, 0.0534,  ..., 0.0454, 0.0454, 0.0454],\n",
            "          [0.0296, 0.0288, 0.0282,  ..., 0.0560, 0.0560, 0.0560],\n",
            "          ...,\n",
            "          [0.0657, 0.0446, 0.0311,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0657, 0.0446, 0.0311,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0657, 0.0446, 0.0311,  ..., 0.0502, 0.0502, 0.0502]],\n",
            "\n",
            "         [[0.0443, 0.0577, 0.1177,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0835, 0.0912, 0.0395,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0667, 0.0261, 0.1022,  ..., 0.0485, 0.0485, 0.0485],\n",
            "          ...,\n",
            "          [0.0624, 0.0424, 0.0491,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.0624, 0.0424, 0.0491,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.0624, 0.0424, 0.0491,  ..., 0.0448, 0.0448, 0.0448]]],\n",
            "\n",
            "\n",
            "        [[[0.0435, 0.0434, 0.0230,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0478, 0.0662, 0.0521,  ..., 0.0413, 0.0413, 0.0413],\n",
            "          [0.0474, 0.0423, 0.0880,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          ...,\n",
            "          [0.0443, 0.0571, 0.0380,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0443, 0.0571, 0.0380,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0443, 0.0571, 0.0380,  ..., 0.0436, 0.0436, 0.0436]],\n",
            "\n",
            "         [[0.0313, 0.0566, 0.0618,  ..., 0.0621, 0.0621, 0.0621],\n",
            "          [0.0571, 0.0441, 0.0326,  ..., 0.0521, 0.0521, 0.0521],\n",
            "          [0.0498, 0.0852, 0.0481,  ..., 0.0529, 0.0529, 0.0529],\n",
            "          ...,\n",
            "          [0.0535, 0.0341, 0.0242,  ..., 0.0479, 0.0479, 0.0479],\n",
            "          [0.0535, 0.0341, 0.0242,  ..., 0.0479, 0.0479, 0.0479],\n",
            "          [0.0535, 0.0341, 0.0242,  ..., 0.0479, 0.0479, 0.0479]],\n",
            "\n",
            "         [[0.0341, 0.0167, 0.0401,  ..., 0.0645, 0.0645, 0.0645],\n",
            "          [0.0811, 0.0490, 0.0466,  ..., 0.0508, 0.0508, 0.0508],\n",
            "          [0.0652, 0.0378, 0.0694,  ..., 0.0500, 0.0500, 0.0500],\n",
            "          ...,\n",
            "          [0.0856, 0.0591, 0.0565,  ..., 0.0341, 0.0341, 0.0341],\n",
            "          [0.0856, 0.0591, 0.0565,  ..., 0.0341, 0.0341, 0.0341],\n",
            "          [0.0856, 0.0591, 0.0565,  ..., 0.0341, 0.0341, 0.0341]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0808, 0.0464, 0.0379,  ..., 0.0344, 0.0344, 0.0344],\n",
            "          [0.0548, 0.0622, 0.0573,  ..., 0.0382, 0.0382, 0.0382],\n",
            "          [0.0543, 0.0700, 0.0812,  ..., 0.0410, 0.0410, 0.0410],\n",
            "          ...,\n",
            "          [0.0575, 0.0539, 0.0303,  ..., 0.0494, 0.0494, 0.0494],\n",
            "          [0.0575, 0.0539, 0.0303,  ..., 0.0494, 0.0494, 0.0494],\n",
            "          [0.0575, 0.0539, 0.0303,  ..., 0.0494, 0.0494, 0.0494]],\n",
            "\n",
            "         [[0.0507, 0.0432, 0.0524,  ..., 0.0522, 0.0522, 0.0522],\n",
            "          [0.0501, 0.0769, 0.0819,  ..., 0.0422, 0.0422, 0.0422],\n",
            "          [0.0492, 0.0657, 0.0746,  ..., 0.0370, 0.0370, 0.0370],\n",
            "          ...,\n",
            "          [0.0366, 0.0600, 0.0390,  ..., 0.0549, 0.0549, 0.0549],\n",
            "          [0.0366, 0.0600, 0.0390,  ..., 0.0549, 0.0549, 0.0549],\n",
            "          [0.0366, 0.0600, 0.0390,  ..., 0.0549, 0.0549, 0.0549]],\n",
            "\n",
            "         [[0.0383, 0.0473, 0.0509,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0623, 0.0451, 0.0402,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0443, 0.0406, 0.0186,  ..., 0.0634, 0.0634, 0.0634],\n",
            "          ...,\n",
            "          [0.0660, 0.1060, 0.0660,  ..., 0.0362, 0.0362, 0.0362],\n",
            "          [0.0660, 0.1060, 0.0660,  ..., 0.0362, 0.0362, 0.0362],\n",
            "          [0.0660, 0.1060, 0.0660,  ..., 0.0362, 0.0362, 0.0362]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0702, 0.0554, 0.0576,  ..., 0.0288, 0.0412, 0.0672],\n",
            "          [0.0674, 0.0495, 0.0384,  ..., 0.0645, 0.0488, 0.0410],\n",
            "          [0.0484, 0.0332, 0.0287,  ..., 0.0476, 0.0422, 0.0588],\n",
            "          ...,\n",
            "          [0.0918, 0.0492, 0.0420,  ..., 0.0358, 0.0416, 0.0820],\n",
            "          [0.0281, 0.0742, 0.0493,  ..., 0.0447, 0.0632, 0.0451],\n",
            "          [0.0506, 0.0334, 0.0682,  ..., 0.0393, 0.0326, 0.0473]],\n",
            "\n",
            "         [[0.0599, 0.0760, 0.0262,  ..., 0.0478, 0.0587, 0.0411],\n",
            "          [0.0549, 0.0274, 0.0478,  ..., 0.0374, 0.0722, 0.0282],\n",
            "          [0.0675, 0.0867, 0.0241,  ..., 0.0992, 0.0420, 0.0682],\n",
            "          ...,\n",
            "          [0.0749, 0.0902, 0.0278,  ..., 0.0350, 0.0588, 0.0270],\n",
            "          [0.0575, 0.0577, 0.0717,  ..., 0.0340, 0.0431, 0.0537],\n",
            "          [0.0718, 0.0439, 0.0278,  ..., 0.0400, 0.0697, 0.0420]],\n",
            "\n",
            "         [[0.0314, 0.0778, 0.0384,  ..., 0.0528, 0.0350, 0.0673],\n",
            "          [0.0397, 0.0784, 0.0428,  ..., 0.0494, 0.0302, 0.0364],\n",
            "          [0.0254, 0.0331, 0.0300,  ..., 0.0867, 0.0436, 0.0265],\n",
            "          ...,\n",
            "          [0.0742, 0.0384, 0.0859,  ..., 0.0559, 0.0335, 0.0322],\n",
            "          [0.0643, 0.0355, 0.0517,  ..., 0.0402, 0.0521, 0.0455],\n",
            "          [0.0483, 0.0403, 0.0739,  ..., 0.0724, 0.0430, 0.0389]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0688, 0.0402, 0.0709,  ..., 0.0428, 0.0282, 0.0595],\n",
            "          [0.0394, 0.0866, 0.0475,  ..., 0.0355, 0.0503, 0.0372],\n",
            "          [0.0349, 0.0565, 0.0615,  ..., 0.0607, 0.0382, 0.0264],\n",
            "          ...,\n",
            "          [0.0345, 0.0283, 0.0325,  ..., 0.0223, 0.0758, 0.0583],\n",
            "          [0.0490, 0.0667, 0.0595,  ..., 0.0380, 0.0481, 0.0549],\n",
            "          [0.0752, 0.0474, 0.0398,  ..., 0.0675, 0.0434, 0.0554]],\n",
            "\n",
            "         [[0.0712, 0.0452, 0.0638,  ..., 0.0313, 0.0546, 0.0442],\n",
            "          [0.0673, 0.0444, 0.0411,  ..., 0.0512, 0.0397, 0.0375],\n",
            "          [0.0344, 0.0576, 0.0420,  ..., 0.0330, 0.0581, 0.0731],\n",
            "          ...,\n",
            "          [0.0577, 0.0499, 0.0334,  ..., 0.0402, 0.0509, 0.0301],\n",
            "          [0.0536, 0.0646, 0.0684,  ..., 0.0518, 0.0528, 0.0498],\n",
            "          [0.0495, 0.0453, 0.0631,  ..., 0.0609, 0.0614, 0.0525]],\n",
            "\n",
            "         [[0.0645, 0.0547, 0.0311,  ..., 0.0493, 0.0293, 0.0521],\n",
            "          [0.0448, 0.0661, 0.0423,  ..., 0.0273, 0.0381, 0.0527],\n",
            "          [0.0457, 0.0307, 0.0739,  ..., 0.0302, 0.0690, 0.0323],\n",
            "          ...,\n",
            "          [0.0483, 0.0410, 0.0477,  ..., 0.0367, 0.0313, 0.0615],\n",
            "          [0.0507, 0.0465, 0.0491,  ..., 0.0565, 0.0248, 0.0601],\n",
            "          [0.0383, 0.0410, 0.0617,  ..., 0.0483, 0.0576, 0.0493]]],\n",
            "\n",
            "\n",
            "        [[[0.0703, 0.0404, 0.0578,  ..., 0.0407, 0.0407, 0.0407],\n",
            "          [0.0503, 0.0426, 0.0649,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0414, 0.0675, 0.0402,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          ...,\n",
            "          [0.0554, 0.0368, 0.0723,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0554, 0.0368, 0.0723,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0554, 0.0368, 0.0723,  ..., 0.0400, 0.0400, 0.0400]],\n",
            "\n",
            "         [[0.0481, 0.0516, 0.0722,  ..., 0.0362, 0.0362, 0.0362],\n",
            "          [0.0383, 0.0442, 0.0522,  ..., 0.0558, 0.0558, 0.0558],\n",
            "          [0.0708, 0.0432, 0.0302,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          ...,\n",
            "          [0.0640, 0.0539, 0.0683,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          [0.0640, 0.0539, 0.0683,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          [0.0640, 0.0539, 0.0683,  ..., 0.0465, 0.0465, 0.0465]],\n",
            "\n",
            "         [[0.0800, 0.0450, 0.0461,  ..., 0.0464, 0.0464, 0.0464],\n",
            "          [0.0548, 0.0397, 0.0660,  ..., 0.0458, 0.0458, 0.0458],\n",
            "          [0.0381, 0.0379, 0.0309,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          ...,\n",
            "          [0.0375, 0.0769, 0.0537,  ..., 0.0303, 0.0303, 0.0303],\n",
            "          [0.0375, 0.0769, 0.0537,  ..., 0.0303, 0.0303, 0.0303],\n",
            "          [0.0375, 0.0769, 0.0537,  ..., 0.0303, 0.0303, 0.0303]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0465, 0.0535, 0.0591,  ..., 0.0348, 0.0348, 0.0348],\n",
            "          [0.0348, 0.0552, 0.0423,  ..., 0.0486, 0.0486, 0.0486],\n",
            "          [0.0562, 0.0712, 0.0475,  ..., 0.0424, 0.0424, 0.0424],\n",
            "          ...,\n",
            "          [0.0277, 0.0412, 0.0610,  ..., 0.0485, 0.0485, 0.0485],\n",
            "          [0.0277, 0.0412, 0.0610,  ..., 0.0485, 0.0485, 0.0485],\n",
            "          [0.0277, 0.0412, 0.0610,  ..., 0.0485, 0.0485, 0.0485]],\n",
            "\n",
            "         [[0.0487, 0.0661, 0.0579,  ..., 0.0591, 0.0591, 0.0591],\n",
            "          [0.0357, 0.0422, 0.0389,  ..., 0.0406, 0.0406, 0.0406],\n",
            "          [0.0531, 0.0638, 0.0361,  ..., 0.0292, 0.0292, 0.0292],\n",
            "          ...,\n",
            "          [0.0754, 0.0513, 0.0397,  ..., 0.0471, 0.0471, 0.0471],\n",
            "          [0.0754, 0.0513, 0.0397,  ..., 0.0471, 0.0471, 0.0471],\n",
            "          [0.0754, 0.0513, 0.0397,  ..., 0.0471, 0.0471, 0.0471]],\n",
            "\n",
            "         [[0.0648, 0.0426, 0.0485,  ..., 0.0466, 0.0466, 0.0466],\n",
            "          [0.0479, 0.0292, 0.0576,  ..., 0.0391, 0.0391, 0.0391],\n",
            "          [0.0369, 0.0600, 0.0329,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          ...,\n",
            "          [0.0455, 0.0581, 0.0715,  ..., 0.0340, 0.0340, 0.0340],\n",
            "          [0.0455, 0.0581, 0.0715,  ..., 0.0340, 0.0340, 0.0340],\n",
            "          [0.0455, 0.0581, 0.0715,  ..., 0.0340, 0.0340, 0.0340]]],\n",
            "\n",
            "\n",
            "        [[[0.0630, 0.0510, 0.0642,  ..., 0.0362, 0.0362, 0.0362],\n",
            "          [0.0570, 0.0398, 0.0787,  ..., 0.0459, 0.0459, 0.0459],\n",
            "          [0.0521, 0.0523, 0.0852,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          ...,\n",
            "          [0.0619, 0.0419, 0.0804,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0619, 0.0419, 0.0804,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0619, 0.0419, 0.0804,  ..., 0.0455, 0.0455, 0.0455]],\n",
            "\n",
            "         [[0.0330, 0.0813, 0.1068,  ..., 0.0312, 0.0312, 0.0312],\n",
            "          [0.0557, 0.0431, 0.0506,  ..., 0.0544, 0.0544, 0.0544],\n",
            "          [0.0462, 0.0410, 0.0399,  ..., 0.0607, 0.0607, 0.0607],\n",
            "          ...,\n",
            "          [0.0725, 0.0550, 0.0405,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          [0.0725, 0.0550, 0.0405,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          [0.0725, 0.0550, 0.0405,  ..., 0.0475, 0.0475, 0.0475]],\n",
            "\n",
            "         [[0.0268, 0.0518, 0.0593,  ..., 0.0313, 0.0313, 0.0313],\n",
            "          [0.0312, 0.0436, 0.0397,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0353, 0.0383, 0.0161,  ..., 0.0677, 0.0677, 0.0677],\n",
            "          ...,\n",
            "          [0.0288, 0.0799, 0.0773,  ..., 0.0314, 0.0314, 0.0314],\n",
            "          [0.0288, 0.0799, 0.0773,  ..., 0.0314, 0.0314, 0.0314],\n",
            "          [0.0288, 0.0799, 0.0773,  ..., 0.0314, 0.0314, 0.0314]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0512, 0.0630, 0.0274,  ..., 0.0406, 0.0406, 0.0406],\n",
            "          [0.0468, 0.0476, 0.0382,  ..., 0.0420, 0.0420, 0.0420],\n",
            "          [0.0774, 0.0687, 0.0353,  ..., 0.0411, 0.0411, 0.0411],\n",
            "          ...,\n",
            "          [0.0480, 0.0410, 0.0435,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0480, 0.0410, 0.0435,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0480, 0.0410, 0.0435,  ..., 0.0483, 0.0483, 0.0483]],\n",
            "\n",
            "         [[0.0689, 0.0572, 0.0417,  ..., 0.0539, 0.0539, 0.0539],\n",
            "          [0.0598, 0.0475, 0.0209,  ..., 0.0457, 0.0457, 0.0457],\n",
            "          [0.0770, 0.0320, 0.0399,  ..., 0.0641, 0.0641, 0.0641],\n",
            "          ...,\n",
            "          [0.0588, 0.0526, 0.0407,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0588, 0.0526, 0.0407,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0588, 0.0526, 0.0407,  ..., 0.0483, 0.0483, 0.0483]],\n",
            "\n",
            "         [[0.0283, 0.0375, 0.0431,  ..., 0.0608, 0.0608, 0.0608],\n",
            "          [0.0753, 0.0309, 0.0564,  ..., 0.0413, 0.0413, 0.0413],\n",
            "          [0.0636, 0.0763, 0.0397,  ..., 0.0603, 0.0603, 0.0603],\n",
            "          ...,\n",
            "          [0.0563, 0.0685, 0.0404,  ..., 0.0401, 0.0401, 0.0401],\n",
            "          [0.0563, 0.0685, 0.0404,  ..., 0.0401, 0.0401, 0.0401],\n",
            "          [0.0563, 0.0685, 0.0404,  ..., 0.0401, 0.0401, 0.0401]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15318f2a-e854-4b63-9175-fe5ec01953f1"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### 각 head의 결과물 병합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c400da-cf7d-4c15-9967-6fbb4d6358fb"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4143db5b-b7ab-4552-f8a0-b337acb32755"
      },
      "source": [
        "# w_0 : (d_model, d_model)\n",
        "# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n",
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0340,  0.1785,  0.1140,  ..., -0.2309,  0.0606, -0.0093],\n",
            "         [-0.1011,  0.1640,  0.1647,  ..., -0.2290,  0.0757, -0.0131],\n",
            "         [-0.0088,  0.1614,  0.1052,  ..., -0.1820,  0.0577,  0.0167],\n",
            "         ...,\n",
            "         [-0.0282,  0.1478,  0.1180,  ..., -0.2435,  0.0289, -0.0318],\n",
            "         [-0.0282,  0.1478,  0.1180,  ..., -0.2435,  0.0289, -0.0318],\n",
            "         [-0.0282,  0.1478,  0.1180,  ..., -0.2435,  0.0289, -0.0318]],\n",
            "\n",
            "        [[-0.0935,  0.3263,  0.4433,  ..., -0.1985,  0.0151,  0.0284],\n",
            "         [-0.1923,  0.2931,  0.4930,  ..., -0.2123,  0.0227,  0.0939],\n",
            "         [-0.0979,  0.3147,  0.4543,  ..., -0.1673,  0.0500,  0.0169],\n",
            "         ...,\n",
            "         [-0.1700,  0.3166,  0.4764,  ..., -0.1604,  0.0295,  0.0503],\n",
            "         [-0.1700,  0.3166,  0.4764,  ..., -0.1604,  0.0295,  0.0503],\n",
            "         [-0.1700,  0.3166,  0.4764,  ..., -0.1604,  0.0295,  0.0503]],\n",
            "\n",
            "        [[-0.0119,  0.3691,  0.2650,  ..., -0.0173,  0.1910, -0.0545],\n",
            "         [-0.0683,  0.3236,  0.2740,  ..., -0.0157,  0.2044, -0.0180],\n",
            "         [-0.0657,  0.3174,  0.2923,  ..., -0.0165,  0.1344, -0.0262],\n",
            "         ...,\n",
            "         [-0.1117,  0.2909,  0.2541,  ..., -0.0906,  0.2252, -0.0025],\n",
            "         [-0.1117,  0.2909,  0.2541,  ..., -0.0906,  0.2252, -0.0025],\n",
            "         [-0.1117,  0.2909,  0.2541,  ..., -0.0906,  0.2252, -0.0025]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0567,  0.2110, -0.0677,  ..., -0.0721,  0.2246,  0.0228],\n",
            "         [ 0.0844,  0.2011, -0.0665,  ..., -0.0407,  0.1650,  0.0110],\n",
            "         [ 0.0108,  0.2284, -0.1114,  ..., -0.0738,  0.2088, -0.0057],\n",
            "         ...,\n",
            "         [ 0.0995,  0.2035, -0.1276,  ..., -0.0855,  0.1989, -0.0070],\n",
            "         [ 0.0576,  0.1959, -0.0819,  ..., -0.0593,  0.1625, -0.0040],\n",
            "         [ 0.0912,  0.1991, -0.0619,  ..., -0.0432,  0.1864,  0.0195]],\n",
            "\n",
            "        [[-0.0198,  0.0437,  0.2128,  ..., -0.1225, -0.0442,  0.1199],\n",
            "         [-0.0939,  0.0123,  0.2463,  ..., -0.1518, -0.1225,  0.1280],\n",
            "         [-0.0713,  0.0168,  0.2046,  ..., -0.1315, -0.1389,  0.0734],\n",
            "         ...,\n",
            "         [-0.1038,  0.0043,  0.2396,  ..., -0.1191, -0.0819,  0.0923],\n",
            "         [-0.1038,  0.0043,  0.2396,  ..., -0.1191, -0.0819,  0.0923],\n",
            "         [-0.1038,  0.0043,  0.2396,  ..., -0.1191, -0.0819,  0.0923]],\n",
            "\n",
            "        [[-0.1029,  0.2170,  0.3115,  ..., -0.0086,  0.0175,  0.0058],\n",
            "         [-0.0699,  0.1814,  0.2538,  ..., -0.0265, -0.0116,  0.0089],\n",
            "         [-0.0340,  0.3148,  0.2534,  ...,  0.0148,  0.0137, -0.0777],\n",
            "         ...,\n",
            "         [-0.1250,  0.1742,  0.2580,  ..., -0.0277,  0.0088, -0.0549],\n",
            "         [-0.1250,  0.1742,  0.2580,  ..., -0.0277,  0.0088, -0.0549],\n",
            "         [-0.1250,  0.1742,  0.2580,  ..., -0.0277,  0.0088, -0.0549]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "## Req. 2-2 Multi-head self-attention 모듈 클래스 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n",
        "\n",
        "아래 코드의 TODO 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "\n",
        "    # Q, K, V learnable matrices\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # Linear projection for concatenated outputs\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\n",
        "\n",
        "  # scaled-dot product attention\n",
        "  def self_attention(self, q, k, v):\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    return attn_values\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    batch_size = q.shape[0]\n",
        "\n",
        "    # linear projection\n",
        "    ################################################################################\n",
        "    # TODO 1: Implement the forward pass for linear projection.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # head만큼 쪼개준다\n",
        "    ################################################################################\n",
        "    # TODO 2: Implement the forward pass for \bsplit head.                #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n",
        "\n",
        "    return self.w_0(attn_values)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56b2102-9aae-42f4-dd80-f81fcffff764"
      },
      "source": [
        "print(outputs)\n",
        "print(outputs.shape)  # (batch_size, length, d_model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-5.8577e-02,  1.9090e-01,  3.0195e-02,  ...,  9.1379e-02,\n",
            "          -1.1316e-01, -2.2785e-02],\n",
            "         [ 1.6478e-01,  1.0062e-01,  1.3772e-01,  ...,  1.1550e-01,\n",
            "          -2.2411e-01,  2.7540e-01],\n",
            "         [ 4.3012e-02, -4.0370e-02, -1.1256e-01,  ..., -4.4907e-02,\n",
            "           7.4560e-02, -1.4822e-01],\n",
            "         ...,\n",
            "         [-1.9461e-01, -6.9398e-02,  2.2076e-01,  ...,  2.7266e-01,\n",
            "           1.5912e-01,  1.3727e-01],\n",
            "         [-1.9461e-01, -6.9398e-02,  2.2076e-01,  ...,  2.7266e-01,\n",
            "           1.5912e-01,  1.3727e-01],\n",
            "         [-1.9461e-01, -6.9398e-02,  2.2076e-01,  ...,  2.7266e-01,\n",
            "           1.5912e-01,  1.3727e-01]],\n",
            "\n",
            "        [[ 5.6246e-02,  2.0459e-02,  5.8797e-02,  ..., -1.1778e-01,\n",
            "          -5.1849e-02,  6.4371e-02],\n",
            "         [-9.1925e-02, -1.9409e-01, -1.1947e-01,  ...,  8.2040e-02,\n",
            "          -2.3640e-03, -1.7494e-01],\n",
            "         [ 5.3210e-02,  7.9480e-02, -4.3399e-02,  ...,  7.6058e-02,\n",
            "           4.4118e-02,  1.5110e-02],\n",
            "         ...,\n",
            "         [-5.1321e-01, -3.3201e-01,  4.1897e-01,  ...,  8.5119e-01,\n",
            "           5.7368e-01, -1.7695e-02],\n",
            "         [-5.1321e-01, -3.3201e-01,  4.1897e-01,  ...,  8.5119e-01,\n",
            "           5.7368e-01, -1.7695e-02],\n",
            "         [-5.1321e-01, -3.3201e-01,  4.1897e-01,  ...,  8.5119e-01,\n",
            "           5.7368e-01, -1.7695e-02]],\n",
            "\n",
            "        [[ 6.1997e-02,  1.2047e-01,  7.8751e-02,  ..., -1.1321e-01,\n",
            "          -5.1555e-02,  6.6309e-02],\n",
            "         [-1.1007e-01, -4.6279e-02,  1.8171e-01,  ...,  1.0501e-01,\n",
            "          -8.0174e-03, -5.6132e-02],\n",
            "         [-6.6627e-02, -2.2592e-01,  1.4684e-01,  ...,  1.2672e-01,\n",
            "           1.2426e-01,  5.2042e-02],\n",
            "         ...,\n",
            "         [-3.5535e-01, -2.1794e-01,  3.1220e-01,  ...,  5.4628e-01,\n",
            "           3.5524e-01,  1.3108e-01],\n",
            "         [-3.5535e-01, -2.1794e-01,  3.1220e-01,  ...,  5.4628e-01,\n",
            "           3.5524e-01,  1.3108e-01],\n",
            "         [-3.5535e-01, -2.1794e-01,  3.1220e-01,  ...,  5.4628e-01,\n",
            "           3.5524e-01,  1.3108e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-4.4825e-02, -1.2705e-01,  7.8563e-02,  ...,  4.2779e-02,\n",
            "           8.6446e-02, -1.7579e-03],\n",
            "         [-7.0242e-02, -7.2319e-02,  5.4703e-03,  ..., -1.0565e-01,\n",
            "           1.1565e-01, -7.3370e-02],\n",
            "         [ 1.0596e-01,  9.0878e-02, -7.6339e-02,  ...,  6.3899e-02,\n",
            "           8.2671e-03, -7.1325e-02],\n",
            "         ...,\n",
            "         [-9.7313e-02,  1.3641e-01,  5.1505e-02,  ...,  3.3123e-02,\n",
            "          -6.1352e-02,  1.5543e-02],\n",
            "         [-5.1542e-02,  2.2251e-02,  1.0327e-01,  ...,  1.8160e-01,\n",
            "           1.2233e-01,  7.7744e-02],\n",
            "         [-1.4347e-01, -3.0053e-02,  1.4945e-01,  ...,  5.2536e-02,\n",
            "           3.2741e-02,  1.6837e-01]],\n",
            "\n",
            "        [[-1.1200e-02, -5.0780e-03,  9.8144e-02,  ..., -7.6861e-02,\n",
            "           2.6477e-01, -4.9937e-02],\n",
            "         [-2.3564e-01, -2.4632e-01,  6.0245e-02,  ...,  1.7331e-02,\n",
            "           4.7844e-02, -2.5958e-01],\n",
            "         [ 2.6393e-02, -1.1467e-01,  9.6207e-02,  ...,  1.5821e-01,\n",
            "          -9.4202e-02, -1.5468e-01],\n",
            "         ...,\n",
            "         [-1.6649e-01, -1.0603e-01,  1.8660e-01,  ...,  2.7267e-01,\n",
            "           1.3300e-01,  1.1284e-01],\n",
            "         [-1.6649e-01, -1.0603e-01,  1.8660e-01,  ...,  2.7267e-01,\n",
            "           1.3300e-01,  1.1284e-01],\n",
            "         [-1.6649e-01, -1.0603e-01,  1.8660e-01,  ...,  2.7267e-01,\n",
            "           1.3300e-01,  1.1284e-01]],\n",
            "\n",
            "        [[ 9.7469e-02, -5.9604e-02,  5.6569e-02,  ..., -1.8234e-04,\n",
            "          -7.6961e-02, -1.3575e-02],\n",
            "         [-2.3470e-01, -2.3668e-01,  4.1252e-02,  ..., -6.7016e-04,\n",
            "           5.0534e-03, -2.2809e-01],\n",
            "         [-1.1615e-01, -9.0341e-03, -1.9735e-02,  ...,  1.5954e-01,\n",
            "          -7.1301e-02,  2.9330e-02],\n",
            "         ...,\n",
            "         [-2.7017e-01, -1.3247e-01,  2.7156e-01,  ...,  3.5153e-01,\n",
            "           2.1846e-01,  1.0901e-01],\n",
            "         [-2.7017e-01, -1.3247e-01,  2.7156e-01,  ...,  3.5153e-01,\n",
            "           2.1846e-01,  1.0901e-01],\n",
            "         [-2.7017e-01, -1.3247e-01,  2.7156e-01,  ...,  3.5153e-01,\n",
            "           2.1846e-01,  1.0901e-01]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTku1fySVR3L"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}